<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[FFmpeg命令大全]]></title>
    <url>%2F2019%2F05%2F04%2FFFmpeg%2FFFmpeg%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[1. 前言FFMPEG 是特别强大的专门用于处理音视频的开源库。你既可以使用它的 API 对音视频进行处理，也可以使用它提供的工具，如 ffmpeg, ffplay, ffprobe，来编辑你的音视频文件。 本文将简要介绍一下 FFMPEG 库的基本目录结构及其功能，然后详细介绍一下我们在日常工作中，如何使用 ffmpeg 提供的工具来处理音视频文件。 2. FFMPEG 目录及作用 libavcodec： 提供了一系列编码器的实现。 libavformat： 实现在流协议，容器格式及其本IO访问。 libavutil： 包括了hash器，解码器和各类工具函数。 libavfilter： 提供了各种音视频过滤器。 libavdevice： 提供了访问捕获设备和回放设备的接口。 libswresample： 实现了混音和重采样。 libswscale： 实现了色彩转换和缩放工能。 3. FFMPEG 基本概念在讲解 FFMPEG 命令之前，我们先要介绍一些音视频格式的基要概念。 音／视频流 在音视频领域，我们把一路音／视频称为一路流。如我们小时候经常使用VCD看港片，在里边可以选择粤语或国语声音，其实就是CD视频文件中存放了两路音频流，用户可以选择其中一路进行播放。 容器 我们一般把 MP4､ FLV、MOV 等文件格式称之为容器。也就是在这些常用格式文件中，可以存放多路音视频文件。以 MP4 为例，就可以存放一路视频流，多路音频流，多路字幕流。 channel channel 是音频中的概念，称之为声道。在一路音频流中，可以有单声道，双声道或立体声。 4. FFMPEG 命令我们按使用目的可以将 FFMPEG 命令分成以下几类： 基本信息查询命令 录制 分解 / 复用 处理原始数据 滤镜 切割与合并 图／视互转 直播相关 除了 FFMPEG 的基本信息查询命令外，其它命令都按下图所示的流程处理音视频。 然后将编码的数据包传送给解码器（除非为数据流选择了流拷贝，请参阅进一步描述）。 解码器产生未压缩的帧（原始视频/ PCM音频/ …），可以通过滤波进一步处理（见下一节）。 在过滤之后，帧被传递到编码器，编码器并输出编码的数据包。 最后，这些传递给复用器，将编码的数据包写入输出文件。 默认情况下，ffmpeg只包含输入文件中每种类型（视频，音频，字幕）的一个流，并将其添加到每个输出文件中。 它根据以下标准挑选每一个的“最佳”：对于视频，它是具有最高分辨率的流，对于音频，它是具有最多channel的流，对于字幕，是第一个字幕流。 在相同类型的几个流相等的情况下，选择具有最低索引的流。 您可以通过使用 -vn / -an / -sn / -dn 选项来禁用某些默认设置。 要进行全面的手动控制，请使用 -map选项，该选项禁用刚描述的默认设置。 下面我们就来详细介绍一下这些命令。 5. 基本信息查询命令FFMPEG 可以使用下面的参数进行基本信息查询。例如，想查询一下现在使用的 FFMPEG 都支持哪些 filter，就可以用 ffmpeg -filters 来查询。详细参数说明如下： 参数 说明 -version 显示版本。 -formats 显示可用的格式（包括设备）。 -demuxers 显示可用的demuxers。 -muxers 显示可用的muxers。 -devices 显示可用的设备。 -codecs 显示libavcodec已知的所有编解码器。 -decoders 显示可用的解码器。 -encoders 显示所有可用的编码器。 -bsfs 显示可用的比特流filter。 -protocols 显示可用的协议。 -filters 显示可用的libavfilter过滤器。 -pix_fmts 显示可用的像素格式。 -sample_fmts 显示可用的采样格式。 -layouts 显示channel名称和标准channel布局。 -colors 显示识别的颜色名称。 接下来介绍的是 FFMPEG 处理音视频时使用的命令格式与参数。 6. 命令基本格式及参数下面是 FFMPEG 的基本命令格式： 12$ ffmpeg [global_options] &#123;[input_file_options] -i input_url&#125; ... &#123;[output_file_options] output_url&#125; ... ffmpeg 通过 -i 选项读取输任意数量的输入“文件”（可以是常规文件，管道，网络流，抓取设备等），并写入任意数量的输出“文件”。 原则上，每个输入 / 输出“文件”都可以包含任意数量的不同类型的视频流（视频 / 音频 / 字幕 / 附件 / 数据）。 流的数量和 / 或类型是由容器格式来限制。 选择从哪个输入进入到哪个输出将自动完成或使用 -map 选项。 要引用选项中的输入文件，您必须使用它们的索引（从 0 开始）。 例如。 第一个输入文件是0，第二个输入文件是1，等等。类似地，文件内的流被它们的索引引用。 例如： 2：3 是指第三个输入文件中的第四个流。 上面就是 FFMPEG 处理音视频的常用命令，下面是一些常用参数： 6.1 主要参数 参数 说明 -f fmt（输入/输出） 强制输入或输出文件格式。 格式通常是自动检测输入文件，并从输出文件的文件扩展名中猜测出来，所以在大多数情况下这个选项是不需要的。 -i url（输入） 输入文件的网址 -y（全局参数） 覆盖输出文件而不询问。 -n（全局参数） 不要覆盖输出文件，如果指定的输出文件已经存在，请立即退出。 -c [：stream_specifier] codec（输入/输出，每个流） 选择一个编码器（当在输出文件之前使用）或解码器（当在输入文件之前使用时）用于一个或多个流。codec 是解码器/编码器的名称或 copy（仅输出）以指示该流不被重新编码。如：ffmpeg -i INPUT -map 0 -c:v libx264 -c:a copy OUTPUT -codec [：stream_specifier]编解码器（输入/输出，每个流） 同 -c -t duration（输入/输出） 当用作输入选项（在-i之前）时，限制从输入文件读取的数据的持续时间。当用作输出选项时（在输出url之前），在持续时间到达持续时间之后停止输出。 -ss位置（输入/输出） 当用作输入选项时（在-i之前），在这个输入文件中寻找位置。 请注意，在大多数格式中，不可能精确搜索，因此ffmpeg将在位置之前寻找最近的搜索点。 当转码和-accurate_seek被启用时（默认），搜索点和位置之间的这个额外的分段将被解码和丢弃。 当进行流式复制或使用-noaccurate_seek时，它将被保留。当用作输出选项（在输出url之前）时，解码但丢弃输入，直到时间戳到达位置。 -frames [：stream_specifier] framecount（output，per-stream） 停止在帧计数帧之后写入流。 -filter [：stream_specifier] filtergraph（output，per-stream） 创建由filtergraph指定的过滤器图，并使用它来过滤流。filtergraph是应用于流的filtergraph的描述，并且必须具有相同类型的流的单个输入和单个输出。在过滤器图形中，输入与标签中的标签相关联，标签中的输出与标签相关联。有关filtergraph语法的更多信息，请参阅ffmpeg-filters手册。 6.2 视频参数 参数 说明 -vframes num（输出） 设置要输出的视频帧的数量。对于-frames：v，这是一个过时的别名，您应该使用它。 -r [：stream_specifier] fps（输入/输出，每个流） 设置帧率（Hz值，分数或缩写）。作为输入选项，忽略存储在文件中的任何时间戳，根据速率生成新的时间戳。这与用于-framerate选项不同（它在FFmpeg的旧版本中使用的是相同的）。如果有疑问，请使用-framerate而不是输入选项-r。作为输出选项，复制或丢弃输入帧以实现恒定输出帧频fps。 -s [：stream_specifier]大小（输入/输出，每个流） 设置窗口大小。作为输入选项，这是video_size专用选项的快捷方式，由某些分帧器识别，其帧尺寸未被存储在文件中。作为输出选项，这会将缩放视频过滤器插入到相应过滤器图形的末尾。请直接使用比例过滤器将其插入到开头或其他地方。格式是’wxh’（默认 - 与源相同）。 -aspect [：stream_specifier] 宽高比（输出，每个流） 设置方面指定的视频显示宽高比。aspect可以是浮点数字符串，也可以是num：den形式的字符串，其中num和den是宽高比的分子和分母。例如“4：3”，“16：9”，“1.3333”和“1.7777”是有效的参数值。如果与-vcodec副本一起使用，则会影响存储在容器级别的宽高比，但不会影响存储在编码帧中的宽高比（如果存在）。 -vn（输出） 禁用视频录制。 -vcodec编解码器（输出） 设置视频编解码器。这是 -codec：v 的别名。 -vf filtergraph（输出） 创建由filtergraph指定的过滤器图，并使用它来过滤流。 6.3 音频参数 参数 说明 -aframes（输出） 设置要输出的音频帧的数量。这是 -frames：a 的一个过时的别名。 -ar [：stream_specifier] freq（输入/输出，每个流） 设置音频采样频率。对于输出流，它默认设置为相应输入流的频率。对于输入流，此选项仅适用于音频捕获设备和原始分路器，并映射到相应的分路器选件。 -ac [：stream_specifier]通道（输入/输出，每个流） 设置音频通道的数量。对于输出流，它默认设置为输入音频通道的数量。对于输入流，此选项仅适用于音频捕获设备和原始分路器，并映射到相应的分路器选件。 -an（输出） 禁用录音。 -acodec编解码器（输入/输出） 设置音频编解码器。这是-codec的别名：a。 -sample_fmt [：stream_specifier] sample_fmt（输出，每个流） 设置音频采样格式。使用-sample_fmts获取支持的样本格式列表。 -af filtergraph（输出） 创建由filtergraph指定的过滤器图，并使用它来过滤流。 了解了这些基本信息后，接下来我们看看 FFMPEG 具体都能干些什么吧。 7. 录制首先通过下面的命令查看一下 mac 上都有哪些设备。 1$ ffmpeg -f avfoundation -list_devices true -i "" 录屏 1$ ffmpeg -f avfoundation -i 1 -r 30 out.yuv -f 指定使用 avfoundation 采集数据。 -i 指定从哪儿采集数据，它是一个文件索引号。在我的MAC上，1代表桌面（可以通过上面的命令查询设备索引号）。 -r 指定帧率。按ffmpeg官方文档说-r与-framerate作用相同，但实际测试时发现不同。-framerate 用于限制输入，而 -r 用于限制输出。 注意：桌面的输入对帧率没有要求，所以不用限制桌面的帧率。其实限制了也没用。 录屏+声音 1$ ffmpeg -f avfoundation -i 1:0 -r 29.97 -c:v libx264 -crf 0 -c:a libfdk_aac -profile:a aac_he_v2 -b:a 32k out.flv -i 1:0 冒号前面的 “1” 代表的屏幕索引号。冒号后面的”0”代表的声音索相号。 -c:v 与参数 -vcodec 一样，表示视频编码器。c 是 codec 的缩写，v 是video的缩写。 -crf 是 x264 的参数。 0 表式无损压缩。 -c:a 与参数 -acodec 一样，表示音频编码器。 -profile 是 fdk_aac 的参数。 aac_he_v2 表式使用 AAC_HE v2 压缩数据。 -b:a 指定音频码率。 b 是 bitrate的缩写, a是 audio的缩与。 录视频 1$ ffmpeg -framerate 30 -f avfoundation -i 0 out.mp4 -framerate 限制视频的采集帧率。这个必须要根据提示要求进行设置，如果不设置就会报错。 -f 指定使用 avfoundation 采集数据。 -i 指定视频设备的索引号。 视频+音频 1$ ffmpeg -framerate 30 -f avfoundation -i 0:0 out.mp4 录音 1$ ffmpeg -f avfoundation -i :0 out.wav 录制音频裸数据 1$ ffmpeg -f avfoundation -i :0 -ar 44100 -f s16le out.pcm 8. 分解与复用流拷贝是通过将 copy 参数提供给-codec选项来选择流的模式。它使得ffmpeg省略了指定流的解码和编码步骤，所以它只能进行多路分解和多路复用。 这对于更改容器格式或修改容器级元数据很有用。 在这种情况下，上图将简化为： 由于没有解码或编码，速度非常快，没有质量损失。 但是，由于许多因素，在某些情况下可能无法正常工作。 应用过滤器显然也是不可能的，因为过滤器处理未压缩的数据。 抽取音频流 1$ ffmpeg -i input.mp4 -acodec copy -vn out.aac acodec: 指定音频编码器，copy 指明只拷贝，不做编解码。 vn: v 代表视频，n 代表 no 也就是无视频的意思。 抽取视频流 1$ ffmpeg -i input.mp4 -vcodec copy -an out.h264 vcodec: 指定视频编码器，copy 指明只拷贝，不做编解码。 an: a 代表视频，n 代表 no 也就是无音频的意思。 转格式 1$ ffmpeg -i out.mp4 -vcodec copy -acodec copy out.flv 上面的命令表式的是音频、视频都直接 copy，只是将 mp4 的封装格式转成了 flv。 音视频合并 1$ ffmpeg -i out.h264 -i out.aac -vcodec copy -acodec copy out.mp4 9. 处理原始数据提取YUV数据 12$ ffmpeg -i input.mp4 -an -c:v rawvideo -pixel_format yuv420p out.yuv$ ffplay -s wxh out.yuv -c:v rawvideo 指定将视频转成原始数据 -pixel_format yuv420p 指定转换格式为 yuv420p YUV 转 H264 1$ ffmpeg -f rawvideo -pix_fmt yuv420p -s 320x240 -r 30 -i out.yuv -c:v libx264 -f rawvideo out.h264 提取 PCM 数据 12$ ffmpeg -i out.mp4 -vn -ar 44100 -ac 2 -f s16le out.pcm$ ffplay -ar 44100 -ac 2 -f s16le -i out.pcm PCM 转 WAV 1$ ffmpeg -f s16be -ar 8000 -ac 2 -acodec pcm_s16be -i input.raw output.wav 10. 滤镜在编码之前，ffmpeg 可以使用 libavfilter 库中的过滤器处理原始音频和视频帧。 几个链式过滤器形成一个过滤器图形。 ffmpeg 区分两种类型的过滤器图形：简单和复杂。 10.1 简单滤镜简单的过滤器图是那些只有一个输入和输出，都是相同的类型。 在上面的图中，它们可以通过在解码和编码之间插入一个额外的步骤来表示： 简单的 filtergraphs 配置了 per-stream-filter 选项（分别为视频和音频使用 -vf 和 -af 别名）。 一个简单的视频 filtergraph 可以看起来像这样的例子： 请注意，某些滤镜会更改帧属性，但不会改变帧内容。 例如。 上例中的 fps 过滤器会改变帧数，但不会触及帧内容。 另一个例子是 setpts 过滤器，它只设置时间戳，否则不改变帧。 10.2 复杂滤镜复杂的过滤器图是那些不能简单描述为应用于一个流的线性处理链的过滤器图。 例如，当图形有多个输入和/或输出，或者当输出流类型与输入不同时，就是这种情况。 他们可以用下图来表示： 复杂的过滤器图使用 -filter_complex 选项进行配置。 请注意，此选项是全局性的，因为复杂的过滤器图形本质上不能与单个流或文件明确关联。 -lavfi 选项等同于 -filter_complex。 一个复杂的过滤器图的一个简单的例子是覆盖过滤器，它有两个视频输入和一个视频输出，包含一个视频叠加在另一个上面。 它的音频对应是 amix 滤波器。 添加水印 1$ ffmpeg -i out.mp4 -vf "movie=logo.png,scale=64:48[watermask];[in][watermask] overlay=30:10 [out]" water.mp4 -vf 中的 movie 指定 logo 位置。scale 指定 logo 大小。overlay 指定 logo 摆放的位置。 删除水印 先通过 ffplay 找到要删除 LOGO 的位置 1$ ffplay -i test.flv -vf delogo=x=806:y=20:w=70:h=80:show=1 使用 delogo 滤镜删除 LOGO 1$ ffmpeg -i test.flv -vf delogo=x=806:y=20:w=70:h=80 output.flv 视频缩小一倍 1$ ffmpeg -i out.mp4 -vf scale=iw/2:-1 scale.mp4 -vf scale 指定使用简单过滤器 scale，iw/2:-1 中的 iw 指定按整型取视频的宽度。 -1 表示高度随宽度一起变化。 视频裁剪 1$ ffmpeg -i VR.mov -vf crop=in_w-200:in_h-200 -c:v libx264 -c:a copy -video_size 1280x720 vr_new.mp4 crop 格式：crop=out_w:out_h:x:y out_w: 输出的宽度。可以使用 in_w 表式输入视频的宽度。 out_h: 输出的高度。可以使用 in_h 表式输入视频的高度。 x : X坐标 y : Y坐标 如果 x 和 y 设置为 0, 说明从左上角开始裁剪。如果不写是从中心点裁剪。 倍速播放 1$ ffmpeg -i out.mp4 -filter_complex "[0:v]setpts=0.5*PTS[v];[0:a]atempo=2.0[a]" -map "[v]" -map "[a]" speed2.0.mp4 -filter_complex 复杂滤镜，[0:v] 表示第一个（文件索引号是 0）文件的视频作为输入。setpts=0.5*PTS 表示每帧视频的 pts 时间戳都乘 0.5 ，也就是差少一半。[v] 表示输出的别名。音频同理就不详述了。 map 可用于处理复杂输出，如可以将指定的多路流输出到一个输出文件，也可以指定输出到多个文件。”[v]” 复杂滤镜输出的别名作为输出文件的一路流。上面 map的用法是将复杂滤镜输出的视频和音频输出到指定文件中。 对称视频 1$ ffmpeg -i out.mp4 -filter_complex "[0:v]pad=w=2*iw[a];[0:v]hflip[b];[a][b]overlay=x=w" duicheng.mp4 hflip 水平翻转 如果要修改为垂直翻转可以用 vflip。 画中画 1$ ffmpeg -i out.mp4 -i out1.mp4 -filter_complex "[1:v]scale=w=176:h=144:force_original_aspect_ratio=decrease[ckout];[0:v][ckout]overlay=x=W-w-10:y=0[out]" -map "[out]" -movflags faststart new.mp4 录制画中画 12345$ ffmpeg -f avfoundation -i "1" -framerate 30 -f avfoundation -i "0:0" -r 30 -c:v libx264 -preset ultrafast -c:a libfdk_aac -profile:a aac_he_v2 -ar 44100 -ac 2 -filter_complex "[1:v]scale=w=176:h=144:force_original_aspect_ratio=decrease[a];[0:v][a]overlay=x=W-w-10:y=0[out]" -map "[out]" -movflags faststart -map 1:a b.mp4 多路视频拼接 1$ ffmpeg -f avfoundation -i "1" -framerate 30 -f avfoundation -i "0:0" -r 30 -c:v libx264 -preset ultrafast -c:a libfdk_aac -profile:a aac_he_v2 -ar 44100 -ac 2 -filter_complex "[0:v]scale=320:240[a];[a]pad=640:240[b];[b][1:v]overlay=320:0[out]" -map "[out]" -movflags faststart -map 1:a c.mp4 11. 音视频的拼接与裁剪裁剪 1$ ffmpeg -i out.mp4 -ss 00:00:00 -t 10 out1.mp4 -ss 指定裁剪的开始时间，精确到秒 -t 被裁剪后的时长。 合并 首先创建一个 inputs.txt 文件，文件内容如下： 123$ file '1.flv'$ file '2.flv'$ file '3.flv' 然后执行下面的命令： 1$ ffmpeg -f concat -i inputs.txt -c copy output.flv hls切片 1$ ffmpeg -i out.mp4 -c:v libx264 -c:a libfdk_aac -strict -2 -f hls out.m3u8 -strict -2 指明音频使有AAC。 -f hls 转成 m3u8 格式。 12. 视频图片互转视频转 JPEG 1$ ffmpeg -i test.flv -r 1 -f image2 image-%3d.jpeg 视频转 gif 1$ ffmpeg -i out.mp4 -ss 00:00:00 -t 10 out.gif 图片转视频 1$ ffmpeg -f image2 -i image-%3d.jpeg images.mp4 13. 直播相关推流 1$ ffmpeg -re -i out.mp4 -c copy -f flv rtmp://server/live/streamName 拉流保存 1$ ffmpeg -i rtmp://server/live/streamName -c copy dump.flv 转流 1$ ffmpeg -i rtmp://server/live/originalStream -c:a copy -c:v copy -f flv rtmp://server/live/h264Stream 实时推流 1$ ffmpeg -framerate 15 -f avfoundation -i "1" -s 1280x720 -c:v libx264 -f flv rtmp://localhost:1935/live/room 14. ffplay播放 YUV 数据 1$ ffplay -pix_fmt nv12 -s 192x144 1.yuv 播放 YUV 中的 Y 平面 1$ ffplay -pix_fmt nv21 -s 640x480 -vf extractplanes='y' 1.yuv]]></content>
      <categories>
        <category>FFmpeg</category>
      </categories>
      <tags>
        <tag>FFmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频核心技术]]></title>
    <url>%2F2019%2F04%2F30%2FFFmpeg%2F%E9%9F%B3%E8%A7%86%E9%A2%91%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[1. 学习大纲FFmpeg 常用命令： 视频录制命令 多媒体文件的分解/复用命令 裁剪与合并命令 图片/视频互转命令 直播相关命令 各种滤镜命令 FFmpeg 基本开发： C 语言回顾 FFmpeg 核心概念与常用结构体 实战 - 多媒体文件的分解与复用 实战 - 多媒体格式的互转 实战 - 从 MP4 裁剪一段视频 作业 - 实现一个简单的小咖秀 音视频编解码实战： 实战 - H264 解码 实战 - H264 编码 实战 - 音频 AAC 解码 实战 - 音频 AAC 编码 实战 - 视频转图片 音视频渲染实战： SDL 事件处理 SDL 视频文理渲染 SDL 音频渲染 实战1 - 实现 YUV 视频播放 实战2 - YUV 视频倍数播放 实战3 - 实现 PCM 播放器 FFmpeg 开发播放器核心功能： 实战 - 实现 MP4 文件的视频播放 实战 - 实现 MP4 文件的音频播放 实战 - 实现一个初级播放器 实战 - 音视频同步 实战 - 实现播放器内核 Android 中实战 FFmpeg： 编译 Android 端可以使用的 FFmpeg Java 与 C 语言相互调用 实战 - Android 调用 FFmpeg 学习建议： 牢牢抓住音视频的处理机制，了解其本质 勤加练习，熟能生巧 待着问题去学习，事半功倍 音视频的广泛应用： 直播类：音视频会议、教育直播、娱乐/游戏直播 短视频：抖音、快手、小咖秀 网络视频：优酷、腾讯视频、爱奇艺等 音视频通话：微信、QQ、Skype等 视频监控 人工智能：人脸识别，智能音箱等，更关注算法 播放器架构： 渲染流程： FFmpeg 都能做啥： FFmpeg 是一个非常优秀的多媒体框架 FFmpeg 可以运行在 Linux、Mac、Windows 等平台上 能够解码、编码、转码、复用、解复用、过滤音视频数据 FFmpeg 下载便于与安装： 123$ git clone https://git.ffmpeg.org/ffmpeg.git$ config -- help$ make &amp;&amp; make install 2. FFmpeg 常用命令实战我们按使用目的可以将 FFMPEG 命令分成以下几类： 基本信息查询命令 录制 分解 / 复用 处理原始数据 滤镜 切割与合并 图／视互转 直播相关 除了 FFMPEG 的基本信息查询命令外，其它命令都按下图所示的流程处理音视频。 1$ ffplay -s 2560x1600 -pix_fmt uyvy422 out.yuv 3. 初级开发内容 FFmpeg 日志的使用及目录的操作 介绍 FFmpeg 的基本概念及常用的结构体 对复用/解复用及流程操作的各种实践 FFmpeg 代码结构： libavcodec： 提供了一系列编码器的实现。 libavformat： 实现在流协议，容器格式及其本IO访问。 libavutil： 包括了hash器，解码器和各类工具函数。 libavfilter： 提供了各种音视频过滤器。 libavdevice： 提供了访问捕获设备和回放设备的接口。 libswresample： 实现了混音和重采样。 libswscale： 实现了色彩转换和缩放工能。 3.1 FFmpeg 日志系统12345#include &lt;libavutil/log.h&gt;av_log_set_level(AV_LOG_DEBUG) av_log(NULL, AV_LOG_INFO, "...%s\n", op) AV_LOG_ERROR AV_LOG_WARNING AV_LOG_INFO 1234567891011#include &lt;stdio.h&gt;#include &lt;libavutil/log.h&gt;int main(int argc, char *argv[])&#123; av_log_set_level(AV_LOG_DEBUG); av_log(NULL, AV_LOG_INFO, "hello world: %s!\n", "aaa"); return 0;&#125; 3.2 FFmpeg 文件与目录操作文件的删除与重命名： 12345#include &lt;libavformat/avformat.h&gt;avpriv_io_delete() avpriv_io_move(src, dst) 123456789101112131415161718192021#include &lt;stdio.h&gt;#include &lt;libavutil/log.h&gt;#include &lt;libavformat/avformat.h&gt;int main(int argc, char *argv[])&#123; int ret; ret = avpriv_io_delete("./mytestfile.txt"); if(ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Failed to delete file mytestfile.txt\n"); return -1 &#125; ret = avpriv_io_move("111.txt", "222.txt"); if(ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Filed to rename\n"); return -1; &#125; return 0;&#125; 123456$ clang -g -o ffmpeg_del ffmpeg_file.c `pkg-config --libs libavformat`# pkg-config --libs libavformat 指令可以搜索libavformat库所在路径$ pkg-config --libs libavformat-L/usr/local/ffmpeg/lib -lavformat 3.3 FFmpeg 操作目录重要函数1234# avio_open_dir()avio_read_dir()avio_close_dir() 操作目录重要结构体： AVIODirContext 操作目录的上下文 AVIODirEntry 目录项。用于存放文件名，文件大小等信息 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;libavutil/log.h&gt;#include &lt;libavformat/avformat.h&gt;int main(int argc, char *argv[])&#123; av_log_set_level(AV_LOG_INFO); int ret; AVIODirContext *ctx = NULL; AVIODirEntry *entry = NULL; ret = avio_open_dir(&amp;ctx, "./", NULL); if (ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Cant open dir:%s\n", av_err2str(ret)); return -1; &#125; while(1) &#123; ret = avio_read_dir(ctx, &amp;entry); if (ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Cant read dir: %s\n", av_err2str(ret)); goto __fail; &#125; if (!entry) &#123; break; &#125; av_log(NULL, AV_LOG_INFO, "%l2"PRId64" %s\n", entry-&gt;size, entry-&gt;name); avio_free_directory_entry(&amp;entry); &#125;__fail: avio_close_dir(&amp;ctx); return 0;&#125; 1$ clang -g -o list ffmpeg_list.c `pkg-config --libs libavformat libavutil` 3.4 多媒体文件的基本概念 多媒体文件其实是个容器 在容器里有很多流（Stream/Track) 每种流是由不同的编码器编码的 从流中读出的数据称为包 在一个包中包含着一个或多个帧 几个重要的结构体： AVFormatContext AVStream AVPacket FFmpeg 操作流数据的基本步骤： 解复用 —&gt; 获取流 —&gt; 读取数据包 —&gt; 释放资源 3.5 [实战] 打印音/视频信息123av_register_all()avformat_open_input() / avformat_close_input()av_dump_format() 1234567891011121314151617181920212223242526#include &lt;stdio.h&gt;#include &lt;libavutil/log.h&gt;#include &lt;libavformat/avformat.h&gt;int main(int argc, char *argv[])&#123; int ret; av_log_set_level(AV_LOG_INFO); AVFormatContext *fmt_ctx = NULL; av_register_all(); ret = avformat_open_input(&amp;fmt_ctx, "./test.mp4", NULL, NULL); if (ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Can't open file: %s\n", av_err2str(ret)); return -1; &#125; av_dump_format(fmt_ctx, 0, "./test.mp4", 0); avformat_close_input(&amp;fmt_ctx); return 0;&#125; 3.6 [实战] 抽取音频数据123av_init_packet()av_find_best_stream()av_read_frame() / av_packet_unref() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#include &lt;stdio.h&gt;#include &lt;libavutil/log.h&gt;#include &lt;libavformat/avformat.h&gt;int main(int argc, char *argv[])&#123; int ret; int len; int audio_index; char *src = NULL; char *dst = NULL; av_log_set_level(AV_LOG_INFO); AVPacket pkt; AVFormatContext *fmt_ctx = NULL; av_register_all(); // 1. read two params form console if (argc &lt; 3) &#123; av_log(NULL, AV_LOG_ERROR, "eg: %s in_file out_file\n", argv[0]); return -1; &#125; src = argv[1]; dst = argv[2]; if (!src || !dst) &#123; av_log(NULL, AV_LOG_ERROR, "src or dst is null\n"); return -1; &#125; ret = avformat_open_input(&amp;fmt_ctx, src, NULL, NULL); if (ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Can't open file: %s\n", av_err2str(ret)); return -1; &#125; FILE *dst_fd = fopen(dst, "wb"); if (dst_fd) &#123; av_log(NULL, AV_LOG_ERROR, "Can't open out file!\n"); avformat_close_input(&amp;fmt_ctx); return -1; &#125; av_dump_format(fmt_ctx, 0, src, 0); // 2. get stream ret = av_find_best_stream(fmt_ctx, AVMEDIA_TYPE_AUDIO, -1, -1, NULL, 0); if (ret &lt; 0) &#123; av_log(NULL, AV_LOG_ERROR, "Can't find the best stream!\n"); avformat_close_input(&amp;fmt_ctx); fclose(dst_fd); return -1; &#125; audio_index = ret; av_init_packet(&amp;pkt); while(av_read_frame(fmt_ctx, &amp;pkt) &gt;= 0) &#123; if (pkt.stream_index == audio_index) &#123; // 3. write audio data to aac file. len = fwrite(pkt.data, 1, pkt.size, dst_fd); if (len != pkt.size) &#123; av_log(NULL, AV_LOG_WARNING, "warning, length of data is not equal size of pkt!\n"); &#125; &#125; av_packet_unref(&amp;pkt); &#125; avformat_close_input(&amp;fmt_ctx); if (dst_fd) &#123; fclose(dst_fd); &#125; return 0;&#125; 12$ lang -g -o extra_audio extra_audio.c `pkg-config --libs libavutil libavformat`$ ./extra_audio test.mp4 killer.aa 3.7 [实战] 抽取视频数据 Start code SPS/PPS codec -&gt; extradata 3.8 [实战] 将 MP4 转成 FLV 格式123456avformat_alloc_output_context2() / avformat_free_context();avformat_new_stream();avcodec_parameters_copy();avformat_write_header();av_write_frame() / av_interleaved_write_frame();av_write_trailer() 3.9 [实战] 从 MP4 截取一段视频1av_seek_frame() 3.10 [实战] 一个简单的小咖秀 将两个媒体文件中分别抽取音频与视频轨 将音频与视频轨合并成一个新文件 对音频与视频轨进行裁剪 4. FFmpeg 中级开发内容 FFmpeg H264 解码 FFmpeg H264 编码 FFmpeg AAC 解码 FFmpeg AAC 编码 4.1 FFmpeg H264 解码1#include &lt;libavcodec/avcodec.h&gt; 常用数据结构： AVCodec 编码器结构体 AVCodecContext 编码器上下文 AVFrame 解码后的帧 结构体内存的分配与释放： 123av_frame_alloc / av_frame_free();avcodec_alloc_context3();avcodec_free_context(); 解码步骤： 查找解码器（avcodec_find_decoder） 打开解码器（avcodec_open2） 解码（avcodec_decode_video2） 4.2 FFmpeg H264 编码H264编码流程： 查找编码器（avcodec_find_encoder_by_name） 设置参数，打开编码器（avcondec_open2） 编码（avcondec_encode_video2） 4.3 视频转图片TODO 4.4 FFmpeg AAC 编码 编码流程与视频相同 编码函数 avcodec_encodec_audio2 5. SDL 介绍 SDL 官网 SDL（Simple DirectMedia Layer） 是一套开放源代码的跨平台多媒体开发库 由 C 语言实现的跨平台的媒体开源库 多用于开发游戏、模拟器、媒体播放器等多媒体应用领域 语法与子系统： SDL将功能分成下列数个子系统（subsystem）： Video（图像）—图像控制以及线程（thread）和事件管理（event）。 Audio（声音）—声音控制 Joystick（摇杆）—游戏摇杆控制 CD-ROM（光盘驱动器）—光盘媒体控制 Window Management（视窗管理）－与视窗程序设计集成 Event（事件驱动）－处理事件驱动 以下是一支用C语言写成、非常简单的SDL示例： 12345678910111213141516171819// Headers#include "SDL.h"// Main functionint main(int argc, char* argv[])&#123; // Initialize SDL if(SDL_Init(SDL_INIT_EVERYTHING) == -1) return(1); // Delay 2 seconds SDL_Delay(2000); // Quit SDL SDL_Quit(); // Return return 0;&#125; 上述程序会加载所有SDL子系统（出错则退出程序），然后暂停两秒，最后关闭SDL并退出程序。 5.1 SDL 编译与安装 下载 SDL 源码 生成Makefile configure –prefix=/usr/local 安装 sudo make -j 8 &amp;&amp; make install 5.2 使用 SDL 基本步骤 添加头文件 #include &lt;SDL.h&gt; 初始化 SDL 退出 SDL SDL 渲染窗口： 123SDL_Init() / SDL_Quit();SDL_CreateWindow() / SDL_DestoryWindow();SDL_CreateRender(); // 创建渲染器 1$ clang -g -o first_sdl first_sdl.c `pkg-config --libs sdl2` SDL 渲染窗口： 123SDL_CreateRender() / SDL_DestoryRenderer();SDL_RenderClear();SDL_RenderPresent(); 5.3 SDL 事件基本原理 SDL 将所有的事件都存放在一个队列中 所有对事件的操作，其实就是队列的操作 SDL 事件种类： SDL_WindowEvent：窗口事件 SDL_KeyboardEvent：键盘事件 SDL_MouseMotionEvent：鼠标事件 自定义事件 SDL 事件处理： 123SDL_PollEvent(); // 轮询检测SDL_WaitEvent(); // 常用的方式SDL_WaitEventTimeout(); 5.4 文理渲染SDL 渲染基本原理： SDL 文理相关 API： 12345SDL_CreateTexture();- format: YUV, RGB- access: Texture 类型， Target， StreamSDL_DestroyTexture(); SDL 渲染相关 API： 1234SDL_SetRenderTarget();SDL_RenderClear();SDL_RenderCopy();SDL_RenderPresent(); 5.5 [实战] YUV 视频播放器创建线程： 1234SDL_CreateThread();- fn: 线程执行函数- name: 线程名- data: 执行函数参数 SDL 更新文理： 12SDL_UpdateTexture();SDL_UpdateYUVTexture(); 5.6 SDL 播放音频播放音频基本流程： 播放音频的基本原则： 声卡向你要数据而不是你主动推给声卡 数据的多少由音频参数决定的 SDL 音频 API： 123SDL_OpenAudio() / SDL_CloseAudio();SDL_PauseAudio();SDL_MixAudio(); 5.7 实现 PCM 播放器TODO 6. 最简单的播放器 该播放器只实现视频播放 将 FFmpeg 与 SDL 结合到一起 通过 FFmpeg 解码视频数据 通过 SDL 进行渲染 1$ clang -g -o player2 player2.c `pkg-config --cflags --libs sdl2 libavformat libavutil libswscale libavcodec libswresample` 最简单的播放器之二： 可以同时播放音频与视频 使用队列存放音频包 6.1 多线程与锁为什么要用多线程： 多线程的好处 多线程带来的问题 线程的互斥与同步： 互斥 同步 大的任务分为很多小任务通过信号协调 锁与信号量： 锁的种类 通过信号进行同步 锁的中种类： 读写锁 自旋锁 可重入锁 SDL 线程的创建： 12SDL_CreateThread();SDL_WaitThread(); SDL 锁： 12SDL_CreateMutex() / SDL_DestroyMutex(); // 创建互斥量SDL_LockMutex() / SDL_UnlockMutex(); // 锁互斥量于解锁互斥量 SDL 条件变量： 12SDL_CreateCond() / SDL_DestroyCond();SDL_CondWait() / SDL_CondSignal(); 6.2 锁与条件变量的使用TODO 6.3 播放器线程模型 6.4 线程的退出机制 主线程接收到退出事件 解复用线程在循环分流时对 quit 进行判断 视频解码线程从视频流队列中取包时对 quit 进行判断 音视解码从音频流队列中取包时对 quit 进行判断 音视循环解码时对 quit 进行判断 在收到信号变量消息时对 quit 进行判断 6.5 音视频同步时间戳： PTS：Presentation timestamp 渲染时间戳 DTS：Decoding timestamp 解码时间戳 I（intra）/ B（bidirectional）/ P（predicted）帧 时间戳顺序： 实际帧顺序：I B B P 存放帧顺序：I P B B 解码时间戳：1 4 2 3 展示时间戳：1 2 3 4 从哪儿获得 PTS： AVPacket 中的 PTS AVFrame 中的 PTS av_frame_get_best_effort_timestamp() 时间基： tbr：帧率 tbn：time base of stream 流的时间基 tbc：time base of codec 解码的时间基 计算当前帧的 PTS： PTS = PTS * av_q2d(video_stream-&gt;time_base) av_q2d(AVRotional a){ return a.num / (double)a.den; } 计算下一帧的 PTS： video_clock：预测的下一帧视频的 PTS frame_delay：1/tbr audio_clock：音频当前播放的时间戳 音视频同步方式： 视频同步到音频 音频同步到视频 音频和视频都同步到系统时钟 视频播放的基本思路： 一般的做法，展示第一帧视频帧后，获得要显示的下一个视频帧的 PTS，然后设置一个定时器，当定时器超时时后，刷新新的视屏帧，如此反复操作。 7. 如何在 Android 下使用 FFmpegAndroid 架构： 内容： Java 与 C 之间的相互调用 Android 下 FFmpeg 的编译 Android 下如何使用FFmpeg 第一个 JNI 程序： TODO JNI 基本概念： JNIEnv JavaVM 一个Android APP只有一个 JavaVM， 一个 JavaVM 可以有多个JNIEnv 线程 一个线程对应一个JNIEnv Java调用C/C++ 方法一： 在Java层定义 native 关键字函数 方法一：在C/C++层创建 Java_packname_classname_methodname 函数 Java调用C/C++方法二： 什么是Signature： Java与C/C++ 相互调用时，表式函数参数的描述符 输入参数放在（）内，输出参数放在（）外 多个参数之间顺序存放，且用 “；” 分割 C/C++ 调用 Java 方法： FindClass GetMethodID / GetFieldID NewObject Call&lt;TYPE&gt;Method / [G/S]et&lt;type&gt;Field 7.1 [实战] Android 下的播放器TODO 8. IOS 下使用 FFmpegTODO 9. 音视频进阶 FFmpeg Filter 的使用 FFmpeg 裁剪与优化 视频渲染（OpenGL / Metal） 声音的特效 网络传输 Webrtc - 实时互动、直播、P2P音视频传输 AR技术 OpenCV 行业痛点： 回音消除 降噪 视频秒开 多人多视频实时互动 PC端/APP/网页实时视频互通 实时互动与大并发负载]]></content>
      <categories>
        <category>FFmpeg</category>
      </categories>
      <tags>
        <tag>FFmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[音视频入门知识]]></title>
    <url>%2F2019%2F04%2F23%2FFFmpeg%2F%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%9F%B3%E8%A7%86%E9%A2%91%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1. 万人直播架构讲解直播产品的种类： 泛娱乐化直播 花椒、映客等娱乐直播，还有斗鱼、熊猫等游戏直播 实时互动直播 音视频会议、教育直播等，像 思科、全时、声网 泛娱乐化直播架构 信令服务器：创建房间、聊天、礼物。。。。 美女主播 –信令–&gt; 信令服务器 信令服务器–rtmp流地址–&gt;美女主播 美女主播 –推流–&gt; 流媒体云CDN 观众 –信令–&gt; 信令服务器：将观众加入到美女主播间 信令服务器–rmtp流地址–&gt; 观众 观众 &lt;–拉流–&gt; 流媒体云CDN 泛娱乐化直播架构 基于TCP协议实现 发送信令到信令服务器, 服务器收到\执行后, 返回给共享端一个流媒体云的地址 共享端采集自己音视频数据, 形成rtmp流, 推送到CDN网络(推流) 获取流媒体云地址 拉流 实时互动直播架构 基于UDP实现 自有网络: UDP没有自有网络, 需自己搭建 多个节点: 为了保障服务的稳定性以及负载均衡 控制中心: 每个节点定期(心跳)向控制中心报告健康程度, 控制中心根据响应的数据做出决策 内总线: 数据安全性\吞吐量等可靠性得以保障 媒体服务器: 将RTP协议的数据转换成RTMP协议的数据 CDN网络: 根据用户需求进行拉流 2. CDN网络介绍 CDN网络是为了解决什么问题而出现的？ 总结为一句话：CDN网络是为了解决用户访问网络资源慢而出现的一个技术，两个原因： 网络链路太长 人为因素（南电信北联通，利益相关） CDN构成： 边缘结点：用户从边缘节点上获取数据 二级节点：主干网节点，主要用于缓存、减转源站压力 源站：CP(内容提供方)将内容放到源站 查找顺序：边缘结点-&gt;二级节点-&gt;源站 3. 亲手搭建一套简单的直播系统安装nginx 配置rtmp 123456$ brew install nginx-full --with-rtmp-module#(这一步卡了我好久，安装nginx提示一直找不到nginx-full,网上相关的教程没更新，原因在于nginx仓库已搬迁)$ brew tap denji/nginx$ nginx -s reload 重启$ nginx 启动 1$ vi /usr/local/etc/nginx/nginx.conf 12345# 推流$ ffmpeg -re -i out.mp4 -c copy -f flv rtmp://server/live/streamName# 拉流$ ffmpeg -i rtmp://server/live/streamName -c copy dump.flv 4. 音频基础知识 图一音量：甲乙的振动频率相同、振幅不同。图二音调：甲乙振幅相同、频率不同 5. 音频的量化与编码模拟信号进行采样，采样时分频率的从模拟信号获取数据波形值，采样后，进行数据量化，量化后进行编码，把采样的十进制转化为计算机的二进制，也就是数字信号。 模拟数据——》采样——》量化——》编码——》数字信号 采样大小决定了音频的振幅的高度，采样时指一个采样用多少bit存放，常用的是16bit 12345# bit：位 一个二进制数据0或1，是1bit# byte：字节 存储空间的基本计量单位，如：MySQL中定义 VARCHAR(45) 即是指 45个字节；# 1 Byte = 8 Bit = 1 字节# 2^8 = 256, 2^16 = 65535 aac通常44.1k采样率 采样率:采样频率8k/秒、16k/秒、32k/秒、44.1k/秒、48k/秒 什么是音频的采样率？采样率和音质有没有关系？ - 知乎 人能听到的声音范围是20hz-2whz 码率 = 采样率 x 采样大小 x 声道数 12# 宽带速率的单位用 bps(或b/s)表示# 1 B = 8 b 1 B/s = 8 b/s 原始的wav文件，大小是1411.2Kb/s 做完aaclc的编码，大小是128Kb/s 如果是aache-vr这种编码，大小是32Kb/s 6. 音频压缩技术讲解音频压缩技术 1、消除冗余数据（有损压缩技术）。 压缩的主要方法是去除采集到的音频冗余信息，所谓冗余信息包括人耳听觉范围外的音频信号以及被掩蔽掉的音频信号 信号的掩蔽可分为频域掩蔽和时域掩蔽 频域掩蔽：一个强纯音会掩蔽在其附近同时发声的弱纯音。也称同时掩蔽 时域掩蔽：在时间上相邻的声音之间也有掩蔽现象，主要原因是人的大脑处理信息需要花费时间。 同步掩蔽效应和不同频率声音的频率和相对竟是有关，而时间掩蔽则仅仅和时间有关。如果两个声音在时间上特别接近，分辨会有困难（如两个声音音量相差较大且两个声音间隔时间低于5毫秒，则其中弱的那个声音会听不到）。 2、哈夫曼无损编码 音频压缩：频域，时域。 频域: 截取人耳能听到的频率范围，滤掉响度低的声音，去掉某个高频周围低频的声音； 时域: 滤掉某个长时间说话中的低音 7. 音频编解码器选型网上测评结果：音频编解码器 opus &gt; aac &gt; vorbis 音频编解码器： 1：opus， 口模型：实时互动，对实时性要求非常高 耳模型：高保真，对质量要求非常高 至于什么时候使用那个模型，由opus自己内部来决定，同时，他是性能最好的，压缩率最好。 2：AAC，经常用于泛娱乐化直播，因为其对实时性要求不是很高但是对音质要求可能较高，所以，选用AAC，当然也可以选用opus的耳模型 3：sppex，最大的特点就是不仅可以编码音频，还可以对音频进行降噪，优化，尽可能的获取原音频数据 4：G.711(722)，主要用于音视频会议，为了和固话进行相应的融合 8. AAC 讲解cdn，rtmp 支持 aac AAC 产生的目的是取代 MP3 格式： AAC 相对优点：压缩率高，损耗低 aac 三种类型aacaacv1: aac+sbr(频率复用-高频部分采样率高，低频部分采样率低)aacv2: aac+sbr+ps(声道关联，一个声道采集全部，一个声道只采集相对不同的声音) AAC规格描述（AAC、AAC HE、AAC HE V2）–&gt; AAC+SBR=AAC HE V1, AAC + SBR + PS = AAC HE V2 AAC格式： 1、ADIF(Audio Data Interchange Format):只能从头开始解码，常用在磁盘文件中。 2、ADTS(Audio Data Transport Stream)：这种格式每一帧都有一个同步字，可以在音频流的任何位置开始解码，它似于数据流格式（缺点：文件比ADIF大，优点:每个帧都可以被单独解码播放） aac 编码库 ffmpeg AAC，libfdk AAC 9. 视频基本知识I帧：关键帧，采用帧内压缩技术 P帧：向前参考帧，压缩时只参考前一个帧，属于帧间压缩技术 B帧：双向参考帧，压缩时即参考前一帧也参考后一帧，属于帧间压缩技术 一般实时互动都不会使用 B 帧 GOF(group of frame): 一组帧，可以将一段时间内画面变化不大的所有帧划为一组帧 SPS与PPS（这两种都划为 I 帧）： SPS(Sequence Parameter Set): 序列参数集，存放帧数、参考帧数目、解码图像尺寸、帧场编码模式选择标识等。 PPS(Picture Parameter Set): 图像参数集，存放熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系统数调整标识等 视频花屏/卡顿原因： 1、如果 GOP 分组中的 P 帧丢失会造成解码端的图像发生错误（于是形成了花屏）。 2、为了避免花屏问题的发生，一般如果发现 P 帧或者I帧丢失，就不显示本 GOP 内的所有帧，直到下一个 I 帧来后重新刷新图像（因为丢了一组数据，所以形成了卡顿） 视频编码器： 1、x264/x265。 2、openH264(支持 SVC（分层传输） 技术)。 3、vp8/vp9 10. H264 宏块的划分与帧分组H264压缩技术 帧内预测压缩，解决的是空域数据冗余问题（将一幅图里的人眼不是很敏感的色彩、光亮等数据剔除） 帧间预测压缩，解决的是时域数据冗余问题（将一组图里面连续的重复性高的帧剔除） 整数离散余弦变换(DCT)，将空间上的相关性变为频域上无关的数据然后进行量化 CABAC压缩，也叫上下文适应无损压缩 宏块的划分与分组： H264宏块划分与子块划分：宏块里面可以再包含很多子块 子块划分： 帧分组(一组连续的图片，一幅图片为一帧) 11. 视频压缩技术详解 帧间预测: 解决时间数据冗余，比较相邻两帧不同给出运动矢量 + 残差值 帧内预测: 解决空间数据冗余，每一个宏块有一个预测模式，然后讲预测后的图像与原图比较算差值，最后存储预测模式和差值即可。帧内压缩是针对于 I 帧的 11.1 帧间预测组内宏块查找： 11.2 帧内预测 11.3 DCT 压缩 11.4 VLC 压缩 11.5 CABAC 压缩 12. H264 结构与码流H264编码分层： 1、NAL层（Network Abstraction Layer）, 视频数据网络抽象层。 2、VCL层（Video Coding Layer），视频数据编码层，对原始数据进行压缩 码流基本概念： 1、SODB（String Of Data Bits）,原始数据比特流，长度不一定是8的倍数，它是由VCL层产生的。 2、RBSP（Raw Byte Sequence Payload,SODB+trailing bits），算法是在SODB最后一位补1，不按字节对齐则补0。 3、EBSP(Encapsulate Byte Sequence Payload)，需到两个连续的0x00就增加一个0x03。 4、NALU，NAL Header(1B)+EBSP 以太网最大传输字节 1500 字节。 一个H264帧最少要有一个切片(NAL Unit) 切片与宏块的关系： 每个切片都包括切片头和切片数据， 每个切片数据又包括了很多宏块， 每个宏块又包括了宏块的类型、宏块的预测、编码的残渣数据等 13. NAL 单元详解 5 - 关键帧 7- SPS 序列参数集 8- PPS 图像参数集 如： P帧B帧很多都是单一类型。 SPS和PPS这两个NAL单元一般放在同一个RTP包里头 14. YUV 讲解 YUV常见格式：YUV4:2:0、YUV4:2:2、YUV4:4:4 RGB8:8:8 UV 混存则为packed(打包存储)， UV分开存则为planar(平面存储) 15. 总结 rtmp 实时消息传输: tcp/ip 应用层协议 推送/直播 基本数据单元为消息 1B 消息类型 2B 长度 3B 时间 4B 流id 消息体 传输时 消息回被拆分成消息块 chunk chunk header + chunk data flv: 大块音视频 加入标记头信息 延迟表现和大规模并发成熟 HLS：分成5-10s 用m3u8索引管理 用于朋友圈分享 m3u8索引： 直播信号源–视频编码器（后台视频处理）–流切片器–各种ts媒体文件（分发模块）–索引文件（数据库）–客户端 cdn网络 为了解决用户访问资源慢出现的技术 边缘节点 二级节点（大城市） 源站 搭建流媒体服务： 准备流媒体服务器 linux max 编译安装nginx服务 配置rtmp服务并启动nginx服务 声音三要素：音调 音量 音色 音频量化(模数转换)：模拟数据 采样 量化 编码 数字信号 == 0101001110 码率 = 采样率（1.6w/44.1/48k）x 采样大小(8位-电话/16位-常见) x 声道数（单/双） 音频压缩： 有损消除冗余数据 哈夫曼无损编码 音频编码： 时域转频域—心里声学模型—量化编码—比特流格式化—比特流 音频编解码 ： opus（口 耳 实时互动 最快） aac(直播用 次快) speed(回音 降噪等) g.711（固话） aac : 取代mp3 加入 sir ps 技术 aac lc 128k / aac he v2 64k / aac he v2 32k/ aac 格式 ： adif 从头开始解码，用在磁盘文件中 adts 每一帧都有一个同步字，可以在任何位置解码 aac 编码库 ： libfdk_aac &gt; ffmpeg aac &gt;libfaac&gt; libvo_aacenc H264： I帧 关键 帧内压缩 / p帧 向前参考1帧 / B帧 双向参考帧 sps: 序列参数集/pps:图像参数集 GOF： 一组帧数 p帧丢失 会花屏卡顿 视频编码器： x264/x265 /open h264(svc)/vp8/vp9 h264 压缩技术-编码原理： 帧内预测压缩，空域冗余数据/帧间预测压缩，时域冗余数据/dcp整数离散余炫变换，傅立叶变换/cabac压缩 h264结构：视频序列–图像–片–宏块–子快 h264编码分层：nal 视频数据网络抽象层–vcl 视频数据编码层 码率：sodb 原始比特流 / rbsp sodb最后补1 / ebsp 起始码增加一个起始位0x03 / nalu nal+ebsp nal unit = nalu 头部 + 一个切片（头/数据） 切片 yuv格式：4：4:4/4:4:2/4：2:0 （平坦编码 /半平坦编码）]]></content>
      <categories>
        <category>FFmpeg</category>
      </categories>
      <tags>
        <tag>FFmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 快捷键]]></title>
    <url>%2F2018%2F03%2F20%2FvimHotKey%2F</url>
    <content type="text"><![CDATA[Vim使用笔记 1. 文档操作 :e – 重新加载当前文档。 :e! – 重新加载当前文档，并丢弃已做的改动。 :e file – 关闭当前编辑的文件，并开启新的文件。 如果对当前文件的修改未保存，vi 会警告。 :e! file – 放弃对当前文件的修改，编辑新的文件。 :e# 或 ctrl+^ – 回到刚才编辑的文件，很实用。 gf – 打开以光标所在字符串为文件名的文件。 :saveas newfilename – 另存为 2. 光标的移动 gj : 移动到一段内的下一行； gk : 移动到一段内的上一行； w : 前移一个单词，光标停在下一个单词开头； b : 后移一个单词，光标停在上一个单词开头； ( : 前移1句。 ) : 后移1句。 { : 前移1段。 } : 后移1段。 fc : 把光标移到同一行的下一个 c 字符处 Fc : 把光标移到同一行的上一个 c 字符处 tc : 把光标移到同一行的下一个 c 字符前 Tc : 把光标移到同一行的上一个 c 字符后 ; : 配合 f &amp; t 使用，重复一次 , : 配合 f &amp; t 使用，反向重复一次 上面的操作都可以配合 n 使用，比如在正常模式(下面会讲到)下输入3h， 则光标向左移动 3 个字符。 0 : 移动到行首。 g0 : 移到光标所在屏幕行行首。 ^ : 移动到本行第一个非空白字符。 g^: 同 ^ ，但是移动到当前屏幕行第一个非空字符处。 $ : 移动到行尾。 g$ : 移动光标所在屏幕行行尾。 n| : 把光标移到递 n 列上。 nG : 到文件第 n 行。 :n&lt;cr&gt; : 移动到第 n 行。 :$&lt;cr&gt; : 移动到最后一行。 H : 把光标移到屏幕最顶端一行。 M : 把光标移到屏幕中间一行。 L : 把光标移到屏幕最底端一行。 gg : 到文件头部。 G : 到文件尾部。 2.1 翻屏 ctrl+f : 下翻一屏。 ctrl+b : 上翻一屏。 ctrl+d : 下翻半屏。 ctrl+u : 上翻半屏。 ctrl+e : 向下滚动一行。 ctrl+y : 向上滚动一行。 n% : 到文件 n% 的位置。 zz : 将当前行移动到屏幕中央。 zt : 将当前行移动到屏幕顶端。 zb : 将当前行移动到屏幕底端。 2.2 标记使用标记可以快速移动。到达标记后，可以用 Ctrl+o 返回原来的位置。 Ctrl+o 和 Ctrl+i 很像浏览器上的 后退 和 前进 。 m{a-z} : 标记光标所在位置，局部标记，只用于当前文件。 m{A-Z} : 标记光标所在位置，全局标记。标记之后，退出Vim， 重新启动，标记仍然有效。 ``{a-z}` : 移动到标记位置。 &#39;{a-z} : 移动到标记行的行首。 ``{0-9}` ：回到上[2-10]次关闭vim时最后离开的位置。 ``: 移动到上次编辑的位置。’’ 也可以，不过``精确到列，而 ‘’ 精确到行 。如果想跳转到更老的位置，可以按 C-o，跳转到更新的位置用 C-i。 `” : 移动到上次离开的地方。 `. : 移动到最后改动的地方。 :marks – 显示所有标记。 :delmarks a b – 删除标记 a 和 b。 :delmarks a-c – 删除标记 a、b 和 c。 :delmarks a c-f – 删除标记 a、c、d、e、f。 :delmarks! – 删除当前缓冲区的所有标记。 :help mark-motions – 查看更多关于 mark 的知识。 3. 插入文本3.1 基本插入 i : 在光标前插入；一个小技巧：按 8，再按 i，进入插入模式，输入 =， 按 esc 进入命令模式，就会出现 8 个 = 。 这在插入分割线时非常有用，如30i+&lt;esc&gt; 就插入了 36 个 + 组成的分割线。 :r filename : 在当前位置插入另一个文件的内容。 :r !date : 在光标处插入当前日期与时间。同理，:r !command 可以将其它 shell 命令的输出插入当前文档。 3.2 改写插入 c[n]w : 改写光标后 1(n) 个词。 c[n]l : 改写光标后 n 个字母。 c[n]h : 改写光标前 n 个字母。 [n]cc : 修改当前 [n] 行。 [n]s : 以输入的文本替代光标之后 1(n) 个字符，相当于 c[n]l。 [n]S : 删除指定数目的行，并以所输入文本代替之。 注意，类似 cnw,dnw,ynw 的形式同样可以写为 ncw,ndw,nyw。 4. 剪切复制和寄存器4.1 剪切和复制、粘贴 [n]x : 剪切光标右边 n 个字符，相当于 d[n]l。 [n]X : 剪切光标左边 n 个字符，相当于 d[n]h。 y : 复制在可视模式下选中的文本。 yy or Y : 复制整行文本。 y[n]w : 复制一 (n) 个词。 y[n]l : 复制光标右边 1(n) 个字符。 y[n]h : 复制光标左边 1(n) 个字符。 y$ : 从光标当前位置复制到行尾。 y0 : 从光标当前位置复制到行首。 :m,ny&lt;cr&gt; : 复制 m 行到 n 行的内容。 y1G 或 ygg : 复制光标以上的所有行。 yG : 复制光标以下的所有行。 yaw 和 yas：复制一个词和复制一个句子，即使光标不在词首和句首也没关系。 d : 删除（剪切）在可视模式下选中的文本。 d$ or D : 删除（剪切）当前位置到行尾的内容。 d[n]w: 删除（剪切）1(n)个单词 d[n]l: 删除（剪切）光标右边 1(n) 个字符。 d[n]h: 删除（剪切）光标左边 1(n) 个字符。 d0: 删除（剪切）当前位置到行首的内容 [n] dd: 删除（剪切）1(n) 行。 :m,nd&lt;cr&gt; : 剪切 m 行到 n 行的内容。 d1G 或 dgg : 剪切光标以上的所有行。 dG : 剪切光标以下的所有行。 daw 和 das：剪切一个词和剪切一个句子，即使光标不在词首和句首也没关系。 d/f&lt;cr&gt;：这是一个比较高级的组合命令，它将删除当前位置 到下一个 f 之间的内容。 p: 在光标之后粘贴。 P : 在光标之前粘贴。 4.2 文本对象 aw：一个词 as：一句。 ap：一段。 ab：一块（包含在圆括号中的）。 y, d, c, v 都可以跟文本对象。 4.3 寄存器 a-z：都可以用作寄存器名。&quot;ayy 把当前行的内容放入 a 寄存器。 A-Z：用大写字母索引寄存器，可以在寄存器中追加内容。 如 &quot;Ayy 把当前行的内容追加到 a 寄存器中。 :reg : 显示所有寄存器的内容。 &quot;&quot;：不加寄存器索引时，默认使用的寄存器。 &quot;*：当前选择缓冲区，&quot;*yy 把当前行的内容放入当前选择缓冲区。 &quot;+：系统剪贴板。&quot;+yy 把当前行的内容放入系统剪贴板。 5. 查找与替换5.1 查找 /something : 在后面的文本中查找 something。 ?something : 在前面的文本中查找 something。 /pattern/+number : 将光标停在包含 pattern 的行后面第 number 行上。 /pattern/-number : 将光标停在包含 pattern 的行前面第 number 行上。 n : 向后查找下一个。 N : 向前查找下一个。 可以用 grep 或 vimgrep 查找一个模式都在哪些地方出现过，其中 :grep 是调用外部的 grep 程序，而 :vimgrep 是 vim 自己的查找算法。 用法为： :vim[grep]/pattern/[g] [j] files g 的含义是如果一个模式在一行中多次出现，则这一行也在结果中多次出现。 j 的含义是 grep 结束后，结果停在第 j 项，默认是停在第一项。 vimgrep 前面可以加数字限定搜索结果的上限，如 :1vim/pattern/ % 只查找那个模式在本文件中的第一个出现。 其实 vimgrep 在读纯文本电子书时特别有用，可以生成导航的目录。 比如电子书中每一节的标题形式为：n. xxxx。你就可以这样：:vim/^d{1,}./ % 然后用 :cw 或 :copen 查看结果，可以用 C-w H 把 quickfix 窗口移到左侧，就更像个目录了。 5.2 替换 :s/old/new – 用 new 替换当前行第一个 old。 :s/old/new/g – 用 new 替换当前行所有的 old。 :n1,n2s/old/new/g – 用 new 替换文件 n1 行到 n2 行所有的 old。 :%s/old/new/g – 用 new 替换文件中所有的 old。 :%s/^/xxx/g – 在每一行的行首插入 xxx，^ 表示行首。 :%s/$/xxx/g – 在每一行的行尾插入 xxx，$ 表示行尾。 所有替换命令末尾加上 c，每个替换都将需要用户确认。 如：%s/old/new/gc，加上i则忽略大小写(ignore)。 还有一种比替换更灵活的方式，它是匹配到某个模式后执行某种命令， 语法为 :[range]g/pattern/command 例如 : %g/^ xyz/normal dd。 表示对于以一个空格和 xyz 开头的行执行 normal 模式下的 dd 命令。 关于 range 的规定为： 如果不指定 range，则表示当前行。 m,n : 从 m 行到 n 行。 0 : 最开始一行（可能是这样）。 $ : 最后一行 . : 当前行 % : 所有行 5.3 正则表达式高级的查找替换就要用到正则表达式。 \d : 表示十进制数（我猜的） \s : 表示空格 \S : 非空字符 \a : 英文字母 \| : 表示 或 \. : 表示. {m,n} : 表示 m 到 n 个字符。这要和 \s 与 \a 等连用，如 \a\{m,n} 表示 m 到 n 个英文字母。 {m,}: 表示 m 到无限多个字符。 **: 当前目录下的所有子目录。 :help pattern 得到更多帮助。 6. 编辑多个文件6.1 一次编辑多个文件我们可以一次打开多个文件，如 1$ vi a.txt b.txt c.txt 使用 :next(:n) 编辑下一个文件。 :2n 编辑下 2 个文件。 使用 :previous或:N 编辑上一个文件。 使用 :wnext，保存当前文件，并编辑下一个文件。 使用 :wprevious，保存当前文件，并编辑上一个文件。 使用 :args 显示文件列表。 :n filenames 或 :args filenames 指定新的文件列表。 vi -o filenames 在水平分割的多个窗口中编辑多个文件。 vi -O filenames 在垂直分割的多个窗口中编辑多个文件。 6.2 多标签编辑 vim -p files : 打开多个文件，每个文件占用一个标签页。 :tabe, tabnew – 如果加文件名，就在新的标签中打开这个文件， 否则打开一个空缓冲区。 ^w gf – 在新的标签页里打开光标下路径指定的文件。 :tabn – 切换到下一个标签。Control + PageDown，也可以。 :tabp – 切换到上一个标签。Control + PageUp，也可以。 [n] gt – 切换到下一个标签。如果前面加了 n ， 就切换到第 n 个标签。第一个标签的序号就是 1。 :tab split – 将当前缓冲区的内容在新页签中打开。 :tabc[lose] – 关闭当前的标签页。 :tabo[nly] – 关闭其它的标签页。 :tabs – 列出所有的标签页和它们包含的窗口。 :tabm[ove] [N] – 移动标签页，移动到第N个标签页之后。 如 tabm 0 当前标签页，就会变成第一个标签页。 6.3 缓冲区 :buffers 或 :ls 或 :files 显示缓冲区列表。 ctrl+^：在最近两个缓冲区间切换。 :bn – 下一个缓冲区。 :bp – 上一个缓冲区。 :bl – 最后一个缓冲区。 :b[n] 或 :[n]b – 切换到第 n 个缓冲区。 :nbw(ipeout) – 彻底删除第 n 个缓冲区。 :nbd(elete) – 删除第 n 个缓冲区，并未真正删除，还在 unlisted 列表中。 :ba[ll] – 把所有的缓冲区在当前页中打开，每个缓冲区占一个窗口。 7. 分屏编辑 vim -o file1 file2 : 水平分割窗口，同时打开 file1 和 file2 vim -O file1 file2 : 垂直分割窗口，同时打开 file1 和 file2 7.1 水平分割 :split(:sp) – 把当前窗水平分割成两个窗口。(CTRL-W s 或 CTRL-W CTRL-S) 注意如果在终端下，CTRL-S 可能会冻结终端，请按 CTRL-Q 继续。 :split filename – 水平分割窗口，并在新窗口中显示另一个文件。 :nsplit(:nsp) – 水平分割出一个 n 行高的窗口。 :[N]new – 水平分割出一个N行高的窗口，并编辑一个新文件。 ( CTRL-W n 或 CTRL-W CTRL-N) ctrl+w f –水平分割出一个窗口，并在新窗口打开名称为光标所在词的文件 。 C-w C-^ – 水平分割一个窗口，打开刚才编辑的文件。 7.2 垂直分割 :vsplit(:vsp) – 把当前窗口分割成水平分布的两个窗口。 (CTRL-W v 或 CTRL CTRL-V) :[N]vne[w] – 垂直分割出一个新窗口。 :vertical 水平分割的命令： 相应的垂直分割。 7.3 关闭子窗口 :qall – 关闭所有窗口，退出 vim。 :wall – 保存所有修改过的窗口。 :only – 只保留当前窗口，关闭其它窗口。(CTRL-W o) :close – 关闭当前窗口，CTRL-W c能实现同样的功能。 (象 :q :x 同样工作 ) 7.4 调整窗口大小 ctrl+w + –当前窗口增高一行。也可以用 n 增高 n 行。 ctrl+w - –当前窗口减小一行。也可以用 n 减小 n 行。 ctrl+w _ –当前窗口扩展到尽可能的大。也可以用 n 设定行数。 :resize n – 当前窗口 n 行高。 ctrl+w = – 所有窗口同样高度。 n ctrl+w _ – 当前窗口的高度设定为 n 行。 ctrl+w &lt; –当前窗口减少一列。也可以用 n 减少 n 列。 ctrl+w &gt; –当前窗口增宽一列。也可以用 n 增宽 n 列。 ctrl+w | –当前窗口尽可能的宽。也可以用 n 设定列数。 7.5 切换和移动窗口如果支持鼠标，切换和调整子窗口的大小就简单了。 ctrl+w ctrl+w : 切换到下一个窗口。或者是 ctrl+w w。 ctrl+w p : 切换到前一个窗口。 ctrl+w h(l,j,k) :切换到左（右，下，上）的窗口。 ctrl+w t(b) :切换到最上（下）面的窗口。 ctrl+w H(L,K,J) : 将当前窗口移动到最左（右、上、下）面。 ctrl+w r：旋转窗口的位置。 ctrl+w T : 将当前的窗口移动到新的标签页上。 8. 快速编辑8.1 改变大小写 ~ : 反转光标所在字符的大小写。 可视模式下的 U 或 u：把选中的文本变为大写或小写。 gu(U) 接范围（如$，或 G），可以把从光标当前位置到指定位置之间字母全部 转换成小写或大写。如ggguG，就是把开头到最后一行之间的字母全部变为小 写。再如 gu5j，把当前行和下面四行全部变成小写。 8.2 替换（normal模式） r : 替换光标处的字符，同样支持汉字。 R : 进入替换模式，按 esc 回到正常模式。 8.3 撤消与重做（normal模式） [n] u : 取消一(n)个改动。 :undo 5 – 撤销 5 个改变。 :undolist – 你的撤销历史。 ctrl + r : 重做最后的改动。 U : 取消当前行中所有的改动。 :earlier 4m – 回到 4 分钟前 :later 55s – 前进 55 秒 8.4 宏 . –重复上一个编辑动作 qa：开始录制宏 a（键盘操作记录） q：停止录制 @a：播放宏 a 9. 编辑特殊文件9.1 文件加解密 vim -x file : 开始编辑一个加密的文件。 :X – 为当前文件设置密码。 :set key= – 去除文件的密码。 这里是 滇狐总结的比较高级的 vi 技巧。 9.2 文件的编码 :e ++enc=utf8 filename, 让 vim 用 utf-8 的编码打开这个文件。 :w ++enc=gbk，不管当前文件什么编码，把它转存成 gbk 编码。 :set fenc 或 :set fileencoding，查看当前文件的编码。 在 vimrc 中添加 set fileencoding=ucs-bom,utf-8,cp936，vim 会根据要打开的文件选择合适的编码。 注意：编码之间不要留空格。 cp936 对应于 gbk 编码。 ucs-bom 对应于 windows 下的文件格式。 让 vim 正确处理文件格式和文件编码，有赖于 ~/.vimrc的正确配置 9.3 文件格式大致有三种文件格式：unix, dos, mac. 三种格式的区别主要在于回车键的编码：dos 下是回车加换行，unix 下只有 换行符，mac 下只有回车符。 :e ++ff=dos filename, 让 vim 用 dos 格式打开这个文件。 :w ++ff=mac filename, 以 mac 格式存储这个文件。 :set ff，显示当前文件的格式。 在 vimrc 中添加 set fileformats=unix,dos,mac，让 vim 自动识别文件格式。 10. 编程辅助10.1 一些按键 gd : 跳转到局部变量的定义处； gD : 跳转到全局变量的定义处，从当前文件开头开始搜索； g; : 上一个修改过的地方； g, : 下一个修改过的地方； [[ : 跳转到上一个函数块开始，需要有单独一行的 {。 ]] : 跳转到下一个函数块开始，需要有单独一行的 {。 [] : 跳转到上一个函数块结束，需要有单独一行的 }。 ][ : 跳转到下一个函数块结束，需要有单独一行的 }。 [{ : 跳转到当前块开始处； ]} : 跳转到当前块结束处； [/ : 跳转到当前注释块开始处； ]/ : 跳转到当前注释块结束处； % : 不仅能移动到匹配的 (),{} 或 []上，而且能在 #if，#else， #endif 之间跳跃。 下面的括号匹配对编程很实用的。 ci&#39;, di&#39;, yi&#39;：修改、剪切或复制 &#39; 之间的内容。 ca&#39;, da&#39;, ya&#39;：修改、剪切或复制 &#39; 之间的内容，包含 &#39;。 ci&quot;, di&quot;, yi&quot;：修改、剪切或复制 &quot; 之间的内容。 ca&quot;, da&quot;, ya&quot;：修改、剪切或复制 &quot; 之间的内容，包含 &quot;。 ci(, di(, yi(：修改、剪切或复制 ()之间的内容。 ca(, da(, ya(：修改、剪切或复制 () 之间的内容，包含 ()。 ci[, di[, yi[：修改、剪切或复制 [] 之间的内容。 ca[, da[, ya[：修改、剪切或复制 []之间的内容，包含 []。 ci{, di{, yi{：修改、剪切或复制 {} 之间的内容。 ca{, da{, ya{：修改、剪切或复制 {} 之间的内容，包含 {}。 ci&lt;, di&lt;, yi&lt;：修改、剪切或复制 &lt;&gt; 之间的内容。 ca&lt;, da&lt;, ya&lt;：修改、剪切或复制 &lt;&gt; 之间的内容，包含&lt;&gt;。 10.2 ctags Ctrl + ] 找到光标所在位置的标签定义的地方 Ctrl + t 回到跳转之前的标签处 Ctrl + o 退回原来的地方 [I 查找全局标识符. Vim会列出它所找出的匹配行，不仅在当前文件内查找，还会在所有的包含文件中查找 [i 从当前文件起始位置开始查找第一处包含光标所指关键字的位置 ]i 类似上面的 [i，但这里是从光标当前位置开始往下搜索 [{ 转到上一个位于第一列的”{“。（前提是 “{” 和 “}” 都在第一列。） ]} 转到下一个位于第一列的”}” Ctrl+＼+ s 会出现所有调用、定义该函数的地方，输入索引号，回车即可 ctags -R : 生成 tag 文件，-R 表示也为子目录中的文件生成 tags :set tags=path/tags – 告诉 ctags 使用哪个 tag 文件 :tag xyz – 跳到 xyz 的定义处，或者将光标放在 xyz 上按 C-]，返回用 C-t :stag xyz – 用分割的窗口显示 xyz 的定义，或者 C-w ]， 如果用 C-w n ]，就会打开一个 n 行高的窗口 :ptag xyz – 在预览窗口中打开 xyz 的定义，热键是 C-w }。 :pclose – 关闭预览窗口。热键是 C-w z。 :pedit abc.h – 在预览窗口中编辑 abc.h :psearch abc – 搜索当前文件和当前文件 include 的文件，显示包含 abc 的行。 有时一个 tag 可能有多个匹配，如函数重载，一个函数名就会有多个匹配。 这种情况会先跳转到第一个匹配处。 :[n]tnext – 下一 [n] 个匹配。 :[n]tprev – 上一 [n]个匹配。 :tfirst – 第一个匹配 :tlast – 最后一个匹配 :tselect tagname – 打开选择列表 tab 键补齐 :tag xyz&lt;tab&gt; – 补齐以 xyz 开头的 tag 名，继续按 tab 键，会显示其他的。 :tag /xyz&lt;tab&gt; – 会用名字中含有 xyz 的 tag 名补全。 ctags 对 c++ 生成 tags : 1ctags -R --c++-kinds=+p --fields=+iaS --extra=+q 每个参数解释如下： -R : ctags 循环生成子目录的 tags --c++-kinds=+px : ctags 记录 c++ 文件中的函数声明和各种外部和前向声明 --fields=+iaS : ctags 要求描述的信息 其中 i 表示如果有继承，则标识出父类； a 表示如果元素是类成员的话，要标明其调用权限（即是 public 还是 private）； S 表示如果是函数，则标识函数的 signature。 --extra=+q : 强制要求 ctags 做如下操作—如果某个语法元素是类的一个成员，ctags 默认会给其记录一行，可以要求 ctags 对同一个语法元斯屹记一行，这样可以保证在 VIM 中多个同名函数可以通过路径不同来区分。 10.3 cscope查看阅读 c++ 代码 cscope 缺省只解析 C 文件 (.c 和 .h)、lex 文件( .l )和 yacc 文件( .y )，虽然它也可以支持 C++ 以及 Java，但它在扫描目录时会跳过 C++ 及 Java 后缀的文件。如果希望 cscope 解析 C++ 或 Java 文件，需要把这些文件的名字和路径保存在一个名为 cscope.files 的文件。当 cscope 发现在当前目录中存在 cscope.files 时，就会为 cscope.files 中列出的所有文件生成索引数据库。 下面的命令会查找当前目录及子目录中所有后缀名为 &quot;.h&quot;, &quot;.c&quot;, &quot;cc&quot; 和 &quot;.cpp&quot; 的文件，并把查找结果重定向到文件 cscope.files 中。然后 cscope 根据 cscope.files 中的所有文件，生成符号索引文件。最后一条命令使用 ctags 命令，生成一个 tags 文件，在 vim 中执行 &quot;:help tags&quot; 命令查询它的用法。它可以和 cscope 一起使用。 123$ find . -name "*.h" -o -name "*.c" -o -name "*.cc" -o "*.cpp" &gt; cscope.files$ cscope -bkq -i cscope.files$ ctags -R ​ cscope -Rbq : 生成 cscope.out 文件 :cs add /path/to/cscope.out /your/work/dir :cs find c func – 查找 func 在哪些地方被调用 s: 查找 C 语言符号，即查找函数名、宏、枚举值等出现的地方 g: 查找函数、宏、枚举等定义的位置，类似 ctags 所提供的功能 d: 查找本函数调用的函数 c: 查找调用本函数的函数 t: 查找指定的字符串 e: 查找 egrep 模式，相当于 egrep 功能，但查找速度快多了 f: 查找并打开文件，类似 vim 的 find 功能 i: 查找包含本文件的文件 :cw – 打开 quickfix 窗口查看结果 10.4 gtagsGtags 综合了 ctags 和 cscope 的功能。 使用 Gtags 之前，你需要安装 GNU Gtags。 然后在工程目录运行 gtags 。 :Gtags funcname 定位到 funcname 的定义处。 :Gtags -r funcname 查询 funcname被引用的地方。 :Gtags -s symbol 定位 symbol 出现的地方。 :Gtags -g string Goto string 出现的地方。 :Gtags -gi string 忽略大小写。 :Gtags -f filename 显示 filename 中的函数列表。 你可以用 :Gtags -f % 显示当前文件。 :Gtags -P pattern 显示路径中包含特定模式的文件。 如 :Gtags -P .h$ 显示所有头文件， :Gtags -P /vm/ 显示 vm 目录下的文件。 10.5 编译vim 提供了 :make 来编译程序，默认调用的是 make， 如果你当前目录下有 makefile，简单地 :make 即可。 如果你没有 make 程序，你可以通过配置 makeprg 选项来更改 make 调用的程序。 如果你只有一个 abc.java 文件，你可以这样设置： 1set makeprg=javac\ abc.java 然后 :make 即可。如果程序有错，可以通过 quickfix 窗口查看错误。 不过如果要正确定位错误，需要设置好errorformat，让 vim 识别错误信息。 如： 1:setl efm=%A%f:%l:\ %m,%-Z%p^,%-C%.%# %f 表示文件名，%l 表示行号， %m 表示错误信息，其它的还不能理解。 请参考 :help errorformat。 10.6 快速修改窗口其实是 quickfix 插件提供的功能， 对编译调试程序非常有用 :copen – 打开快速修改窗口。 :cclose – 关闭快速修改窗口。 快速修改窗口在 make 程序时非常有用，当 make 之后： :cl – 在快速修改窗口中列出错误。 :cn – 定位到下一个错误。 :cp – 定位到上一个错误。 :cr – 定位到第一个错误。 10.7 自动补全 C-x C-s – 拼写建议。 C-x C-v – 补全 vim 选项和命令。 C-x C-l – 整行补全。 C-x C-f – 自动补全文件路径。弹出菜单后，按 C-f 循环选择，当然也可以按 C-n 和 C-p。 C-x C-p 和C-x C-n – 用文档中出现过的单词补全当前的词。 直接按 C-p 和 C-n也可以。 C-x C-o – 编程时可以补全关键字和函数名啊。 C-x C-i – 根据头文件内关键字补全。 C-x C-d – 补全宏定义。 C-x C-n – 按缓冲区中出现过的关键字补全。 直接按 C-n 或 C-p 即可。 当弹出补全菜单后： C-p 向前切换成员； C-n 向后切换成员； C-e 退出下拉菜单，并退回到原来录入的文字； C-y 退出下拉菜单，并接受当前选项。 10.8 多行缩进缩出 正常模式下，按两下 &gt;; 光标所在行会缩进。 如果先按了 n，再按两下 &gt;;，光标以下的 n 行会缩进。 对应的，按两下 &lt;;，光标所在行会缩出。 如果在编辑代码文件，可以用 = 进行调整。 在可视模式下，选择要调整的代码块，按 =，代码会按书写规则缩排好。 或者 n =，调整 n 行代码的缩排。 10.9 折叠 zf – 创建折叠的命令，可以在一个可视区域上使用该命令； zd – 删除当前行的折叠； zD – 删除当前行的折叠； zfap – 折叠光标所在的段； zo – 打开折叠的文本； zc – 收起折叠； za – 打开/关闭当前折叠； zr – 打开嵌套的折行； zm – 收起嵌套的折行； zR (zO) – 打开所有折行； zM (zC) – 收起所有折行； zj – 跳到下一个折叠处； zk – 跳到上一个折叠处； zi -- enable/disable fold; 11. 其它11.1 工作目录 :pwd 显示vim的工作目录。 :cd path 改变 vim 的工作目录。 :set autochdir 可以让 vim 根据编辑的文件自动切换工作目录。 11.2 一些快捷键（收集中） K : 打开光标所在词的 manpage。 * : 向下搜索光标所在词。 g* : 同上，但部分符合即可。 \# : 向上搜索光标所在词。 g# : 同上，但部分符合即可。 g C-g : 统计全文或统计部分的字数。 11.3 在线帮助 :h(elp) 或 F1 打开总的帮助。 :help user-manual 打开用户手册。 命令帮助的格式为：第一行指明怎么使用那个命令； 然后是缩进的一段解释这个命令的作用，然后是进一步的信息。 :helptags somepath 为 somepath 中的文档生成索引。 :helpgrep 可以搜索整个帮助文档，匹配的列表显示在 quickfix 窗口中。 Ctrl+] 跳转到 tag 主题，Ctrl+t 跳回。 :ver 显示版本信息。 高亮所有搜索模式匹配 shift + * 向后搜索光标所在位置的单词 shift + # 向前搜索光标所在位置的单词 n 和 N 可以继续向后或者向前搜索匹配的字符串 :set hlsearch 高亮所有匹配的字符串 :nohlsearch 临时关闭 :set nohlsearch 彻底关闭，只有重新 :set hlsearch 才可以高亮搜索 vim 高亮显示光标所在的单词，在单词的地方输入 gd 语法高亮 syntax on syntax off vim自动补全 ctrl + n 或者 ctrl + p 复制 vim 文件中所有内容 gg 回到文件首 shift + v 进入 VISUAL LINE 模式 shift + g 全选所有内容 ctrl + insert 复制所选的内容]]></content>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebSocket教程]]></title>
    <url>%2F2017%2F05%2F16%2FWebSocket%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[转自阮一峰网络编程 WebSocket 是一种网络通信协议，很多高级功能都需要它。 为什么需要 WebSocker初次接触 WebSocket 的人，都会问同样的问题：我们已经有了 HTTP 协议，为什么还需要另一个协议？它能带来什么好处？ 答案很简单，因为 HTTP 协议有一个缺陷：通信只能由客户端发起。 举例来说，我们想了解今天的天气，只能是客户端向服务器发出请求，服务器返回查询结果。HTTP 协议做不到服务器主动向客户端推送信息。 这种单向请求的特点，注定了如果服务器有连续的状态变化，客户端要获知就非常麻烦。我们只能使用“轮询”：每隔一段时候，就发出一个询问，了解服务器有没有新的信息。最典型的场景就是聊天室。 轮询的效率低，非常浪费资源（因为必须不停连接，或者 HTTP 连接始终打开）。因此，工程师们一直在思考，有没有更好的方法。WebSocket 就是这样发明的。 简介WebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。 它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于服务器推送技术的一种。 其他特点包括： （1）建立在 TCP 协议之上，服务器端的实现比较容易。 （2）与 HTTP 协议有着良好的兼容性。默认端口也是80和443，并且握手阶段采用 HTTP 协议，因此握手时不容易屏蔽，能通过各种 HTTP 代理服务器。 （3）数据格式比较轻量，性能开销小，通信高效。 （4）可以发送文本，也可以发送二进制数据。 （5）没有同源限制，客户端可以与任意服务器通信。 （6）协议标识符是 ws（如果加密，则为 wss ），服务器网址就是 URL。 1ws://example.com:80/some/path 客户端的简单示例WebSocket 的用法相当简单。 下面是一个网页脚本的例子（点击这里看运行结果），基本上一眼就能明白。 123456789101112131415var ws = new WebSocket("wss://echo.websocket.org");ws.onopen = function(evt) &#123; console.log("Connection open ..."); ws.send("Hello WebSockets!");&#125;;ws.onmessage = function(evt) &#123; console.log( "Received Message: " + evt.data); ws.close();&#125;;ws.onclose = function(evt) &#123; console.log("Connection closed.");&#125;; 客户端的 APIWebSocket 客户端的 API 如下。 WebSocket 构造函数WebSocket 对象作为一个构造函数，用于新建 WebSocket 实例。 1var ws = new WebSocket('ws://localhost:8080'); 执行上面语句之后，客户端就会与服务器进行连接。 实例对象的所有属性和方法清单，参见这里。 webSocket.readyStatereadyState 属性返回实例对象的当前状态，共有四种。 CONNECTING：值为0，表示正在连接。 OPEN：值为1，表示连接成功，可以通信了。 CLOSING：值为2，表示连接正在关闭。 CLOSED：值为3，表示连接已经关闭，或者打开连接失败。 下面是一个示例。 1234567891011121314151617switch (ws.readyState) &#123; case WebSocket.CONNECTING: // do something break; case WebSocket.OPEN: // do something break; case WebSocket.CLOSING: // do something break; case WebSocket.CLOSED: // do something break; default: // this never happens break;&#125; webSocket.onopen实例对象的 onopen 属性，用于指定连接成功后的回调函数。 123ws.onopen = function () &#123; ws.send('Hello Server!');&#125; 如果要指定多个回调函数，可以使用addEventListener`方法。 123ws.addEventListener('open', function (event) &#123; ws.send('Hello Server!');&#125;); webSocket.onclose实例对象的onclose属性，用于指定连接关闭后的回调函数。 12345678910111213ws.onclose = function(event) &#123; var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event&#125;;ws.addEventListener("close", function(event) &#123; var code = event.code; var reason = event.reason; var wasClean = event.wasClean; // handle close event&#125;); webSocket.onmessage实例对象的 onmessage 属性，用于指定收到服务器数据后的回调函数。 123456789ws.onmessage = function(event) &#123; var data = event.data; // 处理数据&#125;;ws.addEventListener("message", function(event) &#123; var data = event.data; // 处理数据&#125;); 注意，服务器数据可能是文本，也可能是二进制数据（ blob 对象或 Arraybuffer 对象）。 12345678910ws.onmessage = function(event)&#123; if(typeof event.data === String) &#123; console.log("Received data string"); &#125; if(event.data instanceof ArrayBuffer)&#123; var buffer = event.data; console.log("Received arraybuffer"); &#125;&#125; 除了动态判断收到的数据类型，也可以使用binaryType属性，显式指定收到的二进制数据类型。 1234567891011// 收到的是 blob 数据ws.binaryType = "blob";ws.onmessage = function(e) &#123; console.log(e.data.size);&#125;;// 收到的是 ArrayBuffer 数据ws.binaryType = "arraybuffer";ws.onmessage = function(e) &#123; console.log(e.data.byteLength);&#125;; webSocket.send( )实例对象的 send( ) 方法用于向服务器发送数据。 发送文本的例子。 1ws.send('your message'); 发送 Blob 对象的例子。 1234var file = document .querySelector('input[type="file"]') .files[0];ws.send(file); 发送 ArrayBuffer 对象的例子。 1234567// Sending canvas ImageData as ArrayBuffervar img = canvas_context.getImageData(0, 0, 400, 320);var binary = new Uint8Array(img.data.length);for (var i = 0; i &lt; img.data.length; i++) &#123; binary[i] = img.data[i];&#125;ws.send(binary.buffer); webSocket.bufferedAmount实例对象的 bufferedAmount 属性，表示还有多少字节的二进制数据没有发送出去。它可以用来判断发送是否结束。 12345678var data = new ArrayBuffer(10000000);socket.send(data);if (socket.bufferedAmount === 0) &#123; // 发送完毕&#125; else &#123; // 发送还没结束&#125; webSocket.onerror实例对象的onerror属性，用于指定报错时的回调函数。 1234567socket.onerror = function(event) &#123; // handle error event&#125;;socket.addEventListener("error", function(event) &#123; // handle error event&#125;); 服务端的实现WebSocket 服务器的实现，可以查看维基百科的列表。 常用的 Node 实现有以下三种。 µWebSockets Socket.IO WebSocket-Node 具体的用法请查看它们的文档，这里不详细介绍了。 WebSocketd下面，我要推荐一款非常特别的 WebSocket 服务器：Websocketd。 它的最大特点，就是后台脚本不限语言，标准输入（stdin）就是 WebSocket 的输入，标准输出（stdout）就是 WebSocket 的输出。 举例来说，下面是一个 Bash 脚本 counter.sh。 123456789#!/bin/bashecho 1sleep 1echo 2sleep 1echo 3 命令行下运行这个脚本，会输出1、2、3，每个值之间间隔1秒。 1234$ bash ./counter.sh123 现在，启动websocketd，指定这个脚本作为服务。 1$ websocketd --port=8080 bash ./counter.sh 上面的命令会启动一个 WebSocket 服务器，端口是 8080 。每当客户端连接这个服务器，就会执行 counter.sh 脚本，并将它的输出推送给客户端。 12345var ws = new WebSocket('ws://localhost:8080/');ws.onmessage = function(event) &#123; console.log(event.data);&#125;; 上面是客户端的 JavaScript 代码，运行之后会在控制台依次输出1、2、3。 有了它，就可以很方便地将命令行的输出，发给浏览器。 1$ websocketd --port=8080 ls 上面的命令会执行ls命令，从而将当前目录的内容，发给浏览器。使用这种方式实时监控服务器，简直是轻而易举（代码）。 更多的用法可以参考官方示例。 Bash 脚本读取客户端输入的例子 五行代码实现一个最简单的聊天服务器 websocketd 的实质，就是命令行的 WebSocket 代理。只要命令行可以执行的程序，都可以通过它与浏览器进行 WebSocket 通信。下面是一个 Node 实现的回声服务 greeter.js。 12345678process.stdin.setEncoding('utf8');process.stdin.on('readable', function() &#123; var chunk = process.stdin.read(); if (chunk !== null) &#123; process.stdout.write('data: ' + chunk); &#125;&#125;); 启动这个脚本的命令如下。 1$ websocketd --port=8080 node ./greeter.js 官方仓库还有其他各种语言的例子。 参考链接 How to Use WebSockets WebSockets - Send &amp; Receive Messages Introducing WebSockets: Bringing Sockets to the Web]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>WebSocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[定制支持串口安装的ubuntu系统镜像]]></title>
    <url>%2F2017%2F05%2F15%2F%E5%AE%9A%E5%88%B6%E6%94%AF%E6%8C%81%E4%B8%B2%E5%8F%A3%E5%AE%89%E8%A3%85%E7%9A%84ubuntu%E7%B3%BB%E7%BB%9F%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[1、所需环境：硬件环境： 笔记本 串口调试线缆 光盘 显示器 FWA产品的任一机型（此次使用的是FWA-4210） SATA或者USB光驱×1 软件环境： 带有genisoimage(旧版是mkisofs)的linux发行版（此次使用的是Ubuntu 16.04 server版） Ubuntu官网通用镜像ISO文件 2、操作过程：2.1 开机进入系统，将光盘挂载到Ubuntu系统CLI命令如下； 1$ mount -o loop ubuntu-16.04.2-server-amd64.iso /mnt/temp 2.2 更改配置相关配置文件（menu.cfg、txt.cfg、isolinux.cfg此文件不是必须要修改，具体见下边解释）。将光盘文件，拷贝到临时目录（家目录或者自己新建目录均可，但建议拷贝到/var或/temp目录下），具体命令如下： 1$ cp -rf /mnt/temp/ /var/mycdrom 因为 /mnt 目录的默认权限是 333 ，所以在此使用 -r 和 -f 参数，-r 代表递归，即文件夹下所有文件都拷贝，-f 代表强制执行； 更改 menu.cfg 文件，如下图，主要是注释掉标准安装的配置文件，以便可以定制安装。 123$ cd /var/mycdrom/temp/isolinux$ vi menu.cfg 注： vi有三种模式，普通模式、编辑模式、命令行模式； I o a进入编辑模式， 普通模式下数字+yy复制 P黏贴 命令行模式：w写入，q离开，！强制执行 注释 menu.cfg 内容如下红框所示： 更改 txt.cfg 文件，主要用于定制串口安装（如下图）： 更改 isolinux.cfg 文件，主要修改grub菜单等待时间（如下图），也可不修改； 2.3 重新打包ISO文件命令如下： 1$ genisoimage -o ubuntu-16.04.2-server-adm64-console_115200.iso -r -J -no-emul-boot -boot-load-size 4 -boot-info-table -b isolinux/isolinux.bin -c isolinux/boot.cat /var/mycdrom/temp genisoimage 是linux各大发行版制作ISO镜像比较流行的工具，若要定制系统，最好在linux下更改相关配置，并使用此工具重新打包；若在Windows平台使用UltraISO等工具解压更改重新打包会出现不稳定的情况（无法找到镜像，无法找到安装源等）。 -o ：是output缩写，用来指定输出镜像名称 -r ： 即rational-rock，用来开放ISO文件所有权限（r、w、x） -J ： 即Joliet，一种ISO9600扩展格式，用来增加兼容性，最好加上 -no-emul-boot -boot-load-size 4 -boot-info-table ：指定兼容模式下虚拟扇区的数量，若不指定，有些BISO会出现一些问题 -b ：指定开机映像文件 -c ：具体开机配置文件 最后加上输出目录 Reboot系统U盘启动，即可安装系统。 3、文本安装系统注意事项3.1 进入安装模式关闭系统插入U盘，启动系统，看到如下提示按F12进入安装系统模式： 1Press F12 for boot menu.. 选择U盘所在的选项。 3.2 分区若是硬盘已有linux发行版系统，那在如下界面，必须umount分区，才能将更改写入分区表 3.3 自动更新如下界面，若有特许需求（需要安装一些特许软件apache、weblogic等）可以选择自动更新（需要联网），一般情况不选则自动更新]]></content>
      <categories>
        <category>Ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell脚本攻略笔记]]></title>
    <url>%2F2017%2F05%2F15%2FShell%E8%84%9A%E6%9C%AC%E6%94%BB%E7%95%A5%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 基本命令1.1 shell 格式输出12345678$ echo 'Hello world !'-n # 忽略结尾的换行符-e # 激活转义字符-E # disable转义字符# echo会将一个换行符追加到输出文本的尾部。可以使用选项-n来忽略结尾的换行符。$ echo -e "1\t2\t3" 打印彩色输出： 1234567# 彩色文本# 重置=0，黑色=30，红色=31，绿色=32，黄色=33，蓝色=34，洋红=35，青色=36，白色=37$ echo -e "\e[1;31m This is red text \e[0m"# 彩色背景# 重置=0，黑色=40，红色=41，绿色=42，黄色=43，蓝色=44，洋红=45，青色=46，白色=47$ echo -e "\e[1;42m Green Background \e[0m" 1$ printf "%-5s %-10s %-4s\n" No Name Mark 原理： %-5s 指明了一个格式为左对齐且宽度为5的字符串替换（ -表示左对齐）。如果不用 - 指定对齐方式，字符串就采用右对齐形式。 %s 、 %c 、%d 和 %f 都是格式替换符（format substitution character），其所对应的参数可以置于带引号的格式字符串之后。 1.2 替换命令 tr123456789101112131415161718192021222324252627282930313233# tr 是 translate的简写$ tr '\0' '\n' # 将 \0 替换成 \n$ tr [选项]… 集合1 [集合2]选项说明：-c, -C, –complement 用集合1中的字符串替换，要求字符集为ASCII。-d, –delete 删除集合1中的字符而不是转换-s, –squeeze-repeats 删除所有重复出现字符序列，只保留第一个；即将重复出现字符串压缩为一个字符串。-t, –truncate-set1 先删除第一字符集较第二字符集多出的字符字符集合的范围：\NNN 八进制值的字符 NNN (1 to 3 为八进制值的字符)\\ 反斜杠\a Ctrl-G 铃声\b Ctrl-H 退格符\f Ctrl-L 走行换页\n Ctrl-J 新行\r Ctrl-M 回车\t Ctrl-I tab键\v Ctrl-X 水平制表符CHAR1-CHAR2 从CHAR1 到 CHAR2的所有字符按照ASCII字符的顺序[CHAR*] in SET2, copies of CHAR until length of SET1[CHAR*REPEAT] REPEAT copies of CHAR, REPEAT octal if starting with 0[:alnum:] 所有的字母和数字[:alpha:] 所有字母[:blank:] 水平制表符，空白等[:cntrl:] 所有控制字符[:digit:] 所有的数字[:graph:] 所有可打印字符，不包括空格[:lower:] 所有的小写字符[:print:] 所有可打印字符，包括空格[:punct:] 所有的标点字符[:space:] 所有的横向或纵向的空白[:upper:] 所有大写字母 1.3 打印变量1234$ var="value"$ echo $var或者$ echo $&#123;var&#125; 1.4 设置环境变量12345# 在PATH中添加一条新路径$ export PATH="$PATH:/home/user/bin"也可以使用：$ PATH="$PATH:/home/user/bin"$ export PATH 1.5 Shell中三种引号的用法123456789101112131415161718# 单引号# 使用单引号时，变量不会被扩展（expand），将依照原样显示。$ var="123"$ echo '$var' will print $var结果为：'$var' will print 123# 双引号# 输出引号中的内容，若存在命令、变量等，会先执行命令解析出结果再输出$ echo "$var" will print $var结果为：123 will print 123# 反引号# 命令替换$ var=`whoami`$ echo $var结果为：root# 备注：反引号和$()作用相同 1.6 获得字符串的长度123456# 用法$ length=$&#123;#var&#125;$ var=12345678901234567890$ echo $&#123;#var&#125;20 1.7 识别当前shell123$ echo $SHELL也可以使用：$ echo $0 1.8 使用shell进行数学运算在Bash shell环境中，可以利用 let、(( )) 和[] 执行基本的算术操作。而在进行高级操作时，expr 和 bc 这两个工具也会非常有用。 使用 let 时，变量名之前不需要再添加 $ 123$ no1=4$ let no1++$ let no1+=6 # 等同于let no=no+6 1234# 操作符[]的使用方法和let命令类似$ result=$[ no1 + no2 ]# 在[]中也可以使用$前缀$ result=$[ $no1 + 5 ] 12# 使用(())时，变量名之前需要加上$$ result=$(( no1 + 50 )) 123# expr同样可以用于基本算术操作$ result=`expr 3 + 4`$ result=$(expr $no1 + 5) bc是一个用于数学运算的高级工具，这个精密计算器包含了大量的选项 。此处不多介绍。 1.9 shell中各种括号的作用()、(())、[]、[[]]、{}1.9.1 小括号，圆括号（）1、单小括号 ( ) 命令组。括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用。 命令替换。等同于cmd，shell扫描一遍命令行，发现了$(cmd)结构 ，便将 $(cmd) 中的cmd执行一次，得到其标准输出，再将此输出放到原来命令。有些shell不支持，如tcsh。 用于初始化数组。如：array=(a b c d)。 2、双小括号 (( )) 整数扩展。这种扩展计算是整数型的计算，不支持浮点型。((exp))结构扩展并计算一个算术表达式的值，如果表达式的结果为0，那么返回的退出状态码为1，或者 是”假”，而一个非零值的表达式所返回的退出状态码将为0，或者是”true”。若是逻辑判断，表达式exp为真则为1,假则为0。 只要括号中的运算符、表达式符合C语言运算规则，都可用在 $((exp))中，甚至是三目运算符。作不同进位(如二进制、八进制、十六进制)运算时，输出结果全都自动转化成了十进制。如：echo $((16#5f)) 结果为95 (16进位转十进制)。 单纯用 (( )) 也可重定义变量值，比如 a=5; ((a++)) 可将 $a 重定义为6。 常用于算术运算比较，双括号中的变量可以不使用$ 符号前缀。括号内支持多个表达式用逗号分开。 只要括号中的表达式符合C语言运算规则,比如可以直接使用for((i=0;i&lt;5;i++)), 如果不使用双括号, 则为for i in seq 0 4或者for i in {0..4}。再如可以直接使用 if (($i&lt;5)) , 如果不使用双括号, 则为 if [ $i -lt 5 ] 。 1.9.2 中括号，方括号[]1、单中括号 [] bash 的内部命令，[和test是等同的。如果我们不用绝对路径指明，通常我们用的都是bash自带的命令。if/test结构中的左中括号是调用test的命令标识，右中括号是关闭条件判断的。这个命令把它的参数作为比较表达式或者作为文件测试，并且根据比较的结果来返回一个退出状态码。if/test结构中并不是必须右中括号，但是新版的Bash中要求必须这样。 Test和[]中可用的比较运算符只有==和!=，两者都是用于字符串比较的，不可用于整数比较，整数比较只能使用-eq，-gt这种形式。无论是字符串比较还是整数比较都不支持大于号小于号。如果实在想用，对于字符串比较可以使用转义形式，如果比较”ab”和”bc”：[ ab &lt; bc ]，结果为真，也就是返回状态为0。[ ]中的逻辑与和逻辑或使用-a 和-o 表示。 字符范围。用作正则表达式的一部分，描述一个匹配的字符范围。作为test用途的中括号内不能使用正则。 在一个array 结构的上下文中，中括号用来引用数组中每个元素的编号。 2、双中括号 [[ ]] [[是 bash 程序语言的关键字。并不是一个命令，[[ ]] 结构比[ ]结构更加通用。在[[和]]之间所有的字符都不会发生文件名扩展或者单词分割，但是会发生参数扩展和命令替换。 支持字符串的模式匹配，使用=~操作符时甚至支持shell的正则表达式。字符串比较时可以把右边的作为一个模式，而不仅仅是一个字符串，比如[[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号。 使用[[ … ]]条件判断结构，而不是[ … ]，能够防止脚本中的许多逻辑错误。比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中，但是如果出现在[ ]结构中的话，会报错。比如可以直接使用 if [[ $a != 1 &amp;&amp; $a != 2 ]] , 如果不适用双括号, 则为 if [ $a -ne 1] &amp;&amp; [ $a != 2 ]或者 if [ $a -ne 1 -a $a != 2 ] 。 bash把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码。 1.9.3 大括号、花括号 {}1）常规用法 大括号拓展。(通配(globbing))将对大括号中的文件名做扩展。在大括号中，不允许有空白，除非这个空白被引用或转义。第一种：对大括号中的以逗号分割的文件列表进行拓展。如 touch {a,b}.txt 结果为a.txt b.txt。第二种：对大括号中以点点（..）分割的顺序文件列表起拓展作用，如：touch {a..d}.txt 结果为a.txt b.txt c.txt d.txt 代码块，又被称为内部组，这个结构事实上创建了一个匿名函数 。与小括号中的命令不同，大括号内的命令不会新开一个子shell运行，即脚本余下部分仍可使用括号内变量。括号内的命令间用分号隔开，最后一个也必须有分号。{}的第一个命令和左括号之间必须要有一个空格。 2）几种特殊的替换结构 1$&#123;var:-string&#125;,$&#123;var:+string&#125;,$&#123;var:=string&#125;,$&#123;var:?string&#125; ${var:-string} 和 ${var:=string}: 若变量var为空，则用在命令行中用string来替换 ${var:-string}，否则变量var不为空时，则用变量var的值来替换 ${var:-string} ；对于 ${var:=string} 的替换规则和 ${var:-string} 是一样的，所不同之处是 ${var:=string} 若var为空时，用string替换 ${var:=string} 的同时，把string赋给变量 var： ${var:=string} 很常用的一种用法是，判断某个变量是否赋值，没有的话则给它赋上一个默认值。 ${var:+string} 的替换规则和上面的相反，即只有当var不是空的时候才替换成string，若var为空时则不替换或者说是替换成变量 var的值，即空值。(因为变量var此时为空，所以这两种说法是等价的) 。 ${var:?string} 替换规则为：若变量var不为空，则用变量var的值来替换 ${var:?string} ；若变量var为空，则把string输出到标准错误中，并从脚本中退出。我们可利用此特性来检查是否设置了变量的值。 补充扩展：在上面这五种替换结构中string不一定是常值的，可用另外一个变量的值或是一种命令的输出。 3）四种模式匹配替换结构 模式匹配记忆方法： 123# 是去掉左边(在键盘上#在$之左边)% 是去掉右边(在键盘上%在$之右边)#和%中的单一符号是最小匹配，两个相同符号是最大匹配。 1$&#123;var%pattern&#125;,$&#123;var%%pattern&#125;,$&#123;var#pattern&#125;,$&#123;var##pattern&#125; 第一种模式：${variable%pattern} ，这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最短的匹配模式 第二种模式：${variable%%pattern}，这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式 第三种模式：${variable#pattern} 这种模式时，shell在variable中查找，看它是否一给的模式pattern开始，如果是，就从命令行把variable中的内容去掉左边最短的匹配模式 第四种模式：${variable##pattern} 这种模式时，shell在variable中查找，看它是否一给的模式pattern结尾，如果是，就从命令行把variable中的内容去掉右边最长的匹配模式 这四种模式中都不会改变variable的值，其中，只有在pattern中使用了匹配符号时，%和%%，#和##才有区别。结构中的pattern支持通配符，表示零个或多个任意字符，?表示仅与一个任意字符匹配，[…]表示匹配中括号里面的字符，[!…]表示不匹配中括号里面的字符。 4）字符串提取和替换 1$&#123;var:num&#125;,$&#123;var:num1:num2&#125;,$&#123;var/pattern/pattern&#125;,$&#123;var//pattern/pattern&#125; 第一种模式：${var:num} ，这种模式时，shell在var中提取第num个字符到末尾的所有字符。若num为正数，从左边0处开始；若num为负数，从右边开始提取字串，但必须使用在冒号后面加空格或一个数字或整个num加上括号，如 ${var: -2} 、${var:1-3} 或 ${var:(-2)}。 第二种模式：${var:num1:num2}，num1是位置，num2是长度。表示从 $var字符串的第$num1 个位置开始提取长度为$num2的子串。不能为负数。 第三种模式：${var/pattern/pattern}表示将var字符串的第一个匹配的pattern替换为另一个pattern。。 第四种模式：${var//pattern/pattern} 表示将var字符串中的所有能匹配的pattern替换为另一个pattern。 1.9.4 符号$后的括号 ${a} 变量a的值, 在不引起歧义的情况下可以省略大括号。 $(cmd) 命令替换，和cmd效果相同，结果为shell命令cmd的输，过某些Shell版本不支持 $() 形式的命令替换, 如tcsh。 $((expression)) 和exprexpression效果相同, 计算数学表达式exp的数值, 其中exp只要符合C语言的运算规则即可, 甚至三目运算符和逻辑表达式都可以计算。 1.9.5 多条命令执行 单小括号，(cmd1;cmd2;cmd3) 新开一个子shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后可以没有分号。 单大括号，{ cmd1;cmd2;cmd3;} 在当前shell顺序执行命令cmd1,cmd2,cmd3, 各命令之间用分号隔开, 最后一个命令后必须有分号, 第一条命令和左括号之间必须用空格隔开。 对 {} 和 () 而言, 括号中的重定向符只影响该条命令，而括号外的重定向符影响到括号中的所有命令。 1.10 Shell特殊变量 `$0, $#, $*, $@, $?, ### 和命令行参数 变量 含义 $0 当前脚本的文件名。 $n 传递给脚本或函数的参数。n是一个数字，表示几个参数。 $# 传递给脚本或函数的参数个数。 $* 传递给脚本或函数的所有参数。 $@ 传递给脚本或函数的所有采纳数。被双引号(“ “)包含是，与$* 稍有不同。 $? 上个命令的退出状态，或函数的返回值。 $$ 当前shell进程ID。对于shell脚本，就是这个脚本所在的进程ID。 1.10.1 命令行参数运行脚本时传递给脚本的参数称为命令行参数。命令行参数用 $n 表示，例如，$1 表示第一个参数，$2 表示第二个参数，依次类推。 1.10.2 $* 和 $@ 的区别$* 和 $@ 都表示传递给函数或脚本的所有参数，不被双引号(“ “)包含时，都以&quot;$1&quot; &quot;$2&quot; … &quot;$n&quot; 的形式输出所有参数。 但是当它们被双引号(“ “)包含时，&quot;$*&quot; 会将所有的参数作为一个整体，以&quot;$1 $2 … $n&quot; 的形式输出所有参数；&quot;$@&quot; 会将各个参数分开，以 &quot;$1&quot; &quot;$2&quot; … &quot;$n&quot;的形式输出所有参数。 1.10.3 退出状态$? 可以获取上一个命令的退出状态。所谓退出状态，就是上一个命令执行后的返回结果。 退出状态是一个数字，一般情况下，大部分命令执行成功会返回 0，失败返回 1。 不过，也有一些命令返回其他值，表示不同类型的错误。 $? 也可以表示函数的返回值，此处不展开。 1.11 Shell重定向1、重定向符号 1234&gt; 输出重定向到一个文件或设备 覆盖原来的文件&gt;! 输出重定向到一个文件或设备 强制覆盖原来的文件&gt;&gt; 输出重定向到一个文件或设备 追加原来的文件&lt; 输入重定向到一个程序 2、标准输入刷出 1234在 bash 命令执行的过程中，主要有三种输出入的状况，分别是：1. 标准输入；代码为 0 ；或称为 stdin ；使用的方式为 &lt;2. 标准输出：代码为 1 ；或称为 stdout；使用的方式为 1&gt;3. 错误输出：代码为 2 ；或称为 stderr；使用的方式为 2&gt; 3、使用实例 12345678910111213# &amp; 是一个描述符，如果1或2前不加&amp;，会被当成一个普通文件。# 1&gt;&amp;2 意思是把标准输出重定向到标准错误.# 2&gt;&amp;1 意思是把标准错误输出重定向到标准输出。# &amp;&gt;filename 意思是把标准输出和标准错误输出都重定向到文件filename中$ cmd &lt;&gt; file # 以读写方式打开文件 file$ cmd &gt;&amp;n # 将 cmd 的输出发送到文件描述符 n$ cmd m&gt;&amp;n # 将本该输出到文件描述符 m 的内容, 发送到文件描述符 n$ cmd m&lt;&amp;n # 除了本该从文件描述符 m 处获取输入，改为从文件描述符 n 处获取$ cmd &gt;&amp;- # 关闭标准输出$ cmd &lt;&amp;- # 关闭标准输入$ cmd &gt;&amp; file # 将标准输出和标准错误都发送到文件 file $ cmd &amp;&gt; file # 作用同上, 更好的格式 要在终端中打印stdout，同时将它重定向到一个文件中，那么可以这样使用tee 。 12345678# 用法：command | tee FILE1 FILE2$ cat a* | tee out.txt | cat -n# 默认情况下， tee命令会将文件覆盖，但它提供了一个-a选项，用于追加内容$ cat a* | tee -a out.txt | cat –n# 我们可以使用stdin作为命令参数。只需要将-作为命令的文件名参数即可# 用法：$ cmd1 | cmd2 | cmd -$ echo who is this | tee - 1.12 Shell数组和关联数组1.12.1 简介数组是Shell脚本非常重要的组成部分，它借助索引将多个独立的独立的数据存储为一个集合。普通数组只能使用整数作为数组索引，关联数组不仅可以使用整数作为索引，也可以使用字符串作为索引。通常情况下，使用字符串做索引更容易被人们理解。Bash从4.0之后开始引入关联数组。 1.12.2 定义打印普通数组数组的方法有如下几种： 1234567#在一行上列出所有元素$ array_var=(1 2 3 4 5 6)#以“索引-值”的形式一一列出$ array_var[0]="test1"$ array_var[1]="test2"$ array_var[2]="test3" 注意：第一种方法要使用圆括号，否则后面会报错。 数组元素的方法有如下几种： 123456$ echo $&#123;array_var[0]&#125; #输出结果为 test1$ index=2$ echo $&#123;array_var[$index]&#125; #输出结果为 test3$ echo $&#123;array_var[*]&#125; #输出所有数组元素$ echo $&#123;array_var[@]&#125; #输出所有数组元素$ echo $&#123;#array_var[*]&#125; #输出值为 3 注意：在ubuntu 14.04中，shell脚本要以#!/bin/bash开头，且执行脚本的方式为 bash test.sh。 1.12.3 定义打印关联数组定义关联数组在关联数组中，可以使用任何文本作为数组索引。定义关联数组时，首先需要使用声明语句将一个变量声明为关联数组，然后才可以在数组中添加元素，过程如下： 12345678$ declare -A ass_array #声明一个关联数组$ ass_array=(["index1"]=index1 ["index2"]=index2)#内嵌“索引-值”列表法$ ass_array["index3"]=index3$ ass_array["index4"]=index4$ echo $&#123;ass_array["index1"]&#125; #输出为index1$ echo $&#123;ass_array["index4"]&#125;$ echo $&#123;!ass_array[*]&#125; #输出索引列表$ echo $&#123;!ass_array[@]&#125; #输出索引列表 注意：对于普通数组，使用上面的方法依然可以列出索引列表，在声明关联数组以及添加数组元素时，都不能在前面添加美元符$。 1.13 使用别名alias命令的作用只是暂时的。一旦关闭当前终端，所有设置过的别名就失效了。为了使别名设置一直保持作用，可以将它放入~/.bashrc文件中。因为每当一个新的shell进程生成时，都会执行 ~/.bashrc中的命令。 1$ alias install='sudo apt-get install' 1.14 获取、设置日期和延时时间方面 : 1234567891011121314% : 印出% %n : 下一行%t : 跳格%H : 小时(00..23)%I : 小时(01..12)%k : 小时(0..23)%l : 小时(1..12)%M : 分钟(00..59)%p : 显示本地 AM 或 PM%r : 直接显示时间 (12 小时制，格式为 hh:mm:ss [AP]M)%s : 从 1970 年 1 月 1 日 00:00:00 UTC 到目前为止的秒数 %S : 秒(00..61)%T : 直接显示时间 (24 小时制)%X : 相当于 %H:%M:%S%Z : 显示时区 日期方面 : 12345678910111213141516171819%a : 星期几 (Sun..Sat)%A : 星期几 (Sunday..Saturday)%b : 月份 (Jan..Dec)%B : 月份 (January..December)%y : 年份的最后两位数字 (00.99)%Y : 完整年份 (0000..9999)%c : 直接显示日期与时间%d : 日 (01..31)%D : 直接显示日期 (mm/dd/yy)%h : 同 %b%j : 一年中的第几天 (001..366)%m : 月份 (01..12)%U : 一年中的第几周 (00..53) (以 Sunday 为一周的第一天的情形)%w : 一周中的第几天 (0..6)%W : 一年中的第几周 (00..53) (以 Monday 为一周的第一天的情形)%x : 直接显示日期 (mm/dd/yy) 若是不以加号作为开头，则表示要设定时间，而时间格式为 MMDDhhmm[[CC]YY][.ss]，其中： 1234567MM 为月份，DD 为日，hh 为小时，mm 为分钟，CC 为年份前两位数字，YY 为年份后两位数字，ss 为秒数 参数 : -d datestr : 显示 datestr 中所设定的时间 (非系统时间) –help : 显示辅助讯息 -s datestr : 将系统时间设为 datestr 中所设定的时间 -u : 显示目前的格林威治时间 –version : 显示版本编号 例子： 12345$ date # 获取日期$ date +%s # 打印纪元时$ date "+%d %B %Y" # 用格式串结合 + 作为date命令的参数，可以按照你的选择打印出对应格式的日期20 May 2010$ date -s "21 June 2009 11:01:22" # 设置日期和时间 1.15 脚本调试1.15.1使用选项–x，启用shell脚本的跟踪调试功能1$ bash -x script.sh 1.15.2 使用set -x和set +x对脚本进行部分调试123456789#!/bin/bash#文件名: debug.shfor i in &#123;1..6&#125;do set -x echo $i set +xdoneecho "Script executed" set –x：在执行时显示参数和命令。 set +x：禁止调试。 set –v：当命令进行读取时显示输入。 set +v：禁止打印输入。 1.15.3 通过传递 _DEBUG环境变量调试123456789#!/bin/bashfunction DEBUG()&#123; [ "$_DEBUG" == "on" ] &amp;&amp; $@ || :&#125;for i in &#123;1..10&#125;do DEBUG echo $idone 可以将调试功能置为”on”来运行上面的脚本： 1$ _DEBUG=on ./script.sh 我们在每一个需要打印调试信息的语句前加上DEBUG。如果没有把 _DEBUG=on传递给脚本，那么调试信息就不会打印出来。在Bash中，命令 : 告诉shell不要进行任何操作。 1.15.4 利用shebang来进行调试shebang的妙用把shebang从 #!/bin/bash 改成 #!/bin/bash -xv，这样一来，不用任何其他选项就可以启用调试功能了。 1.16 函数参数1234567$0 # 脚本名$1 # 第一个参数$2 # 第二个参数$n # 第n个参数"$@" # 被扩展成 "$1" "$2" "$3"等"$*" # 被扩展成 "$1c$2c$3"，其中c是IFS的第一个字符"$@" 要比"$*"用得多。由于 "$*"将所有的参数当做单个字符串，因此它很少被使用。 导出函数： 函数也能像环境变量一样用export导出，如此一来，函数的作用域就可以扩展到子进程中，例如： 1export -f fname 1.17 read命令123456789101112# 从输入中读取n个字符并存入变量$ read -n 2 var$ echo $var# 用无回显的方式读取密码$ read -s var# 显示提示信息$ read -p "Enter input:" var# 在特定时(秒)限内读取输入$ read -t timeout var 1.18 条件比较与测试123456789101112131415# if条件if conditionthen commandsfi# else if和elseif conditionthen commandselse if condition then commandselse commandsfi if的条件判断部分可能会变得很长，但可以用逻辑运算符将它变得简洁一些： 12[ condition ] &amp;&amp; action # 如果condition为真，则执行action[ condition ] || action # 如果condition为假，则执行action &amp;&amp; 是逻辑与运算符， || 是逻辑或运算符。编写Bash脚本时，这是一个很有用的技巧。现在来了解一下条件和比较操作。 算术比较： -gt ：大于。 -lt ：小于。 -ge ：大于或等于。 -le ：小于或等于。 可以按照下面的方法结合多个条件进行测试： 12[ $var1 -ne 0 -a $var2 -gt 2 ] #使用逻辑与-a[ $var1 -ne 0 -o var2 -gt 2 ] #逻辑或 -o 文件系统相关测试： 我们可以使用不同的条件标志测试不同的文件系统相关的属性。 [ -f $file_var ] ：如果给定的变量包含正常的文件路径或文件名，则返回真。 [ -x $var ] ：如果给定的变量包含的文件可执行，则返回真。 [ -d $var ] ：如果给定的变量包含的是目录，则返回真。 [ -e $var ] ：如果给定的变量包含的文件存在，则返回真。 [ -c $var ] ：如果给定的变量包含的是一个字符设备文件的路径，则返回真。 [ -b $var ] ：如果给定的变量包含的是一个块设备文件的路径，则返回真。 [ -w $var ] ：如果给定的变量包含的文件可写，则返回真。 [ -r $var ] ：如果给定的变量包含的文件可读，则返回真。 [ -L $var ] ：如果给定的变量包含的是一个符号链接，则返回真。 字符串比较： 使用字符串比较时，最好用双中括号，因为有时候采用单个中括号会产生错误，所以最好避开它们。 可以用下面的方法检查两个字符串，看看它们是否相同。 [[ $str1 = $str2 ]]：当str1等于str2时，返回真。也就是说， str1和str2包含的文本是一模一样的。 [[ $str1 == $str2 ]] ：这是检查字符串是否相等的另一种写法。 也可以检查两个字符串是否不同。 [[ $str1 != $str2 ]] ：如果str1和str2不相同，则返回真。 我们还可以检查字符串的字母序情况，具体如下所示。 [[ $str1 &gt; $str2 ]] ：如果str1的字母序比str2大，则返回真。 [[ $str1 &lt; $str2 ]] ：如果str1的字母序比str2小，则返回真。 [[ -z $str1 ]] ：如果str1包含的是空字符串，则返回真。 [[ -n $str1 ]] ：如果str1包含的是非空字符串，则返回真。 使用逻辑运算符 &amp;&amp; 和 || 能够很容易地将多个条件组合起来： 1234if [[ -n $str1 ]] &amp;&amp; [[ -z $str2 ]] then commandsfi test命令可以用来执行条件检测。用test可以避免使用过多的括号。之前讲过的[]中的测试条件同样可以用于test命令。 123if [ $var -eq 0 ]; then echo "True"; fi# 也可以写成：if test $var -eq 0 ; then echo "True"; fi 补充内容1. 利用子shell生成一个独立的进程子shell本身就是独立的进程。可以使用 ( )操作符来定义一个子shell ： 123pwd;(cd /bin; ls);pwd; 2. 无限循环的实例1repeat() &#123; while true; do $@ &amp;&amp; return; done &#125; 工作原理： 函数repeat，它包含了一个无限while循环，该循环执行以参数形式（通过 $@ 访问）传入函数的命令。如果命令执行成功，则返回，进而退出循环。 一种更快的做法 ： 在大多数现代系统中， true 是作为 /bin 中的一个二进制文件来实现的。这就意味着每执行一次while循环， shell就不得不生成一个进程。如果不想这样，可以使用shell内建的 :命令，它总是会返回为0的退出码： 1repeat() &#123; while :; do $@ &amp;&amp; return; done &#125; 尽管可读性不高，但是肯定比第一种方法快。 2. 命令之乐2.1 cat命令123456# 摆脱多余的空白行$ cat -s file# 显示行号$ cat -n file# -n甚至会为空白行加上行号。如果你想跳过空白行，那么可以使用选项-b。 2.2 find命令123456# 列出当前目录及子目录下所有的文件和文件夹$ find base_path$ find . -print# -print指明打印出匹配文件的文件名（路径）。当使用 -print时， '\n'作为用于对输出的文件名进行分隔。就算你忽略-print， find命令仍会打印出文件名。# -print0指明使用'\0'作为匹配的文件名之间的定界符。 1、find命令有一个选项 -iname（忽略字母大小写） 12345$ lsexample.txt EXAMPLE.txt file.txt$ find . -iname "example*" -print./example.txt./EXAMPLE.txt 2、如果想匹配多个条件中的一个，可以采用OR条件操作 : 12345$ lsnew.txt some.jpg text.pdf$ find . \( -name "*.txt" -o -name "*.pdf" \) -print./text.pdf./new.txt 3、选项-path的参数可以使用通配符来匹配文件路径。 -name 总是用给定的文件名进行匹配。-path 则将文件路径作为一个整体进行匹配。例如 : 123$ find /home/users -path "*/slynux/*" -print/home/users/list/slynux.txt/home/users/slynux/eg.css 4、选项 -regex 的参数和 -path 的类似，只不过 -regex 是基于正则表达式来匹配文件路径的。 12345678$ lsnew.PY next.jpg test.py$ find . -regex ".*\(\.py\|\.sh\)$"./test.py# 类似地， -iregex可以让正则表达式忽略大小写。例如：$ find . -iregex ".*\(\.py\|\.sh\)$"./test.py./new.PY 5、find也可以用“!”否定参数的含义。例如： 1234567$ lslist.txt new.PY new.txt next.jpg test.py$ find . ! -name "*.txt" -print../next.jpg./test.py./new.PY 6、基于目录深度的搜索 123456# 深度选项-maxdepth和 -mindepth来限制find命令遍历的目录深度# 下列命令将find命令向下的最大深度限制为1:$ find . -maxdepth 1 -name "f*" -print# 打印出深度距离当前目录至少两个子目录的所有文件:$ find . -mindepth 2 -name "f*" -print 注：-maxdepth和-mindepth应该作为find的第三个参数出现。如果作为第4个或之后的参数，就可能会影响到find的效率，因为它不得不进行一些不必要的检查。 根据文件类型搜索 7、根据文件类型搜索 -type 可以对文件搜索进行过滤 文件类型 类型参数 普通文件 f 符号链接 l 目录 d 字符设备 c 块设备 b 套接字 s FIFO p 8、根据文件时间进行搜索 访问时间（-atime）：用户最近一次访问文件的时间。 修改时间（-mtime）：文件内容最后一次被修改的时间。 变化时间（-ctime）：文件元数据（例如权限或所有权）最后一次改变的时间。 -atime、 -mtime、 -ctime可作为find的时间选项。它们可以用整数值指定，单位是天。这些整数值通常还带有 - 或 + ： - 表示小于， + 表示大于。 12345678# 打印出在最近7天内被访问过的所有文件：$ find . -type f -atime -7 -print# 打印出恰好在7天前被访问过的所有文件：$ find . -type f -atime 7 -print# 打印出访问时间超过7天的所有文件：$ find . -type f -atime +7 -print -atime、 -mtime以及-ctime都是基于时间的参数，其计量单位是“天”。还有其他一些基于时间的参数是以分钟作为计量单位的。这些参数包括： -amin（访问时间） -mmin（修改时间） -cmin（变化时间） 使用 -newer ，我们可以指定一个用于比较时间戳的参考文件，然后找出比参考文件更新的（更近的修改时间）所有文件 12# 找出比file.txt修改时间更近的所有文件：$ find . -type f -newer file.txt -print 9、基于文件大小的搜索 12345678$ find . -type f -size +2k# 大于2KB的文件$ find . -type f -size -2k# 小于2KB的文件$ find . -type f -size 2k# 大小等于2KB的文件 b —— 块（512字节） c —— 字节 w —— 字（2字节） k —— 1024字节 M —— 1024k字节 G —— 1024M字节 10、删除匹配的文件 -delete 可以用来删除find查找到的匹配文件。 12# 删除当前目录下所有的 .swp文件：$ find . -type f -name "*.swp" -delete 11、基于文件权限和所有权的匹配 12$ find . -type f -perm 644 -print# 打印出权限为644的文件 -perm指明find应该只匹配具有特定权限值的文件。 12、利用find执行命令或动作 find命令可以借助选项-exec与其他命名进行结合。 -exec算得上是find最强大的特性之一。 123$ find . -type f -user root -exec chown slynux &#123;&#125; \;# &#123;&#125;是一个与 -exec选项搭配使用的特殊字符串。对于每一个匹配的文件，&#123;&#125;会被替换成相应的文件名。 -exec 结合多个命令 : 我们无法在-exec参数中直接使用多个命令。它只能够接受单个命令，不过我们可以耍一个小花招。把多个命令写到一个shell脚本中（例如command.sh），然后在-exec中使用这个脚本： 1-exec ./commands.sh &#123;&#125; \; 13、让find跳过特定的目录 123$ find devel/source_path \( -name ".git" -prune \) -o \( -type f -print \)# 以上命令打印出不包括在.git目录中的所有文件的名称（路径）。 \( -name &quot;.git&quot; -prune \) 的作用是用于进行排除，它指明了 .git目录应该被排除在外，而\( -type f -print \) 指明了需要执行的动作。这些动作需要被放置在第二个语句块中（打印出所有文件的名称和路径）。 2.3 玩转xargsxargs 擅长将标准输入数据转换成命令行参数。 xargs 命令把从 stdin接收到的数据重新格式化，再将其作为参数提供给其他命令。 2.3.1 将多行输入转换成单行输出只需要将换行符移除，再用” “（空格）进行代替，就可以实现多行输入的转换。 123456$ cat example.txt # 样例文件1 2 3 4 5 67 8 9 1011 12$ cat example.txt | xargs1 2 3 4 5 6 7 8 9 10 11 12 2.3.2 将单行输入转换成多行输出指定每行最大的参数数量 n，我们可以将任何来自stdin的文本划分成多行，每行 n 个参数。 12345$ cat example.txt | xargs -n 31 2 34 5 67 8 910 11 12 2.3.3 定制定界符用 -d 选项为输入指定一个定制的定界符： 123456$ echo "splitXsplitXsplitXsplit" | xargs -d Xsplit split split split$ echo "splitXsplitXsplitXsplit" | xargs -d X -n 2split splitsplit split 在这里，我们明确指定X作为输入定界符，而在默认情况下， xargs采用内部字段分隔符（空格）作为输入定界符。 2.3.4 读取stdin，将格式化参数传递给命令-I 指定替换字符串，这个字符串在xargs扩展时会被替换掉。如果将 -I 与 xargs 结合使用，对于每一个参数，命令都会被执行一次。 12345678$ cat args.txtarg1arg2arg3$ cat args.txt | xargs -I &#123;&#125; ./cecho.sh -p &#123;&#125; -l-p arg1 -l #-p arg2 -l #-p arg3 -l # -I {} 指定了替换字符串。对于每一个命令参数，字符串 {} 都会被从stdin读取到的参数替换掉。 使用 -I 的时候，命令以循环的方式执行。 xargs和find算是一对死党。两者结合使用可以让任务变得更轻松。 不过人们通常却是以一种错误的组合方式使用它们。例如： 1$ find . -type f -name "*.txt" -print | xargs rm -f 这样做很危险。 有时可能会删除不必要删除的文件。 只要我们把 find 的输出作为 xargs 的输入，就必须将 -print0 与 find 结合使用，以字符null（&#39;\0&#39;）来分隔输出。 12345$ find . -type f -name "*.txt" -print0 | xargs -0 rm -f# xargs -0将\0作为输入定界符。$ find source_code_dir_path -type f -name "*.c" -print0 | xargs -0 wc -l# 统计源代码目录中所有C程序文件的行数 2.4 校验和与核实校验和（checksum）程序用来从文件中生成校验和密钥，然后利用这个校验和密钥核实文件的完整性。文件可以通过网络或任何存储介质分发到不同的地点。 最知名且使用最为广泛的校验和技术是md5sum和SHA-1。它们对文件内容使用相应的算法来生成校验和。 123456789101112$ md5sum filename68b329da9893e34099c7d8ad5cb9c940 filename$ md5sum filename &gt; file_sum.md5$ md5sum file1 file2 file3 ..$ md5sum -c file_sum.md5# 这个命令会输出校验和是否匹配的消息# 如果需要用所有的.md5信息来检查所有的文件，可以使用：$ md5sum -c *.md5 计算SAH-1串的命令是sha1sum。其用法和md5sum的非常相似。只需要把先前讲过的那些命令中的md5sum替换成sha1sum就行了，记住将输入文件名从file_sum.md5改为file_sum.sha1。 对目录进行校验： 123456789$ md5deep -rl directory_path &gt; directory.md5# -r使用递归的方式# -l使用相对路径。默认情况下， md5deep会输出文件的绝对路径# 或者也可以结合find来递归计算校验和：$ find directory_path -type f -print0 | xargs -0 md5sum &gt;&gt; directory.md5# 用下面的命令进行核实：$ md5sum -c directory.md5 2.4.1 加密工具与散列crypt、 gpg、 base64、 md5sum、 sha1sum 以及 openssl 的用法。 1）crypt是一个简单的加密工具，它从stdin接受一个文件以及口令作为输入，然后将加密数据输出到Stdout（因此要对输入、输出文件使用重定向）。 1234567$ crypt &lt;input_file &gt;output_fileEnter passphrase:# 它会要求输入一个口令。我们也可以通过命令行参数来提供口令。$ crypt PASSPHRASE &lt;input_file &gt;encrypted_file# 如果需要解密文件，可以使用：$ crypt PASSPHRASE -d &lt;encrypted_file &gt;output_file 2）gpg（GNU隐私保护）是一种应用广泛的工具，它使用加密技术来保护文件，以确保数据在送达目的地之前无法被读取。这里我们讨论如何加密、解密文件。 12345# 用gpg加密文件：$ gpg -c filename# 该命令采用交互方式读取口令，并生成filename.gpg。使用以下命令解密gpg文件：$ gpg filename.gpg# 该命令读取口令，然后对文件进行解密。 3）Base64是一组相似的编码方案，它将ASCII字符转换成以64为基数的形式，以可读的ASCII字符串来描述二进制数据。 base64命令可以用来编码/解码Base64字符串。要将文件编码为Base64格式，可以使用： 123456789$ base64 filename &gt; outputfile# 或者$ cat file | base64 &gt; outputfile# base64可以从stdin中进行读取。# 解码Base64数据：$ base64 -d file &gt; outputfile# 或者$ cat base64_file | base64 -d &gt; outputfile 4）md5sum与sha1sum都是单向散列算法，均无法逆推出原始数据。它们通常用于验证数据完整性或为特定数据生成唯一的密钥： 1234$ md5sum file8503063d5488c3080d4800ff50850dc9 file$ sha1sum file1ba02b66e2e557fede8f61b7df282cd0a27b816b file 这种类型的散列算法是存储密码的理想方案。密码使用其对应的散列值来存储。如果某个用户需要进行认证，读取该用户提供的密码并转换成散列值，然后将其与之前存储的散列值进行比对。如果相同，用户就通过认证，被允许访问；否则，就会被拒绝访问。 5）openssl 用openssl生成shadow密码。 shadow密码通常都是salt密码。所谓SALT就是额外的一个字符串，用来起一个混淆的作用，使加密更加不易被破解。 salt由一些随机位组成，被用作密钥生成函数的输入之一，以生成密码的salt散列值。 123$ opensslpasswd -1 -salt SALT_STRING PASSWORD$1$SALT_STRING$323VkWkSLHuhbt1zkSsUG.# 将SALT_STRING替换为随机字符串，并将PASSWORD替换成你想要使用的密码。 2.5 排序、唯一与重复1234567891011121314151617# 对一组文件进行排序：$ sort file1.txt file2.txt &gt; sorted.txt# 按照数字顺序进行排序：$ sort -n file.txt# 按照逆序进行排序：$ sort -r file.txt# 按照月份进行排序（依照一月，二月，三月……）：$ sort -M months.txt# 合并两个已排序过的文件：$ sort -m sorted1 sorted2# 找出已排序文件中不重复的行：$ sort file1.txt file2.txt | uniq 检查文件是否已经排序过： 12345678#!/bin/bash#功能描述：排序sort -C filename ;if [ $? -eq 0 ]; then echo Sorted;else echo Unsorted;fi -k 指定了排序应该按照哪一个键（key）来进行。键指的是列号，而列号就是执行排序时的依据。 -r 告诉sort命令按照逆序进行排序。例如： 12345678910111213# 依据第1列，以逆序形式排序$ sort -nrk 1 data.txt4 linux 10003 bsd 10002 winxp 40001 mac 2000# -nr表明按照数字，采用逆序形式排序# 依据第2列进行排序$ sort -k 2 data.txt3 bsd 10004 linux 10001 mac 20002 winxp 4000 有时文本中可能会包含一些像空格之类的不必要的多余字符。如果需要忽略这些字符，并以字典序进行排序，可以使用： 12$ sort -bd unsorted.txt# 选项-b用于忽略文件中的前导空白行，选项-d用于指明以字典序进行排序。 sort选项： 1234567891011121314151617181920212223-b：忽略每行前面开始出的空格字符；-c：检查文件是否已经按照顺序排序； -d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符； -f：排序时，将小写字母视为大写字母； -i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符；-m：将几个排序号的文件进行合并； -M：将前面3个字母依照月份的缩写进行排序； -n：依照数值的大小排序； -o&lt;输出文件&gt;：将排序后的结果存入制定的文件； -r：以相反的顺序来排序； -t&lt;分隔字符&gt;：指定排序时所用的栏位分隔字符； +&lt;起始栏位&gt;-&lt;结束栏位&gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。 uniq选项： 1234567891011-c或——count：在每列旁边显示该行重复出现的次数； -d或--repeated：仅显示重复出现的行列； -f&lt;栏位&gt;或--skip-fields=&lt;栏位&gt;：忽略比较指定的栏位； -s&lt;字符位置&gt;或--skip-chars=&lt;字符位置&gt;：忽略比较指定的字符； -u或——unique：仅显示出一次的行列； -w&lt;字符位置&gt;或--check-chars=&lt;字符位置&gt;：指定要比较的字符。 wc选项： 12345678910111213-c或--bytes或——chars：只显示Bytes数； # 统计字符数-l或——lines：只显示列数； # 统计行数-w或——words：只显示字数。 # 统计单词数# 当不使用任何选项执行wc时，它会分别打印出文件的行数、单词数和字符数：$ wc file1435 15763 112200# 使用-L选项打印出文件中最长一行的长度：$ wc file -L205 2.6 临时文件命名与随机数123456789101112131415161718# 创建临时文件：$ filename=`mktemp`$ echo $filename/tmp/tmp.8xvhkjF5fH# 创建临时目录：$ dirname=`mktemp -d`$ echo $dirnametmp.NI8xzW7VRX# 如果仅仅是想生成文件名，又不希望创建实际的文件或目录，方法如下：$ tmpfile=`mktemp -u`$ echo $tmpfile/tmp/tmp.RsGmilRpcT# 根据模板创建临时文件名：$mktemp test.XXXtest.2tc 如果提供了定制模板， X会被随机的字符（字母或数字）替换。注意， mktemp正常工作的前提是保证模板中只少要有3个X。 2.7 split 分割文件和数据1234# 将文件分割成多个大小为10KB的文件$ split -b 10k data.file$ lsdata.file xaa xab xac xad xae xaf xag xah xai xaj 上面的命令将data.file分割成多个文件，每一个文件大小为10KB。这些文件以xab、 xac、 xad的形式命名。这表明它们都有一个字母后缀。如果想以数字为后缀，可以另外使用-d参数。此外，使用 -a length可以指定后缀长度： 123$ split -b 10k data.file -d -a 4$ lsdata.file x0009 x0019 x0029 x0039 x0049 x0059 x0069 x0079 除了k（KB）后缀，我们还可以使用M（MB）、 G（GB）、 c（byte）、 w（word）等后缀。 12345678910# 为分割后的文件指定文件名前缀 $ split -b 10k data.file -d -a 4 split_file$ lsdata.file split_file0002 split_file0005 split_file0008 strtok.csplit_file0000 split_file0003 split_file0006 split_file0009split_file0001 split_file0004 split_file0007# 如果不想按照数据块大小，而是需要根据行数来分割文件的话，可以使用 -l no_of_lines：$ split -l 10 data.file# 分割成多个文件，每个文件包含10行 csplit。它能够依据指定的条件和字符串匹配选项对日志文件进行分割。 12345678910111213141516171819$ cat server.logSERVER-1[connection] 192.168.0.1 success[connection] 192.168.0.2 failed[disconnect] 192.168.0.3 pending[connection] 192.168.0.4 successSERVER-2[connection] 192.168.0.1 failed[connection] 192.168.0.2 failed[disconnect] 192.168.0.3 success[connection] 192.168.0.4 failedSERVER-3[connection] 192.168.0.1 pending[connection] 192.168.0.2 pending[disconnect] 192.168.0.3 pending[connection] 192.168.0.4 failed$ csplit server.log /SERVER/ -n 2 -s &#123;*&#125; -f server -b "%02d.log" ; rm server00.log$ lsserver01.log server02.log server03.log server.log 有关这个命令的详细说明如下。 /SERVER/ 用来匹配某一行，分割过程即从此处开始。 /[REGEX]/ 表示文本样式。包括从当前行（第一行）直到（但不包括）包含“SERVER”的匹配行。 {*} 表示根据匹配重复执行分割，直到文件末尾为止。可以用{整数}的形式来指定分割执行的次数。 -s 使命令进入静默模式，不打印其他信息。 -n 指定分割后的文件名后缀的数字个数，例如01、 02、 03等。 -f 指定分割后的文件名前缀（在上面的例子中， server就是前缀）。 -b 指定后缀格式。例如%02d.log，类似于C语言中printf的参数格式。在这里文件名=前缀+后缀=server + %02d.log。 因为分割后的第一个文件没有任何内容（匹配的单词就位于文件的第一行中），所以我们删除了server00.log。 2.7.1 根据扩展名切分文件名$、借助 % 操作符可以轻松将名称部分从 “名称.扩展名” 这种格式中提取出来。 12345file_jpg="sample.jpg"name=$&#123;file_jpg%.*&#125;echo File name is: $name输出结果：File name is: sample 将文件名的扩展名部分提取出来，这可以借助 # 操作符实现。 1234extension=$&#123;file_jpg#*.&#125;echo Extension is: jpg输出结果：Extension is: jpg ${VAR%.*} 的含义如下所述： 从 $VAR中删除位于 % 右侧的通配符（在前例中是.*）所匹配的字符串。通配符从右向左进行匹配。 给VAR赋值， VAR=sample.jpg。那么，通配符从右向左就会匹配到.jpg，因此，从 $VAR中删除匹配结果，就会得到输出sample。 %属于非贪婪（non-greedy）操作。它从右到左找出匹配通配符的最短结果。还有另一个操作符 %%，这个操作符与%相似，但行为模式却是贪婪的，这意味着它会匹配符合条件的最长的字符串。 操作符%%则用.*从右向左执行贪婪匹配（.fun.book.txt）。 ${VAR#*.} 的含义如下所述：从$VAR中删除位于#右侧的通配符（即在前例中使用的*.）所匹配的字符串。通配符从左向右进行匹配。和 %% 类似， #也有一个相对应的贪婪操作符 ##。 ##从左向右进行贪婪匹配，并从指定变量中删除匹配结果。 这里有个能够提取域名不同部分的实用案例。假定 URL=&quot;www.google.com&quot;： 12345678$ echo $&#123;URL%.*&#125; # 移除.*所匹配的最右边的内容www.google$ echo $&#123;URL%%.*&#125; # 将从右边开始一直匹配到最左边的*.移除（贪婪操作符）www$ echo $&#123;URL#*.&#125; # 移除*.所匹配的最左边的内容google.com$ echo $&#123;URL##*.&#125; # 将从左边开始一直匹配到最右边的*.移除（贪婪操作符）com 2.8 批量重命名和移动123456789101112131415# 将 *.JPG更名为 *.jpg：$ rename *.JPG *.jpg# 将文件名中的空格替换成字符“_”：$ rename 's/ /_/g' *# 转换文件名的大小写：$ rename 'y/A-Z/a-z/' *$ rename 'y/a-z/A-Z/' *# 将所有的 .mp3文件移入给定的目录：$ find path -type f -name "*.mp3" -exec mv &#123;&#125; target_dir \;# 将所有文件名中的空格替换为字符“_”：$ find path -type f -exec rename 's/ /_/g' &#123;&#125; \; 3 以文件之名3.1 生成任意大小的文件1$ dd if=/dev/zero of=junk.data bs=1M count=1 该命令会创建一个1MB大小的文件junk.data。来看一下命令参数： if代表输入文件（input file），of代表输出文件（output file）， bs代表以字节为单位的块大小（block size）， count代表需要被复制的块数。 使用dd命令时一定得留意，该命令运行在设备底层。要是你不小心出了岔子，搞不好会把磁盘清空或是损坏数据。所以一定要反复检查dd命令所用的语法是否正确，尤其是参数of=。 单元大小 代码 字节（1B） c 字（2B） w 块（512B） b 千字节（1024B） k 兆字节（1024KB） M 吉字节（1024MB） G ls -lS 对当前目录下的所有文件按照文件大小进行排序，并列出文件的详细信息。 3.2 文件权限、所有权和粘滞位用命令ls -l可以列出文件的权限： 123-rw-r--r-- 1 slynux slynux 2497 2010-02-28 11:22 bot.pydrwxr-xr-x 2 slynux slynux 4096 2010-05-27 14:31 a.py-rw-r--r-- 1 slynux slynux 539 2010-02-10 09:11 cl.pl -—— 普通文件。 d —— 目录。 c —— 字符设备。 b —— 块设备。 l —— 符号链接。 s —— 套接字。 p —— 管道。 123456789101112# 更改所有权$ chown user.group filename# 设置粘滞位# 要设置粘滞位，利用chmod将 +t应用于目录：$ chmod a+t directory_name# 以递归的方式设置权限$ chmod 777 . -R# 以递归的方式设置所有权$ chown user.group . -R 3.3 创建不可修改的文件chattr能够将文件设置为不可修改。 12345# 使用下列命令将一个文件设置为不可修改：$ chattr +i file# 如果需要使文件恢复可写状态，移除不可修改属性即可：$ chattr -i file 3.4 查找符号链接及其指向目标12345678910111213141516171819# 创建符号链接：$ ln -s target symbolic_link_name例如：$ ln -l -s /var/www/ ~/web#这个命令在已登录用户的home目录中创建了一个名为Web的符号链接。该链接指向/var/www。# 使用下面的命令来验证是否创建链接：$ ls -l weblrwxrwxrwx 1 slynux slynux 8 2010-06-25 21:34 web -&gt; /var/www# 打印出当前目录下的符号链接：$ ls -l | grep "^l"# 使用find打印当前目录以及子目录下的符号链接：$ find . -type l -print# 使用readlink打印出符号链接所指向的目标路径：$ readlink web/var/www 3.5 列举文件类型统计信息12345678# 用下面的命令打印文件类型信息：$ file filename$ file /etc/passwd/etc/passwd: ASCII text# 打印不包括文件名在内的文件类型信息：$ file -b filenameASCII text 3.6 使用环回文件1234567891011121314151617181920212223242526# 下面的命令可以创建一个1GB大小的文件：$ dd if=/dev/zero of=loobackfile.img bs=1G count=11024+0 records in1024+0 records out1073741824 bytes (1.1 GB) copied, 37.3155 s, 28.8 MB/s# 你会发现创建好的文件大小超过了1GB。这是因为硬盘作为块设备，其分配存储空间时是按照块大小的整数倍来进行的。# 用mkfs命令将1GB的文件格式化成ext4文件系统：$ mkfs.ext4 loopbackfile.img# 使用下面的命令检查文件系统：$ file loobackfile.imgloobackfile.img: Linux rev 1.0 ext4 filesystem data,UUID=c9d56c42-f8e6-4cbd-aeab-369d5056660a (extents) (large files) (huge files)# 现在就可以挂载环回文件了：$ mkdir /mnt/loopback$ mount -o loop loopbackfile.img /mnt/loopback# -o loop用来挂载环回文件系统。# 我们也可以手动来操作：$ losetup /dev/loop1 loopbackfile.img$ mount /dev/loop1 /mnt/loopback# 使用下面的方法进行卸载（umount）：$ umount mount_point 3.7 生成 ISO 文件及混合型 ISO123456789#用下面的命令从/dev/cdrom创建一个ISO镜像：$ cat /dev/cdrom &gt; image.iso#尽管可以奏效。但创建ISO镜像最好的方法还是使用dd工具：$ dd if=/dev/cdrom of=image.iso# mkisofs命令用于创建ISO文件系统。$ mkisofs -V "Label" -o image.iso source_dir/# 选项 -o指定了ISO文件的路径。 source_dir是作为ISO文件内容来源的目录路径，选项 -V指定了ISO文件的卷标。 3.8 diff命令12345678910111213141516171819202122232425262728293031323334- # 指定要显示多少行的文本。此参数必须与-c或-u参数一并使用。-a或--text # diff预设只会逐行比较文本文件。-b或--ignore-space-change # 不检查空格字符的不同。-B或--ignore-blank-lines # 不检查空白行。-c # 显示全部内文，并标出不同之处。-C或--context # 与执行"-c-"指令相同。-d或--minimal # 使用不同的演算法，以较小的单位来做比较。-D或ifdef # 此参数的输出格式可用于前置处理器巨集。-e或--ed # 此参数的输出格式可用于ed的script文件。-f或-forward-ed # 输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处。-H或--speed-large-files # 比较大文件时，可加快速度。-l或--ignore-matching-lines # 若两个文件在某几行有所不同，而这几行同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异。-i或--ignore-case # 不检查大小写的不同。-l或--paginate # 将结果交由pr程序来分页。-n或--rcs # 将比较结果以RCS的格式来显示。-N或--new-file # 在比较目录时，若文件A仅出现在某个目录中，预设会显示：Only in目录：文件A若使用-N参数，则diff会将文件A与一个空白的文件比较。-p # 若比较的文件为C语言的程序码文件时，显示差异所在的函数名称。-P或--unidirectional-new-file # 与-N类似，但只有当第二个目录包含了一个第一个目录所没有的文件时，才会将这个文件与空白的文件做比较。-q或--brief # 仅显示有无差异，不显示详细的信息。-r或--recursive # 比较子目录中的文件。-s或--report-identical-files # 若没有发现任何差异，仍然显示信息。-S或--starting-file # 在比较目录时，从指定的文件开始比较。-t或--expand-tabs # 在输出时，将tab字符展开。-T或--initial-tab # 在每行前面加上tab字符以便对齐。-u,-U或--unified= # 以合并的方式来显示文件内容的不同。-v或--version # 显示版本信息。-w或--ignore-all-space # 忽略全部的空格字符。-W或--width # 在使用-y参数时，指定栏宽。-x或--exclude # 不比较选项中所指定的文件或目录。-X或--exclude-from # 您可以将文件或目录类型存成文本文件，然后在=中指定此文本文件。-y或--side-by-side # 以并列的方式显示文件的异同之处。--help # 显示帮助。--left-column # 在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容。--suppress-common-lines # 在使用-y参数时，仅显示不同之处。 生成目录的差异信息 ： 1$ diff -Naur directory1 directory2 -N：将所有缺失的文件视为空文件。 -a：将所有文件视为文本文件。 -u：生成一体化输出。 -r：遍历目录下的所有文件。 12345678910111213# 生成patch文件$ diff -u version1.txt version2.txt &gt; version.patch# 用下列命令来进行修补：$ patch -p1 version1.txt &lt; version.patchpatching file version1.txt# version1.txt的内容现在和verson2.txt的内容一模一样。# 下面的命令可以撤销做出的修改：$ patch -p1 version1.txt &lt; version.patchpatching file version1.txtReversed (or previously applied) patch detected! Assume -R? [n] y# 修改被撤销 3.9 more、less、head与tail命令3.9.1 more文件内容输出查看工具123456789101112$ more [参数选项] [文件] # 参数如下： +num # 从第num行开始显示； -num # 定义屏幕大小，为num行； +/pattern # 从pattern 前两行开始显示； -c # 从顶部清屏然后显示； -d # 提示Press space to continue, 'q' to quit.（按空格键继续，按q键退出），禁用响铃功能； -l # 忽略Ctrl+l （换页）字符； -p # 通过清除窗口而不是滚屏来对文件进行换页。和-c参数有点相似； -s # 把连续的多个空行显示为一行； -u # 把文件内容中的下划线去掉退出more的动作指令是q 举例： 1234567891011# 显示提示，并从终端或控制台顶部显示；$ more -dc /etc/profile # 从profile的第4行开始显示；$ more +4 /etc/profile # 每屏显示4行；$ more -4 /etc/profile # 从profile中的第一个MAIL单词的前两行开始显示；$ more +/MAIL /etc/profile more 的动作指令： 123456789Enter # 向下n行，需要定义，默认为1行； Ctrl+f # 向下滚动一屏； 空格键 # 向下滚动一屏； Ctrl+b # 返回上一屏； = # 输出当前行的行号； :f # 输出文件名和当前行的行号； v # 调用vi编辑器； ! 命令 # 调用Shell，并执行命令； q # 退出more当我们查看某一文件时，想调用vi来编辑它，不要忘记了v动作指令，这是比较方便的； 其它命令通过管道和more结合的运用例子： 1$ ls -l /etc |more 3.9.2 less查看文件内容工具12345678910-c # 从顶部（从上到下）刷新屏幕，并显示文件内容。而不是通过底部滚动完成刷新； -f # 强制打开文件，二进制文件显示时，不提示警告； -i # 搜索时忽略大小写；除非搜索串中包含大写字母； -I # 搜索时忽略大小写，除非搜索串中包含小写字母； -m # 显示读取文件的百分比； -M # 显法读取文件的百分比、行号及总行数； -N # 在每行前输出行号； -p # pattern 搜索pattern；比如在/etc/profile搜索单词MAIL，就用 less -p MAIL /etc/profile -s # 把连续多个空白行作为一个空白行显示； -Q # 在终端下不响铃； less的动作命令： 123456789101112131415回车键 # 向下移动一行； y # 向上移动一行； 空格键 # 向下滚动一屏； b # 向上滚动一屏； d # 向下滚动半屏； h # less的帮助； u # 向上洋动半屏； w # 可以指定显示哪行开始显示，是从指定数字的下一行显示；比如指定的是6，那就从第7行显示； g # 跳到第一行； G # 跳到最后一行； p # n% 跳到n%，比如 10%，也就是说比整个文件内容的10%处开始显示； /pattern # 搜索pattern ，比如 /MAIL表示在文件中搜索MAIL单词； v # 调用vi编辑器； q # 退出less !command # 调用SHELL，可以运行命令；比如!ls 显示当前列当前目录下的所有文件； 3.9.3 headhead 是显示一个文件的内容的前多少行： 1$ head -n 10 /etc/profile 3.9.4 tailtail 是显示一个文件的内容的最后多少行： 1$ tail -n 5 /etc/profile 3.10 getopts 参数解析3.10.1 getopts（shell内置命令）1234$ type getoptgetopt 是 /usr/bin/getopt$ type getopts getopts 是 shell 内建 getopts不能直接处理长的选项（如：–prefix=/home等） 关于getopts的使用方法，可以man bash 搜索getopts。 getopts有两个参数，第一个参数是一个字符串，包括字符和“：”，每一个字符都是一个有效的选项，如果字符后面带有“：”，表示这个字符有自己的参数。getopts从命令中获取这些参数，并且删去了“-”，并将其赋值在第二个参数中，如果带有自己参数，这个参数赋值在 $OPTARG中。提供getopts的shell内置了 $OPTARG 这个变变，getopts修改了这个变量。 这里变量 $OPTARG 存储相应选项的参数，而 $OPTIND 总是存储原始 $* 中下一个要处理的元素位置。while getopts &quot;:a:bc&quot; opt #第一个冒号表示忽略错误；字符后面的冒号表示该选项必须有自己的参数 getopts后面的字符串就是可以使用的选项列表，每个字母代表一个选项，后面带:的意味着选项除了定义本身之外，还会带上一个参数作为选项的值，比如d:在实际的使用中就会对应-d 30，选项的值就是30；getopts字符串中没有跟随:的是开关型选项，不需要再指定值，相当于true/false，只要带了这个参数就是true。如果命令行中包含了没有在getopts列表中的选项，会有警告信息，如果在整个getopts字符串前面也加上个:，就能消除警告信息了。 两个特殊变量： 12$OPTIND # 特殊变量，option index，会逐个递增, 初始值为1$OPTARG # 特殊变量，option argument，不同情况下有不同的值 例子： 1234567891011121314151617echo $*while getopts ":a:bc" optdo case $opt in a ) echo $OPTARG echo $OPTIND;; b ) echo "b $OPTIND";; c ) echo "c $OPTIND";; ? ) echo "error" exit 1;; esacdoneecho $OPTINDshift $(($OPTIND - 1))#通过shift $(($OPTIND - 1))的处理，$*中就只保留了除去选项内容的参数，可以在其后进行正常的shell编程处理了。echo $0echo $* 12345678$ ./getopts.sh -a 11 -b -c-a 11 -b -c113b 4c 55./getopts.sh 3.10.2 getopt（一个外部工具）具体用用法可以 man getopt -o 表示短选项，两个冒号表示该选项有一个可选参数，可选参数必须紧贴选项，如 -carg 而不能是 -c arg。 --long 表示长选项 例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#!/bin/bash# A small example program for using the new getopt(1) program.# This program will only work with bash(1)# An similar program using the tcsh(1) script. language can be found# as parse.tcsh# Example input and output (from the bash prompt):# ./parse.bash -a par1 'another arg' --c-long 'wow!*\?' -cmore -b " very long "# Option a# Option c, no argument# Option c, argument `more'# Option b, argument ` very long '# Remaining arguments:# --&gt; `par1'# --&gt; `another arg'# --&gt; `wow!*\?'# Note that we use `"$@"' to let each command-line parameter expand to a# separate word. The quotes around `$@' are essential!# We need TEMP as the `eval set --' would nuke the return value of getopt.#-o表示短选项，两个冒号表示该选项有一个可选参数，可选参数必须紧贴选项#如-carg 而不能是-c arg#--long表示长选项#"$@"在上面解释过# -n:出错时的信息# -- ：举一个例子比较好理解：#我们要创建一个名字为 "-f"的目录你会怎么办？# mkdir -f #不成功，因为-f会被mkdir当作选项来解析，这时就可以使用# mkdir -- -f 这样-f就不会被作为选项。TEMP=`getopt -o ab:c:: --long a-long,b-long:,c-long:: \ -n 'example.bash' -- "$@"`if [ $? != 0 ] ; then echo "Terminating..." &gt;&amp;2 ; exit 1 ; fi# Note the quotes around `$TEMP': they are essential!#set 会重新排列参数的顺序，也就是改变$1,$2...$n的值，这些值在getopt中重新排列过了eval set -- "$TEMP"#经过getopt的处理，下面处理具体选项。while true ; do case "$1" in -a|--a-long) echo "Option a" ; shift ;; -b|--b-long) echo "Option b, argument \`$2'" ; shift 2 ;; -c|--c-long) # c has an optional argument. As we are in quoted mode, # an empty parameter will be generated if its optional # argument is not found. case "$2" in "") echo "Option c, no argument"; shift 2 ;; *) echo "Option c, argument \`$2'" ; shift 2 ;; esac ;; --) shift ; break ;; *) echo "Internal error!" ; exit 1 ;; esacdoneecho "Remaining arguments:"for arg do echo '--&gt; '"\`$arg'" ;done 123456$ ./getopt.sh --b-long abc -a -c33 remainOption b, argument `abc'Option aOption c, argument `33'Remaining arguments:--&gt; `remain' 3.11 只列出目录的各种方法123456789101112# 使用ls –d：$ ls -d */# 使用grep结合ls –F：$ ls -F | grep "/$"# 当使用-F时，所有的输出项都会添加上一个代表文件类型的字符，如@、 *、 |等。目录对应的是 / 字符。我们用grep只过滤那些以 /$ 作为行尾标记的输出项。# 使用grep结合ls –l：$ ls -l | grep "^d"# 使用find：$ find . -type d -maxdepth 1 -print 3.12 使用pushd和popd进行快速定位使用pushd和popd时，可以无视cd命令。 123456789101112131415161718# 压入并切换路径：$ pushd /var/www# 再压入下一个目录路径：$ pushd /usr/src# 用下面的命令查看栈内容：$ dirs/usr/src /var/www ~ /usr/share /etc0 1 2 3 4# 当你想切换到列表中任意一个路径时，将每条路径从0到n进行编号，然后使用你希望切换到的路径编号，例如：$ pushd +3# 这条命令会将栈进行翻转并切换到目录 /use/share。# 要删除最后添加的路径并把当前目录更改为上一级目录，可以使用以下命令：$ popd# 用popd +num可以从列表中移除特定的路径。num是从左到右，从0到n开始计数的。 3.13 tree打印目录树1234567891011# 重点标记出匹配某种样式的文件：$ tree PATH -P "*.sh" # 用一个目录路径代替PATH|-- home| |-- packtpub| | `-- automate.sh# 重点标记出除符合某种样式之外的那些文件：$ tree path -I PATTERN# 使用 -h选项同时打印出文件和目录的大小：$ tree -h 4 让文件飞4.1 正则表达式 正则表达式 描述 示例 ^ 行起始标记 ^tux 匹配以tux起始的行 $ 行尾标记 tux$ 匹配以tux结尾的行 . 匹配任意一个字符 Hack.匹配Hackl和Hacki，它只能匹配单个字符 [ ] 匹配包含在 [字符] 之中的任意一个字符 coo[kl] 匹配cook或cool [ ^ ] 匹配除 [^字符] 之外的任意一个字符 9[^01]匹配92、 93，但是不匹配91或90 [ - ] 匹配 [ ] 中指定范围内的任意一个字符 [1-5] 匹配从1～5的任意一个数字 ? 匹配之前的项1次或0次 colou?r 匹配color或colour，但是不能匹配colouur + 匹配之前的项1次或多次 Rollno-9+ 匹配Rollno-99、Rollno-9，但是不能匹配Rollno- * 匹配之前的项0次或多次 co*l 匹配cl、 col、 coool等 ( ) 创建一个用于匹配的子串 ma(tri)?x 匹配max或maxtrix {n} 匹配之前的项n次 [0-9]{3} 匹 配 任 意 一 个 三 位 数 ， [0-9]{3} 可 以 扩 展 为[0-9][0-9][0-9] {n, } 之前的项至少需要匹配n次 [0-9]{2,} 匹配任意一个两位或更多位的数字 {n, m} 指定之前的项所必需匹配的最小次数和最大次数 [0-9]{2,5} 匹配从两位数到五位数之间的任意一个数字 | 交替——匹配 | 两边的任意一项 Oct (1st | 2nd) 匹配Oct 1st或Oct 2nd \ 转义符可以将上面介绍的特殊字符进行转义 a\.b 匹配a.b，但不能匹配ajb。通过在 . 之间加上前缀 \ ，从而忽略了 . 的特殊意义 正则表达式 描述 [:alnum:] 所有的字母和数字 [:alpha:] 所有字母 [:blank:] 水平制表符，空白等 [:cntrl:] 所有控制字符 [:digit:] 所有的数字 [:graph:] 所有可打印字符，不包括空格 [:lower:] 所有的小写字符 [:print:] 所有可打印字符，包括空格 [:punct:] 所有的标点字符 [:space:] 所有的横向或纵向的空白 [:upper:] 所有大写字母 4.2 grep命令1234567891011121314151617181920212223242526-a # 不要忽略二进制的数据。-A&lt;显示列数&gt; # 除了显示符合范本样式的那一列之外，并显示该列之后的内容。-b # 在显示符合范本样式的那一列之前，标示出该列第一个字符的位编号。-B&lt;显示列数&gt; # 除了显示符合范本样式的那一列之外，并显示该列之前的内容。-c # 计算符合范本样式的列数。-C&lt;显示列数&gt;或-&lt;显示列数&gt; # 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; # 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。-e&lt;范本样式&gt; # 指定字符串做为查找文件内容的范本样式。-E # 将范本样式为延伸的普通表示法来使用。-f&lt;范本文件&gt; # 指定范本文件，其内容含有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每列一个范本样式。-F # 将范本样式视为固定字符串的列表。-G # 将范本样式视为普通的表示法来使用。-h # 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H # 在显示符合范本样式的那一列之前，表示该列所属的文件名称。-i # 忽略字符大小写的差别。-l # 列出文件内容符合指定的范本样式的文件名称。-L # 列出文件内容不符合指定的范本样式的文件名称。-n # 在显示符合范本样式的那一列之前，标示出该列的列数编号。-q # 不显示任何信息。-r # 此参数的效果和指定“-d recurse”参数相同。-s # 不显示错误信息。-v # 反转查找。-V # 显示版本信息。-w # 只显示全字符合的列。-x # 只显示全列符合的列。-o # 只输出文件中匹配到的部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# 单个grep命令也可以对多个文件进行搜索：$ grep "match_text" file1 file2 file3 ...# grep -E选项——这意味着使用扩展（extended）正则表达式：$ grep -E "[a-z]+" filename# 或者$ egrep "[a-z]+" filename# 只输出文件中匹配到的文本部分，可以使用选项 -o：$ echo this is a line. | egrep -o "[a-z]+\."line.# 要打印除包含match_pattern行之外的所有行，选项-v可以将匹配结果进行反转（invert）。可使用：$ grep -v match_pattern file# 统计文件或文本中包含匹配字符串的行数：$ grep -c "text" filename10# 需要注意的是-c只是统计匹配行的数量，并不是匹配的次数。。例如：$ echo -e "1 2 3 4\nhello\n5 6" | egrep -c "[0-9]"2# 要文件中统计匹配项的数量，可以使用下面的技巧：$ echo -e "1 2 3 4\nhello\n5 6" | egrep -o "[0-9]" | wc -l6# 打印模式匹配所位于的字符或字节偏移：$ echo gnu is not unix | grep -b -o "not"7:not# 选项 -b总是和 -o配合使用。# 搜索多个文件并找出匹配文本位于哪一个文件中：$ grep -l linux sample1.txt sample2.txtsample1.txtsample2.txt# 和-l相反的选项是-L，它会返回一个不匹配的文件列表。# grep的选项-R和-r功能一样。# 忽略样式中的大小写$ echo hello world | grep -i "HELLO"hello# grep匹配多个样式$ echo this is a line of text | grep -e "this" -e "line" -othisline# 在grep搜索中指定或排除文件$ grep "main()" . -r --include *.&#123;c,cpp&#125; # 目录中递归搜索所有的 .c和 .cpp文件# 如果需要排除目录，可以使用 --exclude-dir选项。# 如果需要从文件中读取所需排除的文件列表，使用--exclude-from FILE。# 使用0值字节作为后缀的grep与xargs，为了指明输入的文件名是以0值字节（\0）作为终止符，需要在xargs中使用-0。# grep使用-Z选项输出以0值字节作为终结符的文件名（\0）。$ grep "test" file* -lZ | xargs -0 rm# -Z通常和 -l结合使用。# grep的静默输出# grep的静默选项（-q）来实现。在静默模式中， grep命令不会输出任何内容。它仅是运行命令，然后根据命令执行成功与否返回退出状态。# 要打印匹配某个结果之后的3行，使用 -A选项：$ seq 10 | grep 5 -A 35678# 要打印匹配某个结果之前的3行，使用 -B选项：$ seq 10 | grep 5 -B 32345# 要打印匹配某个结果之前以及之后的3行，使用-C选项：$ seq 10 | grep 5 -C 32345678# 如果有多个匹配，那么使用--作为各部分之间的定界符：$ echo -e "a\nb\nc\na\nb\nc" | grep a -A 1ab--ab 4.3 cut 按列切分文件1234# 显示第2列和第3列：$ cut -f 2,3 filename# 记法 范围 N - 从第N个字节，字符或字段到行尾 N - M 从第N个字节，字符或字段到第M个（包括第M个在内）字节、字符或字段 - M 第1个字节，字符或字段到第M个（包括第M个在内）字节、字符或字段 结合下列选项将字段指定为某个范围内的字节或字符 ： -b ：表示字节 -c ：表示字符 -f ：用于定义字段 123456789101112131415161718$ cat range_fields.txtabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxy# 打印第1个到第5个字符：$ cut -c1-5 range_fields.txtabcdeabcdeabcdeabcde# 打印前2个字符：$ cut range_fields.txt -c -2abababab 4.4 sed 进行文本替换选项： 12345-e &lt;script&gt; # 以选项中指定的script来处理输入的文本文件-f &lt;script&gt; # 以选项中指定的script文件来处理输入的文本文件-h # 显示帮助-n # 仅显示script处理后的结果-V # 显示版本信息 命令： 1234567891011121314151617181920212223a\ # 在当前行下面插入文本。i\ # 在当前行上面插入文本。c\ # 把选定的行改为新的文本。 d # 删除，删除选择的行。 D # 删除模板块的第一行。s # 替换指定字符 h 拷贝模板块的内容到内存中的缓冲区。 H # 追加模板块的内容到内存中的缓冲区。 g # 获得内存缓冲区的内容，并替代当前模板块中的文本。 G # 获得内存缓冲区的内容，并追加到当前模板块文本的后面。 l # 列表不能打印字符的清单。 n # 读取下一个输入行，用下一个命令处理新的行而不是用第一个命令。 N # 追加下一个输入行到模板块后面并在二者间嵌入一个新行，改变当前行号码。 p # 打印模板块的行。 P(大写) 打印模板块的第一行。 q # 退出Sed。 b lable # 分支到脚本中带有标记的地方，如果分支不存在则分支到脚本的末尾。 r file # 从file中读行。 t label # if分支，从最后一行开始，条件一旦满足或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。 T label # 错误分支，从最后一行开始，一旦发生错误或者T，t命令，将导致分支到带有标号的命令处，或者到脚本的末尾。w file # 写并追加模板块到file末尾。 W file # 写并追加模板块的第一行到file末尾。 ! # 表示后面的命令对所有没有被选定的行发生作用。 = # 打印当前行号码。 # 把注释扩展到下一个换行符以前。 sed 替换标记： 1234567g # 表示行内全面替换。p # 表示打印行。 w # 表示把行写入一个文件。 x # 表示互换模板块中的文本和缓冲区中的文本。 y # 表示把一个字符翻译为另外的字符（但是不用于正则表达式） \1 # 子串匹配标记 &amp; # 已匹配字符串标记 sed 元字符集： 12345678910111213^ # 匹配行开始，如：/^sed/匹配所有以sed开头的行。$ # 匹配行结束，如：/sed$/匹配所有以sed结尾的行。 . # 匹配一个非换行符的任意字符，如：/s.d/匹配s后接一个任意字符，最后是d。 * # 匹配0个或多个字符，如：/*sed/匹配所有模板是一个或多个空格后紧跟sed的行。 [] # 匹配一个指定范围内的字符，如/[ss]ed/匹配sed和Sed。 [^] # 匹配一个不在指定范围内的字符，如：/[^A-RT-Z]ed/ 匹配不包含A-R和T-Z的一个字母开头，紧跟ed的行。\(..\) # 匹配子串，保存匹配的字符，如s/(love)able/\1rs，loveable被替换成lovers。 &amp; # 保存搜索字符用来替换其他字符，如s/love/**&amp;**/，love这成**love**。 \&lt; # 匹配单词的开始，如:/\&lt;love/匹配包含以开头的单词的行。\&gt; # 匹配单词的结束，如:/love\&gt;/匹配包含以love结尾的单词的行。x\&#123;m\&#125; # 重复字符x，m次，如：/0\&#123;5\&#125;/匹配包含5个0的行。 x\&#123;m,\&#125; # 重复字符x，至少m次，如：/0\&#123;5,\&#125;/匹配至少有5个0的行。 x\&#123;m,n\&#125; # 重复字符x，至少m次，不多于n次，如：/0\&#123;5,10\&#125;/匹配5~10个0的行。 123456789101112131415161718192021222324252627282930313233343536373839# sed可以替换给定文本中的字符串。$ sed 's/pattern/replace_string/' file# 如果需要在替换的同时保存更改，可以使用-i选项$ sed -i 's/text/replace/' file# 后缀/g意味着sed会替换每一处匹配。但是有时候我们只需要从第n处匹配开始替换。对此，可以使用/Ng选项。$ sed 's/pattern/replace_string/g' file$ echo thisthisthisthis | sed 's/this/THIS/2g'thisTHISTHISTHIS$ echo thisthisthisthis | sed 's/this/THIS/3g'thisthisTHISTHIS# 字符/在sed中被作为定界符使用。我们可以像下面一样使用任意的定界符：$ sed 's:text:replace:g'$ sed 's|text|replace|g'# 当定界符出现在样式内部时，我们必须用前缀\对它进行转义：$ sed 's|te\|xt|replace|g'# \|是一个出现在样式内部并经过转义的定界符。# 移除空白行$ sed '/^$/d' file# 已匹配字符串标记（&amp;）在sed中，我们可以用 &amp;标记匹配样式的字符串，这样就能够在替换字符串时使用已匹配的内容。$ echo this is an example | sed 's/\w\+/[&amp;]/g'[this] [is] [an] [example]# 正则表达式 \w\+ 匹配每一个单词，然后我们用[&amp;]替换它。 &amp; 对应于之前所匹配到的单词。# 组合多个表达式$ sed 'expression' | sed 'expression'# 它等价于$ sed 'expression; expression'# 或者$ sed -e 'expression' -e expression'# 引用。sed表达式通常用单引号来引用。双引号会通过对表达式求值来对其进行扩展。$ text=hello$ echo hello world | sed "s/$text/HELLO/"HELLO world 4.5 awk 进行高级文本处理4.5.1 awk 常用命令选项 -F fs fs指定输入分隔符，fs可以是字符串或正则表达式，如-F: -v var=value 赋值一个用户定义变量，将外部变量传递给awk -f scripfile 从脚本文件中读取awk命令 -m[fr] val 对val值设置内在限制，-mf 选项限制分配给val的最大块数目；-mr 选项限制记录的最大数目。这两个功能是Bell实验室版awk的扩展功能，在标准awk中不适用。 4.5.2 awk 脚本基本结构1234$ awk 'BEGIN&#123; print "start" &#125; pattern&#123; commands &#125; END&#123; print "end" &#125;' file# 一个awk脚本通常由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句块3部分组成，这三个部分是可选的。任意一个部分都可以不出现在脚本中，脚本通常是被单引号或双引号中，例如：$ awk 'BEGIN&#123; i=0 &#125; &#123; i++ &#125; END&#123; print i &#125;' filename $ awk "BEGIN&#123; i=0 &#125; &#123; i++ &#125; END&#123; print i &#125;" filename 4.5.3 awk 的工作原理1$ awk 'BEGIN&#123; commands &#125; pattern&#123; commands &#125; END&#123; commands &#125;' 第一步：执行 BEGIN{ commands } 语句块中的语句 第二步：从文件或标准输入(stdin)读取一行，然后执行 pattern{ commands } 语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕 第三步：当读至输入流末尾时，执行 END{ commands } 语句块 4.5.4 awk 内置变量（预定义变量）说明： [A][N][P][G]表示第一个支持变量的工具，[A]=awk、[N]=nawk、[P]=POSIXawk、[G]=gawk 12345678910111213141516171819202122$n # 当前记录的第n个字段，比如n为1表示第一个字段，n为2表示第二个字段。 $0 # 这个变量包含执行过程中当前行的文本内容。 [N] ARGC # 命令行参数的数目。 [G] ARGIND # 命令行中当前文件的位置（从0开始算）。 [N] ARGV # 包含命令行参数的数组。 [G] CONVFMT # 数字转换格式（默认值为%.6g）。 [P] ENVIRON # 环境变量关联数组。 [N] ERRNO # 最后一个系统错误的描述。 [G] FIELDWIDTHS # 字段宽度列表（用空格键分隔）。 [A] FILENAME # 当前输入文件的名。 [P] FNR # 同NR，但相对于当前文件。 [A] FS # 字段分隔符（默认是任何空格）。 [G] IGNORECASE # 如果为真，则进行忽略大小写的匹配。 [A] NF # 表示字段数，在执行过程中对应于当前的字段数。 [A] NR # 表示记录数，在执行过程中对应于当前的行号。 [A] OFMT # 数字的输出格式（默认值是%.6g）。 [A] OFS # 输出字段分隔符（默认值是一个空格）。 [A] ORS # 输出记录分隔符（默认值是一个换行符）。 [A] RS # 记录分隔符（默认是一个换行符）。 [N] RSTART # 由match函数所匹配的字符串的第一个位置。 [N] RLENGTH # 由match函数所匹配的字符串的长度。 [N] SUBSEP # 数组下标分隔符（默认值是34）。 1234567891011121314151617181920212223242526272829$ echo -e "line1 f2 f3nline2 f4 f5nline3 f6 f7" | awk '&#123;print "Line No:"NR", No of fields:"NF, "$0="$0, "$1="$1, "$2="$2, "$3="$3&#125;' Line No:1, No of fields:3 $0=line1 f2 f3 $1=line1 $2=f2 $3=f3 Line No:2, No of fields:3 $0=line2 f4 f5 $1=line2 $2=f4 $3=f5 Line No:3, No of fields:3 $0=line3 f6 f7 $1=line3 $2=f6 $3=f7# 使用print $NF可以打印出一行中的最后一个字段，使用$(NF-1)则是打印倒数第二个字段，其他以此类推：$ echo -e "line1 f2 f3n line2 f4 f5" | awk '&#123;print $NF&#125;' f3f5$ echo -e "line1 f2 f3n line2 f4 f5" | awk '&#123;print $(NF-1)&#125;' f2 f4# 打印每一行的第二和第三个字段：$ awk '&#123; print $2,$3 &#125;' filename# 统计文件中的行数：$ awk 'END&#123; print NR &#125;' filename# 一个每一行中第一个字段值累加的例子：$ seq 5 | awk 'BEGIN&#123; sum=0; print "总和：" &#125; &#123; print $1"+"; sum+=$1 &#125; END&#123; print "等于"; print sum &#125;' 总和： 1+ 2+ 3+ 4+ 5+ 等于 15 4.5.5 将外部变量值传递给awk借助 -v 选项，可以将外部值（并非来自stdin）传递给awk： 12345678910$ VAR=10000 $ echo | awk -v VARIABLE=$VAR '&#123; print VARIABLE &#125;'# 另一种传递外部变量方法：$ var1="aaa" $ var2="bbb" $ echo | awk '&#123; print v1,v2 &#125;' v1=$var1 v2=$var2# 当输入来自于文件时使用：$ awk '&#123; print v1,v2 &#125;' v1=$var1 v2=$var2 filename 4.5.6 awk 运算与判断算数运算符： 运算符 描述 + - 加、减 * / &amp; 乘，除与求余 + - ! 一元加、减和逻辑非 ^ *** 求幂 ++ – 增加或减少，作为前缀或后缀 12$ awk 'BEGIN&#123;a="b";print a++,++a;&#125;' 0 2 **注意：**所有用作算术运算符进行操作，操作数自动转为数值，所有非数值都变为0 赋值运算符： 运算符 描述 = += -= = /= %= ^= *= 赋值语句 逻辑运算符： 运算符 描述 || 逻辑或 &amp;&amp; 逻辑与 12$ awk 'BEGIN&#123;a=1;b=2;print (a&gt;5 &amp;&amp; b&lt;=2),(a&gt;5 || b&lt;=2);&#125;'0 1 正则运算符： 运算符 描述 ~ ~! 匹配正则表达式和不匹配正则表达式 12$ awk 'BEGIN&#123;a="100testa";if(a ~ /^100*/)&#123;print "ok";&#125;&#125;' ok 关系运算符： 运算符 描述 &lt; &lt;= &gt; &gt;= != == 关系运算符 12$ awk 'BEGIN&#123;a=11;if(a &gt;= 9)&#123;print "ok";&#125;&#125;' ok **注意：**> < 可以作为字符串比较，也可以用作数值比较，关键看操作数如果是字符串就会转换为字符串比较。两个都为数字才转为数值比较。字符串比较：按照ASCII码顺序比较。 其他运算符： 运算符 描述 $ 字段引用 空格 字符串连接符 ? : C条件表达式 in 数组中是否存在某键值 12345678$ awk 'BEGIN&#123;a="b";print a=="b"?"ok":"err";&#125;' ok $ awk 'BEGIN&#123;a="b";arr[0]="b";arr[1]="c";print (a in arr);&#125;' 0 $ awk 'BEGIN&#123;a="b";arr[0]="b";arr["b"]="c";print (a in arr);&#125;' 1 运算级优先级表： 级别 运算符 说明 1 =, +=, -=, *=, /=, %=, &amp;=, ^=, |=, &lt;&lt;=, &gt;&gt;= 赋值、运算 2 || 逻辑或 3 &amp;&amp; 逻辑与 4 | 按位或 5 ^ 按位异或 6 &amp; 按位与 7 ==, != 等于、不等于 8 &lt;=, &gt;=, &lt;, &gt; 小于等于、大于等于、小于、大于 9 &lt;&lt;, &gt;&gt; 按位左移，按位右移 10 +, - 加、减 11 *, /, % 乘、除、取模 12 !, ~ 逻辑非、按位取反或补码 13 -, + 正、负 级别越高越优先 4.5.7 awk 高级输入输出读取下一条记录： awk中 next 语句使用：在循环逐行匹配，如果遇到 next，就会跳过当前行，直接忽略下面语句。而进行下一行匹配。net语句一般用于多行合并： 12345678910$ cat text.txt a b c d e $ awk 'NR%2==1&#123;next&#125;&#123;print NR,$0;&#125;' text.txt 2 b 4 d 当记录行号除以2余1，就跳过当前行。下面的 print NR,$0 也不会执行。下一行开始，程序有开始判断 NR%2 值。这个时候记录行号是 ：2 ，就会执行下面语句块：&#39;print NR,$0&#39; 分析发现需要将包含有 “web” 行进行跳过，然后需要将内容与下面行合并为一行： 123456789101112131415161718192021$ cat text.txt web01[192.168.2.100] httpd ok tomcat ok sendmail ok web02[192.168.2.101] httpd ok postfix ok web03[192.168.2.102] mysqld ok httpd ok 0 $ awk '/^web/&#123;T=$0;next;&#125;&#123;print T":t"$0;&#125;' test.txt web01[192.168.2.100]: httpd ok web01[192.168.2.100]: tomcat ok web01[192.168.2.100]: sendmail ok web02[192.168.2.101]: httpd ok web02[192.168.2.101]: postfix ok web03[192.168.2.102]: mysqld ok web03[192.168.2.102]: httpd ok 简单地读取一条记录： awk getline 用法：输出重定向需用到 getline函数。getline从标准输入、管道或者当前正在处理的文件之外的其他输入文件获得输入。它负责从输入获得下一行的内容，并给NF,NR和FNR等内建变量赋值。如果得到一条记录，getline函数返回1，如果到达文件的末尾就返回0，如果出现错误，例如打开文件失败，就返回-1。 getline语法：getline var，变量var包含了特定行的内容。 awk getline从整体上来说，用法说明： 当其左右无重定向符 | 或 &lt; 时：getline作用于当前文件，读入当前文件的第一行给其后跟的变量 var 或 $0（无变量），应该注意到，由于awk在处理getline之前已经读入了一行，所以getline得到的返回结果是隔行的。 当其左右有重定向符 | 或 &lt; 时：getline则作用于定向输入文件，由于该文件是刚打开，并没有被awk读入一行，只是getline读入，那么getline返回的是该文件的第一行，而不是隔行。 12345678# 执行linux的date命令，并通过管道输出给getline，然后再把输出赋值给自定义变量out，并打印它：$ awk 'BEGIN&#123; "date" | getline out; print out &#125;' test# 执行shell的date命令，并通过管道输出给getline，然后getline从管道中读取并将输入赋值给out，split函数把变量out转化成数组mon，然后打印数组mon的第二个元素：$ awk 'BEGIN&#123; "date" | getline out; split(out,mon); print mon[2] &#125;' test# 命令ls的输出传递给geline作为输入，循环使getline从ls的输出中读取一行，并把它打印到屏幕。这里没有输入文件，因为BEGIN块在打开输入文件前执行，所以可以忽略输入文件。$ awk 'BEGIN&#123; while( "ls" | getline) print &#125;' 关闭文件： awk中允许在程序中关闭一个输入或输出文件，方法是使用awk的close语句。 1close("filename") filename可以是getline打开的文件，也可以是stdin，包含文件名的变量或者getline使用的确切命令。或一个输出文件，可以是stdout，包含文件名的变量或使用管道的确切命令。 输出到一个文件： 123$ echo | awk '&#123;printf("hello word!n") &gt; "datafile"&#125;'或 $ echo | awk '&#123;printf("hello word!n") &gt;&gt; "datafile"&#125;' 4.5.8 设置字段定界符默认的字段定界符是空格，可以使用 `-F "定界符"` 明确指定一个定界符： 123$ awk -F: '&#123; print $NF &#125;' /etc/passwd 或 $ awk 'BEGIN&#123; FS=":" &#125; &#123; print $NF &#125;' /etc/passwd 在 BEGIN语句块 中则可以用 OFS=“定界符” 设置输出字段的定界符。 4.5.9 流程控制语句条件判断语句： 123456789101112$ awk 'BEGIN&#123; test=100; if(test&gt;90)&#123; print "very good"; &#125; else if(test&gt;60)&#123; print "good"; &#125; else&#123; print "no pass"; &#125; &#125;' very good 每条命令语句后面可以用 ; 分号结尾。 循环语句： while语句： 12345678910$ awk 'BEGIN&#123; test=100; total=0; while(i&lt;=test)&#123; total+=i; i++; &#125; print total; &#125;' 5050 for循环： 格式1： 12345678910111213$ awk 'BEGIN&#123; for(k in ENVIRON)&#123; print k"="ENVIRON[k]; &#125; &#125;' TERM=linux G_BROKEN_FILENAMES=1 SHLVL=1 pwd=/root/text ... logname=root HOME=/root SSH_CLIENT=192.168.1.21 53087 22 注：ENVIRON是awk常量，是子典型数组。 格式2： 123456789$ awk 'BEGIN&#123; total=0; for(i=0;i&lt;=100;i++)&#123; total+=i; &#125; print total; &#125;' 5050 do循环： 12345678910$ awk 'BEGIN&#123; total=0; i=0; do &#123; total+=i;i++; &#125; while(i&lt;=100) print total; &#125;' 5050 其他语句： break 当 break 语句用于 while 或 for 语句时，导致退出程序循环 continue 当 continue 语句用于 while 或 for 语句时，使程序循环移动到下一个迭代 next 能能够导致读入下一个输入行，并返回到脚本的顶部。这可以避免对当前输入行执行其他的操作过程 exit 语句使主输入循环退出并将控制转移到END,如果END存在的话。如果没有定义END规则，或在END中应用exit语句，则终止脚本的执行 4.5.10 数组应用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 得到数组长度$ awk 'BEGIN&#123;info="it is a test";lens=split(info,tA," ");print length(tA),lens;&#125;' 4 4# length返回字符串以及数组长度，split进行分割字符串为数组，也会返回分割得到数组长度。# asort对数组进行排序，返回数组长度。$ awk 'BEGIN&#123;info="it is a test";split(info,tA," ");print asort(tA);&#125;' 4# 输出数组内容（无序，有序输出）：$ awk 'BEGIN&#123;info="it is a test";split(info,tA," ");for(k in tA)&#123;print k,tA[k];&#125;&#125;' 4 test 1 it 2 is 3 a # for…in 输出，因为数组是关联数组，默认是无序的。所以通过 for…in 得到是无序的数组。如果需要得到有序数组，需要通过下标获得。$ awk 'BEGIN&#123;info="it is a test";tlen=split(info,tA," ");for(k=1;k&lt;=tlen;k++)&#123;print k,tA[k];&#125;&#125;' 1 it 2 is 3 a 4 test# 注意：数组下标是从1开始，与C数组不一样。# 判断键值存在以及删除键值：$ awk 'BEGIN&#123;tB["a"]="a1";tB["b"]="b1";if( "c" in tB)&#123;print "ok";&#125;;for(k in tB)&#123;print k,tB[k];&#125;&#125;' a a1 b b1# 删除键值： $ awk 'BEGIN&#123;tB["a"]="a1";tB["b"]="b1";delete tB["a"];for(k in tB)&#123;print k,tB[k];&#125;&#125;' b b1# 二维、多维数组使用$ awk 'BEGIN&#123; for(i=1;i&lt;=9;i++)&#123; for(j=1;j&lt;=9;j++)&#123; tarr[i,j]=i*j; print i,"*",j,"=",tarr[i,j]; &#125; &#125; &#125;' 1 * 1 = 1 1 * 2 = 2 1 * 3 = 3 1 * 4 = 4 1 * 5 = 5 1 * 6 = 6 ... 9 * 6 = 54 9 * 7 = 63 9 * 8 = 72 9 * 9 = 81# 可以通过array[k,k2]引用获得数组内容。# 另一种方法：$ awk 'BEGIN&#123; for(i=1;i&lt;=9;i++)&#123; for(j=1;j&lt;=9;j++)&#123; tarr[i,j]=i*j; &#125; &#125; for(m in tarr)&#123; split(m,tarr2,SUBSEP); print tarr2[1],"*",tarr2[2],"=",tarr[m]; &#125; &#125;' 4.5.11 内置函数awk内置函数，主要分以下3种类似：算数函数、字符串函数、其它一般函数、时间函数。 算数函数： 格式 描述 atan2( y, x ) 返回 y/x 的反正切 cos( x ) 返回 x 的余弦；x 是弧度 sin( x ) 返回 x 的正弦；x 是弧度 exp( x ) 返回 x 幂函数 log( x ) 返回 x 的自然对数 sqrt( x ) 返回 x 平方根 int( x ) 返回 x 的截断至整数的值 rand( ) 返回任意数字 n，其中 0 &lt;= n &lt; 1 srand( [expr] ) 将 rand 函数的种子值设置为 Expr 参数的值，或如果省略 Expr 参数则使用某天的时间。返回先前的种子值。 12345678910$ awk 'BEGIN&#123;OFMT="%.3f";fs=sin(1);fe=exp(10);fl=log(10);fi=int(3.1415);print fs,fe,fl,fi;&#125;' 0.841 22026.466 2.303 3# 获得随机数：$ awk 'BEGIN&#123;srand();fr=int(100*rand());print fr;&#125;' 78 $ awk 'BEGIN&#123;srand();fr=int(100*rand());print fr;&#125;' 31 $ awk 'BEGIN&#123;srand();fr=int(100*rand());print fr;&#125;' 41 字符串函数： 格式 描述 gsub( Ere, Repl, [ In ] ) 除了正则表达式所有具体值被替代这点，它和 sub 函数完全一样地执行 sub( Ere, Repl, [ In ] ) 用 Repl 参数指定的字符串替换 In 参数指定的字符串中的由 Ere 参数指定的扩展正则表达式的第一个具体值。sub 函数返回替换的数量。出现在 Repl 参数指定的字符串中的 &amp;（和符号）由 In 参数指定的与 Ere 参数的指定的扩展正则表达式匹配的字符串替换。如果未指定 In 参数，缺省值是整个记录（$0 记录变量） index( String1, String2 ) 在由 String1 参数指定的字符串（其中有出现 String2 指定的参数）中，返回位置，从 1 开始编号。如果 String2 参数不在 String1 参数中出现，则返回 0（零） length [(String)] 返回 String 参数指定的字符串的长度（字符形式）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量） blength [(String)] 返回 String 参数指定的字符串的长度（以字节为单位）。如果未给出 String 参数，则返回整个记录的长度（$0 记录变量） substr( String, M, [ N ] ) 返回具有 N 参数指定的字符数量子串。子串从 String 参数指定的字符串取得，其字符以 M 参数指定的位置开始。M 参数指定为将 String 参数中的第一个字符作为编号 1。如果未指定 N 参数，则子串的长度将是 M 参数指定的位置到 String 参数的末尾 的长度 match( String, Ere ) 在 String 参数指定的字符串（Ere 参数指定的扩展正则表达式出现在其中）中返回位置（字符形式），从 1 开始编号，或如果 Ere 参数不出现，则返回 0（零）。RSTART 特殊变量设置为返回值。RLENGTH 特殊变量设置为匹配的字符串的长度，或如果未找到任何匹配，则设置为 -1（负一） split( String, A, [Ere] ) 将 String 参数指定的参数分割为数组元素 A[1], A[2], . . ., A[n]，并返回 n 变量的值。此分隔可以通过 Ere 参数指定的扩展正则表达式进行，或用当前字段分隔符（FS 特殊变量）来进行（如果没有给出 Ere 参数）。除非上下文指明特定的元素还应具有一个数字值，否则 A 数组中的元素用字符串值来创建 tolower( String ) 返回 String 参数指定的字符串，字符串中每个大写字符将更改为小写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义 toupper( String ) 返回 String 参数指定的字符串，字符串中每个小写字符将更改为大写。大写和小写的映射由当前语言环境的 LC_CTYPE 范畴定义 sprintf(Format, Expr, Expr, . . . ) 根据 Format 参数指定的 printf 子例程格式字符串来格式化 Expr 参数指定的表达式并返回最后生成的字符串 注：Ere都可以是正则表达式。 1234567891011121314151617181920212223# gsub,sub使用 $ awk 'BEGIN&#123;info="this is a test2010test!";gsub(/[0-9]+/,"!",info);print info&#125;' this is a test!test!# 查找字符串（index使用） $ awk 'BEGIN&#123;info="this is a test2010test!";print index(info,"test")?"ok":"no found";&#125;' ok# 正则表达式匹配查找(match使用） $ awk 'BEGIN&#123;info="this is a test2010test!";print match(info,/[0-9]+/)?"ok":"no found";&#125;' ok# 截取字符串(substr使用） $ awk 'BEGIN&#123;info="this is a test2010test!";print substr(info,4,10);&#125;' s is a tes# 字符串分割（split使用） $ awk 'BEGIN&#123;info="this is a test";split(info,tA," ");print length(tA);for(k in tA)&#123;print k,tA[k];&#125;&#125;' 4 4 test 1 this 2 is 3 a 格式化字符串输出（sprintf使用） 格式化字符串格式： 格式 描述 %d 十进制有符号整数 %u 十进制无符号整数 %f 浮点数 %s 字符串 %c 单个字符 %p 指针的值 %e 指数形式的浮点数 %x %X 无符号以十六进制表示的整数 %o 无符号以八进制表示的整数 %g 自动选择合适的表示法 12$ awk 'BEGIN&#123;n1=124.113;n2=-1.224;n3=1.2345; printf("%.2f,%.2u,%.2g,%X,%on",n1,n2,n3,n1,n1);&#125;' 124.11,18446744073709551615,1.2,7C,174 一般函数： 格式 描述 close( Expression ) 用同一个带字符串值的 Expression 参数来关闭由 print 或 printf 语句打开的或调用 getline 函数打开的文件或管道。如果文件或管道成功关闭，则返回 0；其它情况下返回非零值。如果打算写一个文件，并稍后在同一个程序中读取文件，则 close 语句是必需的 system(command ) 执行 Command 参数指定的命令，并返回退出状态。等同于 system 子例程 Expression | getline [ Variable ] 从来自 Expression 参数指定的命令的输出中通过管道传送的流中读取一个输入记录，并将该记录的值指定给 Variable 参数指定的变量。如果当前未打开将 Expression 参数的值作为其命令名称的流，则创建流。创建的流等同于调用 popen 子例程，此时 Command 参数取 Expression 参数的值且 Mode 参数设置为一个是 r 的值。只要流保留打开且 Expression 参数求得同一个字符串，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录 getline [ Variable ] &lt; Expression 从 Expression 参数指定的文件读取输入的下一个记录，并将 Variable 参数指定的变量设置为该记录的值。只要流保留打开且 Expression 参数对同一个字符串求值，则对 getline 函数的每次后续调用读取另一个记录。如果未指定 Variable 参数，则 $0 记录变量和 NF 特殊变量设置为从流读取的记录 getline [ Variable ] 将 Variable 参数指定的变量设置为从当前输入文件读取的下一个输入记录。如果未指定 Variable 参数，则 $0 记录变量设置为该记录的值，还将设置 NF、NR 和 FNR 特殊变量 12345678910111213141516171819202122# 打开外部文件（close用法） $ awk 'BEGIN&#123;while("cat /etc/passwd"|getline)&#123;print $0;&#125;;close("/etc/passwd");&#125;' root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologin# 逐行读取外部文件(getline使用方法） $ awk 'BEGIN&#123;while(getline &lt; "/etc/passwd")&#123;print $0;&#125;;close("/etc/passwd");&#125;' root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin $ awk 'BEGIN&#123;print "Enter your name:";getline name;print name;&#125;' Enter your name: chengmo chengmo# 调用外部应用程序(system使用方法） $ awk 'BEGIN&#123;b=system("ls -al");print b;&#125;' total 42092 drwxr-xr-x 14 chengmo chengmo 4096 09-30 17:47 . drwxr-xr-x 95 root root 4096 10-08 14:01 .. # b返回值，是执行结果。 时间函数： 格式 描述 函数名 说明 mktime( YYYY MM dd HH MM ss[ DST]) 生成时间格式 strftime([format [, timestamp]]) 格式化时间输出，将时间戳转为时间字符串 具体格式，见下表. systime() 得到时间戳,返回从1970年1月1日开始到当前时间(不计闰年)的整秒数 12345678910# 建指定时间(mktime使用） $ awk 'BEGIN&#123;tstamp=mktime("2001 01 01 12 12 12");print strftime("%c",tstamp);&#125;' 2001年01月01日 星期一 12时12分12秒 $ awk 'BEGIN&#123;tstamp1=mktime("2001 01 01 12 12 12");tstamp2=mktime("2001 02 01 0 0 0");print tstamp2-tstamp1;&#125;' 2634468 # 求2个时间段中间时间差，介绍了strftime使用方法 $ awk 'BEGIN&#123;tstamp1=mktime("2001 01 01 12 12 12");tstamp2=systime();print tstamp2-tstamp1;&#125;' 308201392 strftime日期和时间格式说明符 格式 描述 %a 星期几的缩写（Sun） %A 星期几的完整写法（Sunday） %b 月名的缩写（Oct） %B 月名的完整写法（October） %c 本地日期和时间 %d 十进制日期 %D 日期 08/20/99 %e 日期，如果只有一位会补上一个空格 %H 用十进制表示24小时格式的时间 %I 用十进制表示12小时格式的时间 %j 从1月1日期一年中的第几天 %m 十进制表示的月份 %M 十进制表示的分钟 %p 12小时表示法（AM/PM） %S 十进制表示的秒 %U 十进制表示的一年中的第几个星期（星期天作为一个星期的开始） %w 十进制表示的星期几（星期天是0） %W 十进制表示的一年中的第几个星期（星期一作为一个星期的开始） %x 重新设置本地日期（08/20/99） %X 重新设置本地时间（12 : 00 : 00） %y 两位数字表示的年（99） %Y 当前月份 %Z 时区（PDT） %% 百分号（%） 4.6 find 对目录中的所有文件进行文本替换12345# 将所有.cpp文件中的Copyright替换成Copyleft：$ find . -name *.cpp -print0 | xargs -I&#123;&#125; -0 sed -i 's/Copyright/Copyleft/g' &#123;&#125;# 选项-exec实现同样的效果：$ find . -name *.cpp -exec sed -i 's/Copyright/Copyleft/g' \&#123;\&#125; \; 5 一团乱麻5.1 wget命令12345678910111213141516171819202122-a&lt;日志文件&gt;： # 在指定的日志文件中记录资料的执行过程； -A&lt;后缀名&gt;： # 指定要下载文件的后缀名，多个后缀名之间使用逗号进行分隔； -b： # 进行后台的方式运行wget； -B&lt;连接地址&gt;： # 设置参考的连接地址的基地地址； -c： # 继续执行上次终端的任务； -C&lt;标志&gt;： # 设置服务器数据块功能标志on为激活，off为关闭，默认值为on； -d： # 调试模式运行指令； -D&lt;域名列表&gt;： # 设置顺着的域名列表，域名之间用“，”分隔； -e&lt;指令&gt;： # 作为文件“.wgetrc”中的一部分执行指定的指令； -h： # 显示指令帮助信息； -i&lt;文件&gt;： # 从指定文件获取要下载的URL地址； -l&lt;目录列表&gt;： # 设置顺着的目录列表，多个目录用“，”分隔； -L： # 仅顺着关联的连接； -r： # 递归下载方式； -nc： # 文件存在时，下载文件不覆盖原有文件； -nv： # 下载时只显示更新和出错信息，不显示指令的详细执行过程； -q： # 不显示指令执行过程； -nh： # 不查询主机名称； -v： # 显示详细执行过程； -V： # 显示版本信息； --passive-ftp： # 使用被动模式PASV连接FTP服务器； --follow-ftp： # 从HTML文件中下载FTP连接文件。 12345678910111213141516171819202122# 使用wget下载单个文件 $ wget http://www.linuxde.net/testfile.zip# 下载并以不同的文件名保存 $ wget -O wordpress.zip http://www.linuxde.net/download.aspx?id=1080# wget限速下载 $ wget --limit-rate=300k http://www.linuxde.net/testfile.zip# 使用wget断点续传 $ wget -c http://www.linuxde.net/testfile.zip# 使用wget后台下载 $ wget -b http://www.linuxde.net/testfile.zip Continuing in background, pid 1840. Output will be written to `wget-log'.# 对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载，你可以使用以下命令来察看下载进度： $ tail -f wget-log# 伪装代理名称下载 $ wget --user-agent="Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16" http://www.linuxde.net/testfile.zip# 有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过--user-agent参数伪装。 测试下载链接： 当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加–spider参数进行检查。 1$ wget --spider URL 如果下载链接正确，将会显示: 12345Spider mode enabled. Check if remote file exists. HTTP request sent, awaiting response... 200 OK Length: unspecified [text/html] Remote file exists and could contain further links, but recursion is disabled -- not retrieving. 这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误: 1234$ wget --spider url Spider mode enabled. Check if remote file exists. HTTP request sent, awaiting response... 404 Not Found Remote file does not exist -- broken link!!! 你可以在以下几种情况下使用–spider参数： 定时下载之前进行检查 间隔检测网站是否可用 检查网站页面的死链接 123456789101112131415161718192021222324# 增加重试次数 $ wget --tries=40 URL# 下载多个文件 $ wget -i filelist.txt # 首先，保存一份下载链接文件： $ cat &gt; filelist.txt url1 url2 url3 url4 # 接着使用这个文件和参数-i下载。# 过滤指定格式下载 $ wget --reject=gif ur # 下载一个网站，但你不希望下载图片，可以使用这条命令。# 把下载信息存入日志文件 $ wget -o download.log URL # 不希望下载信息直接显示在终端而是在一个日志文件，可以使用。 # 限制总下载文件大小 $ wget -Q5m -i filelist.txt # 当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 镜像网站： 1$ wget --mirror -p --convert-links -P ./LOCAL URL 下载整个网站到本地。 –mirror 开户镜像下载 -p 下载所有为了html页面显示正常的文件 –convert-links 下载后，转换成本地的链接 -P ./LOCAL URL 保存所有文件和目录到本地指定目录 下载指定格式文件： 1$ wget -r -A.pdf url 可以在以下情况使用该功能： 下载一个网站的所有图片 下载一个网站的所有视频 下载一个网站的所有PDF文件 FTP下载： 12$ wget ftp-url $ wget --ftp-user=USERNAME --ftp-password=PASSWORD url 可以使用wget来完成ftp链接的下载。 使用wget匿名ftp下载： 1$ wget ftp-url 使用wget用户名和密码认证的ftp下载： 1$ wget --ftp-user=USERNAME --ftp-password=PASSWORD url 5.2 curl 命令常见参数： 12345678910111213141516-A/--user-agent &lt;string&gt; # 设置用户代理发送给服务器-b/--cookie &lt;name=string/file&gt; # cookie字符串或文件读取位置-c/--cookie-jar &lt;file&gt; # 操作结束后把cookie写入到这个文件中-C/--continue-at &lt;offset&gt; # 断点续转-D/--dump-header &lt;file&gt; # 把header信息写入到该文件中-e/--referer # 来源网址-f/--fail # 连接失败时不显示http错误-o/--output # 把输出写到该文件中-O/--remote-name # 把输出写到该文件中，保留远程文件的文件名-r/--range &lt;range&gt; # 检索来自HTTP/1.1或FTP服务器字节范围-s/--silent # 静音模式。不输出任何东西-T/--upload-file &lt;file&gt; # 上传文件-u/--user &lt;user[:password]&gt; # 设置服务器的用户和密码-w/--write-out [format] # 什么输出完成后-x/--proxy &lt;host[:port]&gt; # 在给定的端口上使用HTTP代理-#/--progress-bar # 进度条显示当前的传送状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 不显示进度信息使用--silent选项。$ curl URL --silent# 使用选项 -O 将下载的数据写入到文件，必须使用文件的绝对地址：$ curl http://man.linuxde.net/text.iso --silent -O# 选项-o将下载数据写入到指定名称的文件中，并使用--progress显示进度条：$ curl http://man.linuxde.net/test.iso -o filename.iso --progress######################################### 100.0%# 断点续传$ curl URL/File -C 偏移量 # 偏移量是以字节为单位的整数，如果让curl自动推断出正确的续传位置使用-C -： $ curl -C -URL# 使用--referer选项指定参照页字符串： $ curl --referer http://www.google.com http://man.linuxde.net # 用curl设置cookies 使用--cookie "COKKIES"选项来指定cookie，多个cookie使用分号分隔： $ curl http://man.linuxde.net --cookie "user=root;pass=123456" # 将cookie另存为一个文件，使用--cookie-jar选项： $ curl URL --cookie-jar cookie_file # 用curl设置用户代理字符串 有些网站访问会提示只能使用IE浏览器来访问，这是因为这些网站设置了检查用户代理，可以使用curl把用户代理设置为IE，这样就可以访问了。使用--user-agent或者-A选项：$ curl URL --user-agent "Mozilla/5.0" curl URL -A "Mozilla/5.0" # 其他HTTP头部信息也可以使用curl来发送，使用-H"头部信息" 传递多个头部信息，例如： $ curl -H "Host:man.linuxde.net" -H "accept-language:zh-cn" URL # curl的带宽控制和下载配额 使用--limit-rate限制curl的下载速度： $ curl URL --limit-rate 50k # 命令中用k（千字节）和m（兆字节）指定下载速度限制。 # 使用--max-filesize指定可下载的最大文件大小： $ curl URL --max-filesize bytes # 如果文件大小超出限制，命令则返回一个非0退出码，如果命令正常则返回0。 # 用curl进行认证 使用curl选项 -u 可以完成HTTP或者FTP的认证，可以指定密码，也可以不指定密码在后续操作中输入密码： $ curl -u user:pwd http://man.linuxde.net $ curl -u user http://man.linuxde.net # 只打印响应头部信息 通过-I或者-head可以只打印出HTTP头部信息： $ curl -I http://man.linuxde.net HTTP/1.1 200 OK Server: nginx/1.2.5 date: Mon, 10 Dec 2012 09:24:34 GMT Content-Type: text/html; charset=UTF-8 Connection: keep-alive Vary: Accept-Encoding X-Pingback: http://man.linuxde.net/xmlrpc.php 其他参数： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192-a/--append # 上传文件时，附加到目标文件--anyauth # 可以使用“任何”身份验证方法--basic # 使用HTTP基本验证-B/--use-ascii # 使用ASCII文本传输-d/--data &lt;data&gt; # HTTP POST方式传送数据--data-ascii &lt;data&gt; # 以ascii的方式post数据--data-binary &lt;data&gt; # 以二进制的方式post数据--negotiate # 使用HTTP身份验证--digest # 使用数字身份验证--disable-eprt # 禁止使用EPRT或LPRT--disable-epsv # 禁止使用EPSV--egd-file &lt;file&gt; # 为随机数据(SSL)设置EGD socket路径--tcp-nodelay # 使用TCP_NODELAY选项-E/--cert &lt;cert[:passwd]&gt; # 客户端证书文件和密码 (SSL)--cert-type &lt;type&gt; # 证书文件类型 (DER/PEM/ENG) (SSL)--key &lt;key&gt; # 私钥文件名 (SSL)--key-type &lt;type&gt; # 私钥文件类型 (DER/PEM/ENG) (SSL)--pass &lt;pass&gt; # 私钥密码 (SSL)--engine &lt;eng&gt; # 加密引擎使用 (SSL). "--engine list" for list--cacert &lt;file&gt; # CA证书 (SSL)--capath &lt;directory&gt; # CA目 (made using c_rehash) to verify peer against (SSL)--ciphers &lt;list&gt; # SSL密码--compressed # 要求返回是压缩的形势 (using deflate or gzip)--connect-timeout &lt;seconds&gt; # 设置最大请求时间--create-dirs # 建立本地目录的目录层次结构--crlf # 上传是把LF转变成CRLF--ftp-create-dirs # 如果远程目录不存在，创建远程目录--ftp-method [multicwd/nocwd/singlecwd] # 控制CWD的使用--ftp-pasv # 使用 PASV/EPSV 代替端口--ftp-skip-pasv-ip # 使用PASV的时候,忽略该IP地址--ftp-ssl # 尝试用 SSL/TLS 来进行ftp数据传输--ftp-ssl-reqd # 要求用 SSL/TLS 来进行ftp数据传输-F/--form &lt;name=content&gt; # 模拟http表单提交数据-form-string &lt;name=string&gt; # 模拟http表单提交数据-g/--globoff # 禁用网址序列和范围使用&#123;&#125;和[]-G/--get # 以get的方式来发送数据-h/--help # 帮助-H/--header &lt;line&gt; # 自定义头信息传递给服务器--ignore-content-length # 忽略的HTTP头信息的长度-i/--include # 输出时包括protocol头信息-I/--head # 只显示文档信息-j/--junk-session-cookies # 读取文件时忽略session cookie--interface &lt;interface&gt; # 使用指定网络接口/地址--krb4 &lt;level&gt; # 使用指定安全级别的krb4-k/--insecure # 允许不使用证书到SSL站点-K/--config # 指定的配置文件读取-l/--list-only # 列出ftp目录下的文件名称--limit-rate &lt;rate&gt; # 设置传输速度--local-port&lt;NUM&gt; # 强制使用本地端口号-m/--max-time &lt;seconds&gt; # 设置最大传输时间--max-redirs &lt;num&gt; # 设置最大读取的目录数--max-filesize &lt;bytes&gt; # 设置最大下载的文件总量-M/--manual # 显示全手动-n/--netrc # 从netrc文件中读取用户名和密码--netrc-optional # 使用 .netrc 或者 URL来覆盖-n--ntlm # 使用 HTTP NTLM 身份验证-N/--no-buffer # 禁用缓冲输出-p/--proxytunnel # 使用HTTP代理--proxy-anyauth # 选择任一代理身份验证方法--proxy-basic # 在代理上使用基本身份验证--proxy-digest # 在代理上使用数字身份验证--proxy-ntlm # 在代理上使用ntlm身份验证-P/--ftp-port &lt;address&gt; # 使用端口地址，而不是使用PASV-Q/--quote &lt;cmd&gt; # 文件传输前，发送命令到服务器--range-file # 读取（SSL）的随机文件-R/--remote-time # 在本地生成文件时，保留远程文件时间--retry &lt;num&gt; # 传输出现问题时，重试的次数--retry-delay &lt;seconds&gt; # 传输出现问题时，设置重试间隔时间--retry-max-time &lt;seconds&gt; # 传输出现问题时，设置最大重试时间-S/--show-error # 显示错误--socks4 &lt;host[:port]&gt; # 用socks4代理给定主机和端口--socks5 &lt;host[:port]&gt; # 用socks5代理给定主机和端口-t/--telnet-option &lt;OPT=val&gt; # Telnet选项设置--trace &lt;file&gt; # 对指定文件进行debug--trace-ascii &lt;file&gt; # Like --跟踪但没有hex输出--trace-time # 跟踪/详细输出时，添加时间戳--url &lt;URL&gt; # Spet URL to work with-U/--proxy-user &lt;user[:password]&gt; # 设置代理用户名和密码-V/--version # 显示版本信息-X/--request &lt;command&gt; # 指定什么命令-y/--speed-time # 放弃限速所要的时间。默认为30-Y/--speed-limit # 停止传输速度的限制，速度时间'秒-z/--time-cond # 传送时间设置-0/--http1.0 # 使用HTTP 1.0-1/--tlsv1 # 使用TLSv1（SSL）-2/--sslv2 # 使用SSLv2的（SSL）-3/--sslv3 # 使用的SSLv3（SSL）--3p-quote # like -Q for the source URL for 3rd party transfer--3p-url # 使用url，进行第三方传送--3p-user # 使用用户名和密码，进行第三方传送-4/--ipv4 # 使用IP4-6/--ipv6 # 使用IP6 5.3 curl wget两种方法模拟http的get post请求get请求： 12345# 使用curl命令：$ curl "http://www.baidu.com" # 如果这里的URL指向的是一个文件或者一幅图都可以直接下载到本地$ curl -i "http://www.baidu.com" # 显示全部信息$ curl -l "http://www.baidu.com" # 只显示头部信息$ curl -v "http://www.baidu.com" # 显示get请求全过程解析 12# 使用wget命令：$ wget "http://www.baidu.com" post请求： 12# 使用curl命令(通过-d参数，把访问参数放在里面)：$ curl -d "param1=value1&amp;param2=value2" "http://www.baidu.com" 12# 使用wget命令：（--post-data参数来实现）$ wget --post-data 'user=foo&amp;password=bar' http://server.com/auth.PHP 6 B计划6.1 用tar归档tar支持的参数包括： A、 c、 d、 r、 u、 x、 f 和 v 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 用tar对文件进行归档：$ tar -cf output.tar file1 file2 file3 folder1 ..# 使用选项-t列出归档文件中所包含的文件：$ tar -tf archive.tarfile1file2# 如果需要在归档或列出归档文件列表时获知更多的细节信息，可以使用-v或-vv参数$ $ tar -tvf archive.tar-rw-rw-r-- shaan/shaan 0 2013-04-08 21:34 file1-rw-rw-r-- shaan/shaan 0 2013-04-08 21:34 file2# 文件名必须紧跟在-f之后，而且-f应该是选项中的最后一个。# 向归档文件中添加文件,追加选项-r$ tar -rvf original.tar new_file# 用下面的方法列出归档文件中的内容：$ tar -tf archive.tarhello.txt# 从归档文件中提取文件或文件夹, -x 表示提取$ tar -xf archive.tar# 用选项-C来指定需要将文件提取到哪个目录：$ tar -xf archive.tar -C /path/to/extraction_directory# 可以通过将文件名指定为命令行参数来提取特定的文件：$ tar -xvf file.tar file1 file4# 上面的命令只提取file1和file4，忽略其他文件。# 在tar中使用stdin和stdout$ tar cvf - files/ | ssh user@example.com "tar xv -C Documents/"# 在上面的例子中，对files目录中的内容进行了归档并输出到stdout（由'-'指明）。# 拼接两个归档文件, -A 选项轻松地合并多个tar文件$ tar -Af file1.tar file2.tar# 查看内容，验证操作是否成功：$ tar -tvf file1.tar# 通过检查时间戳来更新归档文件中的内容# 可以用更新选项-u指明：只有比归档文件中的同名文件更新时才会被添加。$ tar -tf archive.tarfileafilebfilec# 仅当filea自上次被加入archive.tar后出现了变动才对其进行追加，可以使用：$ tar -uf archive.tar filea# 如果两个filea的时间戳相同，则什么都不会发生。# 可用touch命令修改文件的时间戳，然后再用tar命令：$ tar -uvvf archive.tar filea-rw-r--r-- slynux/slynux 0 2010-08-14 17:53 filea# 比较归档文件与文件系统中的内容, 选项 -d 可以打印出两者之间的差别：$ tar -df archive.tarafile: Mod time differsafile: Size differs# 从归档文件中删除文件, --delete选项从给定的归档文件中删除文件$ tar -tf archive.tarfileafilebfilec# 删除filea：$ tar --delete --file archive.tar filea$ tar -tf archive.tarfilebfilec 压缩tar归档文件： 归档文件通常被压缩成下列格式之一： file.tar.gz file.tar.bz2 file.tar.lzma 不同的tar选项可以用来指定不同的压缩格式： -j 指定bunzip2格式； -z 指定gzip格式； –lzma 指定lzma格式。 12345678910111213141516171819# 为了让tar支持根据扩展名自动进行压缩，使用 -a或 --auto-compress选项：$ tar acvf archive.tar.gz filea fileb filec# 从归档中排除部分文件, --exclude [PATTERN]排除匹配通配符样式的文件$ tar -cf arch.tar * --exclude "*.txt"# 样式应该使用双引号来引用，避免shell对其进行扩展。# 也可以将需要排除的文件列表放入文件中，同时配合选项 -X：$ cat listfileafileb$ tar -cf arch.tar * -X list# 排除版本控制目录， 可以使用tar的 --exclude-vcs选项。例如：$ tar --exclude-vcs -czvvf source_code.tar.gz eye_of_gnome_svn# 打印总字节数，用–totals就可以在归档完成之后打印出总归档字节数：$ tar -cf arc.tar * --exclude "*.txt" --totalsTotal bytes written: 20480 (20KiB, 12MiB/s) 6.2 用cpio归档1234567891011# 创建测试文件：$ touch file1 file2 file3# 将测试文件按照下面的方法进行归档：$ echo file1 file2 file3 | cpio -ov &gt; archive.cpio# 列出cpio归档文件中的内容：$ cpio -it &lt; archive.cpio# 从cpio归档文件中提取文件：$ cpio -id &lt; archive.cpio 对于归档命令： -o 指定了输出； -v 用来打印归档文件列表。 在列出给定cpio归档文件所有内容的命令中： -i 用于指定输入； -t 表示列出归档文件中的内容。 当使用命令进行提取时， -d用来表示提取。 cpio在覆盖文件时不会发出提示。 6.3 使用gzip压缩数据12345678910111213141516171819# 要使用gzip压缩文件，可以使用下面的命令：$ gzip filename$ lsfilename.gz# 将gzip文件解压缩的方法如下：$ gunzip filename.gz$ lsfile# 列出压缩文件的属性信息：$ gzip -l test.txt.gzcompressed uncompressed ratio uncompressed_name35 6 -33.3% test.txt# gzip命令可以从stdin中读入文件，也可以将压缩文件写出到stdout，选项 -c用来将输出指定到stdout。$ cat file | gzip -c &gt; file.gz# 我们可以指定gzip的压缩级别。用 --fast或 --best选项分别提供最低或最高的压缩比。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 压缩归档文件# 方法 1$ tar -czvvf archive.tar.gz [FILES]或者$ tar -cavvf archive.tar.gz [FILES]# 选项 -a表明从文件扩展名自动推断压缩格式。# 方法 2# 首先，创建一个tar归档文件：$ tar -cvvf archive.tar [FILES]# 压缩tar归档文件：$ gzip archive.tar# zcat——无需解压缩，直接读取gzip格式文件$ lstest.gz$ zcat test.gzA test file# 文件test包含了一行文本"A test file"$ lstest.gz# 压缩率# 我们可以指定压缩率，它共有9级，其中：# 1级的压缩率最低，但是压缩速度最快；# 9级的压缩率最高，但是压缩速度最慢。$ gzip -5 test.img# 这应该能在压缩速度和压缩比之间获得一个不错的平衡。# 使用bzip2，唯一的不同在于bzip2的压缩效率比gzip更高，但花费的时间比gzip更长$ bzip2 filename# 解压缩bzip2格式的文件：$ bunzip2 filename.bz2# 生成tar.bz2文件并从中提取内容的方法同之前介绍的tar.gz类似：$ tar -xjvf archive.tar.bz2# 其中-j表明该归档文件是bzip2格式。# 使用lzma# lzma是另一种压缩工具，它的压缩率甚至比gzip和bzip2更好。$ lzma filename# 解压缩lzma文件：$ unlzma filename.lzma# 可以使用tar命令的--lzma选项对生成的tar归档文件进行压缩或提取：$ tar -cvvf --lzma archive.tar.lzma [FILES]或者$ tar -cavvf archive.tar.lzma [FILES]# 如果要将经过lzma压缩过的tar归档文件中的内容提取到指定的目录中，可以使用：$ tar -xvvf --lzma archive.tar.lzma -C extract_directory# 其中， -x用于提取内容， --lzma指定使用lzma对归档文件进行解压缩。# 我们也可以用：$ tar -xavvf archive.tar.lzma -C extract_directory 6.4 用 zip 归档和压缩123456789101112131415161718# 对归档文件采用ZIP格式进行压缩：$ zip file.zip file# 对目录和文件进行递归操作, -r 用于指定递归操作：$ zip -r archive.zip folder1 folder2# 要从ZIP文件中提取内容，可以使用：$ unzip file.zip# 在完成提取操作之后， unzip并不会删除file.zip# 如果需要更新压缩文件中的内容，使用选项 -u：$ zip file.zip -u newfile# 从压缩文件中删除内容，则使用-d：$ zip -d arc.zip file.txt# 列出压缩文件中的内容：$ unzip -l archive.zip 6.5 更快的归档工具 pbzip2123456789101112131415161718192021# 压缩单个文件：$ pbzip2 myfile.tar# pbzip2会自动检测系统中处理器核心的数量，然后将myfile.tar压缩成myfile.tar.bz2。# 要将多个文件或目录进行归档及压缩，可以使用tar配合pbzip2来实现：$ tar cf myfile.tar.bz2 --use-compress-prog=pbzip2 dir_to_compress/或者$ tar -c directory_to_compress/ | pbzip2 -c &gt; myfile.tar.bz2# 从pbzip2格式的文件中进行提取。# 如果是tar.bz2文件，我们可以一次性完成解压缩和提取工作：$ pbzip2 -dc myfile.tar.bz2 | tar x# 如果是经过pbzip2压缩过的归档文件，可以使用：$ pbzip2 -d myfile.tar.bz2# 手动指定处理器数量, 使用pbzip2的-p选项来手动指定处理器核心的数量$ pbzip2 -p4 myfile.tar# 上面的命令告诉pbzip2使用4个处理器核心。# 指定压缩比# 像其他压缩工具一样，我们可以使用从1到9的选项来分别指定最快和最优的压缩比。 6.6 创建压缩文件系统squashfs是一种具有超高压缩率的只读型文件系统，这种文件系统能够将2GB~3GB的数据压缩成一个700MB的文件。 123456789101112131415161718# 添加源目录和文件，创建一个squashfs文件：$ sudo mksquashfs /etc test.squashfsParallel mksquashfs: Using 2 processorsCreating 4.0 filesystem on test.squashfs, block size 131072.[=======================================] 1867/1867 100%# 利用环回形式挂载squashfs文件：$ mkdir /mnt/squash$ mount -o loop compressedfs.squashfs /mnt/squash# 你可以访问/mnt/squashfs访问其中的内容。# 在创建squashfs文件时排除部分文件, 选项-e，将需要排除的文件列表以命令行参数的方式来指定。例如：$ sudo mksquashfs /etc test.squashfs -e /etc/passwd /etc/shadow# 也可以将需要排除的文件名列表写入文件，然后用 -ef指定该文件：$ cat excludelist/etc/passwd/etc/shadow$ sudo mksquashfs /etc test.squashfs -ef excludelist 6.7 使用 rsync 备份系统快照rsync可以对位于不同位置的文件和目录进行同步，它利用差异计算以及压缩技术来最小化数据传输量。 rsync也支持压缩、加密等多种特性。 1234567891011121314151617181920212223242526272829303132# 将源目录复制到目的端：$ rsync -av /home/slynux/data slynux@192.168.0.6:/home/backups/data# 其中： -a表示要进行归档； -v表示在stdout上打印出细节信息或进度。# 将数据备份到远程服务器或主机：$ rsync -av source_dir username@host:PATH# 用下面的方法将远程主机上的数据恢复到本地主机：$ rsync -av username@host:PATH destination# 通过网络进行传输时，压缩数据能够明显改善传输效率。我们可以用rsync的选项 -z 指定在网络传输时压缩数据。例如：$ rsync -avz source destination# 将一个目录中的内容同步到另一个目录：$ rsync -av /home/test/ /home/backups# 这条命令将源目录（/home/test）中的内容（不包括目录本身）复制到现有的backups目录中# 在使用rsync进行归档的过程中排除部分文件$ rsync -avz /home/code/some_code /mnt/disk/backup/code --exclude "*.txt"# 或者我们可以通过一个列表文件指定需要排除的文件。# 这可以利用--exclude-from FILEPATH。# 在更新rsync备份时，删除不存在的文件, rsync并不会在目的端删除那些在源端已不存在的文件$ rsync -avz SOURCE DESTINATION --delete# 定期进行备份$ crontab -ev# 添加上这么一行：0 */10 * * * rsync -avz /home/code user@IP_ADDRESS:/home/backups# 上面的crontab条目将rsync调度为每10个小时运行一次。 6.8 用 fsarchiver 创建全盘镜像1234567891011121314# 创建文件系统/分区备份。# 使用fsarchiver的savefs选项：$ fsarchiver savefs backup.fsa /dev/sda1# 同时备份多个分区。$ fsarchiver savefs backup.fsa /dev/sda1 /dev/sda2# 从备份归档中恢复分区。$ fsarchiver restfs backup.fsa id=0,dest=/dev/sda1# id=0 表 明 我 们 希 望 从 备 份 归 档 中 提 取 第 一 个 分 区 的 内 容 ， 将 其 恢 复 到 由 dest=/dev/sda1所指定的分区中。# 从备份归档中恢复多个分区。# 像之前一样，使用restfs选项：$ fsarchiver restfs backup.fsa id=0,dest=/dev/sda1 id=1,dest=/dev/sdb1 7 无网不利7.1 设置网络1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 手动设置网络接口的IP地址：$ ifconfig wlan0 192.168.0.80# 使用以下命令设置比IP地址的子网掩码：$ ifconfig wlan0 192.168.0.80 netmask 255.255.252.0# 自动配置网络接口$ dhclient eth0# 打印网络接口列表$ ifconfig | cut -c-10 | tr -d ' ' | tr -s '\n'lowlan0# 显示IP地址$ ifconfig wlan0 | egrep -o "inet addr:[^ ]*" | grep -o "[0-9.]*"192.168.0.82# 硬件地址（MAC地址）欺骗$ ifconfig eth0 hw ether 00:1c:bf:87:25:d5# 名字服务器与DNS（域名服务）$ cat /etc/resolv.confnameserver 8.8.8.8# 我们可以像下面这样手动添加名字服务器：$ echo nameserver IP_ADDRESS &gt;&gt; /etc/resolv.conf# DNS查找$ host google.comgoogle.com has address 64.233.181.105google.com has address 64.233.181.99google.com has address 64.233.181.147google.com has address 64.233.181.106google.com has address 64.233.181.103google.com has address 64.233.181.104$ nslookup google.comServer: 8.8.8.8Address: 8.8.8.8#53Non-authoritative answer:Name: google.comAddress: 64.233.181.105Name: google.comAddress: 64.233.181.99Name: google.comAddress: 64.233.181.147Name: google.comAddress: 64.233.181.106Name: google.comAddress: 64.233.181.103Name: google.comAddress: 64.233.181.104Server: 8.8.8.8# 上面最后一行对应着用于DNS解析的默认名字服务器。# 如果不使用DNS服务器，也可以为IP地址解析添加符号名，这只需要向文件 /etc/hosts中加入条目即可。# 用下面的方法进行添加：$ echo IP_ADDRESS symbolic_name &gt;&gt; /etc/hosts# 例如：$ echo 192.168.0.9 backupserver &gt;&gt; /etc/hosts# 添加了条目之后，任何时候解析backupserver，都会返回192.168.0.9。# 显示路由表信息$ routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref UseIface192.168.0.0 * 255.255.252.0 U 2 0 0wlan0link-local * 255.255.0.0 U 1000 0 0wlan0default p4.local 0.0.0.0 UG 0 0 0wlan0# 也可以使用：$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface192.168.0.0 0.0.0.0 255.255.252.0 U 2 0 0 wlan0169.254.0.0 0.0.0.0 255.255.0.0 U 1000 0 0 wlan00.0.0.0 192.168.0.4 0.0.0.0 UG 0 0 0 wlan0# -n指定以数字形式显示地址。如果使用-n， route会以数字形式的IP地址显示每一个条目；否则，如果IP地址具有对应的DNS条目，就会显示符号形式的主机名。# 设置默认网关：$ route add default gw 192.168.0.1 wlan0 7.2 traceroute 命令traceroute，它可以显示分组途径的所有网关的地址。 traceroute信息可以帮助我们搞明白分组到达目的地需要经过多少跳（hop）。中途的网关或路由器的数量给出了一个测量网络上两个节点之间距离的度量（metric）。 traceroute的输出如下： 12345678910111213141516$ traceroute google.comtraceroute to google.com (74.125.77.104), 30 hops max, 60 byte packets1 gw-c6509.lxb.as5577.net (195.26.4.1) 0.313 ms 0.371 ms 0.457 ms2 40g.lxb-fra.as5577.net (83.243.12.2) 4.684 ms 4.754 ms 4.823 ms3 de-cix10.net.google.com (80.81.192.108) 5.312 ms 5.348 ms 5.327 ms4 209.85.255.170 (209.85.255.170) 5.816 ms 5.791 ms 209.85.255.172(209.85.255.172) 5.678 ms5 209.85.250.140 (209.85.250.140) 10.126 ms 9.867 ms 10.754 ms6 64.233.175.246 (64.233.175.246) 12.940 ms 72.14.233.114(72.14.233.114) 13.736 ms 13.803 ms7 72.14.239.199 (72.14.239.199) 14.618 ms 209.85.255.166(209.85.255.166) 12.755 ms 209.85.255.143 (209.85.255.143) 13.803 ms8 209.85.255.98 (209.85.255.98) 22.625 ms 209.85.255.110(209.85.255.110) 14.122 ms*9 ew-in-f104.1e100.net (74.125.77.104) 13.061 ms 13.256 ms 13.484 ms 7.3 列出网络上所有的活动主机 (fping)fping的选项如下： 选项 -a指定打印出所有活动主机的IP地址； 选项 -u指定打印出所有无法到达的主机； 选项 -g指定从 “IP地址/子网掩码”记法或者”IP地址范围”记法中生成一组IP地址； 12345678910111213141516$ fping -a 192.160.1/24 -g# 或者$ fping -a 192.160.1 192.168.0.255 -g# 我们可以用已有的命令行工具来查询网络上的主机状态：$ fping -a 192.160.1/24 -g 2&gt; /dev/null192.168.0.1192.168.0.90# 或者，使用：$ fping -a 192.168.0.1 192.168.0.255 -g# &gt;/dev/null将由于主机无法到达所产生的错误信息打印到null设备。$ fping -a 192.168.0.1 192.168.0.5 192.168.0.6# 将IP地址作为参数传递$ fping -a &lt; ip.list# 从文件中传递一组IP地址 7.4 ssh 命令123456789101112131415# SSH的压缩功能,选项-C启用这一功能：$ ssh -C user@hostname COMMANDS# 将数据重定向至远程shell命令的stdin$ echo 'text' | ssh user@remote_host 'echo'text# 或者# 将文件中的数据进行重定向$ ssh user@remote_host 'echo' &lt; file# 在远程主机中执行图形化命令# 对此，你需要像这样设置变量$DISPLAY：$ ssh user@host "export DISPLAY=:0 ; command1; command2"""# 这将启用远程主机上的图形化输出。如果你想在本地主机上也显示图形化输出，使用SSH的X11转发选项（forwarding option）：$ ssh -X user@host "command1; command2 7.5 通过网络传输文件计算机联网的主要目的就是资源共享。在资源共享方面，使用最多的是文件共享。有多种方法可以用来在网络中传输文件。这则攻略就讨论了如何用常见的协议FTP、 SFTP、 RSYNC和SCP传输文件。 通过FTP传输文件可以使用lftp命令，通过SSH连接传输文件可以使用sftp， RSYNC使用SSH与rsync命令， scp通过SSH进行传输。 文件传输协议（File Transfer Protocol， FTP） ： 1234# 要连接FTP服务器传输文件，可以使用：$ lftp username@ftphost# 它会提示你输入密码，然后显示一个像下面那样的登录提示符：lftp username@ftphost:~&gt; 你可以在提示符后输入命令，如下所示。 用cd directory改变目录。 用lcd改变本地主机的目录。 用mkdir创建目录。 列出远程机器当前目录下的文件使用Is。 用get filename下载文件：lftp username@ftphost:~&gt; get filename 用put filename从当前目录上传文件：lftp username@ftphost:~&gt; put filename 用quit退出lftp会话。 FTP自动传输 ： ftp是另一个可用于FTP文件传输的命令。相比较而言， lftp的用法更灵活。 lftp和ftp为用户启动一个交互式会话（通过显示消息来提示用户输入）。 SFTP（Secure FTP，安全FTP） ： 12345$ cd /home/slynux$ put testfile.jpg$ get serverfile.jpg# 运行sftp：$ sftp user@domainname rsync命令 ： rsync广泛用于网络文件复制及系统备份。 SCP（Secure Copy Program，安全复制程序） ： 123$ scp filename user@remotehost:/home/path$ scp user@remotehost:/home/path/filename filename 用SCP进行递归复制 : 123$ scp -r /home/slynux user@remotehost:/home/backups# 将目录/home/slynux递归复制到远程主机中# scp的 -p 选项能够在复制文件的同时保留文件的权限和模式。 7.6 连接网线网络我们需要用ifconfig分配IP地址和子网掩码才能连接上有线网络。对于无线网络来说，还需要其他工具（如iwconfig和iwlist）来配置更多的参数。 iwlist工具扫描并列出可用的无线网络。用下面的命令进行扫描： 12345678$ iwlist scanwlan0 Scan completed : Cell 01 - Address: 00:12:17:7B:1C:65 Channel:11 Frequency:2.462 GHz (Channel 11) Quality=33/70 Signal level=-77 dBm Encryption key:on ESSID:"model-2" 7.7 在本地挂载点上挂载远程驱动器sshfs允许你将远程文件系统挂载到本地挂载点上。 123456# 将位于远程主机上的文件系统挂载到本地挂载点上：$ sshfs -o allow_other user@remotehost:/home/path /mnt/mountpointPassword:# 完成任务后，可用下面的方法卸载：$ umount /mnt/mountpoint 7.8 网络流量与端口分析列出系统中的开放端口以及运行在端口上的服务的详细信息，可以使用以下命令： 1234$ lsof -i# 要列出本地主机当前的开放端口，可以使用：$ lsof -i | grep ":[0-9]\+-&gt;" -o | grep "[0-9]\+" -o | sort | uniq 用netstat查看开放端口与服务 ： 12# netstat -tnp列出开放端口与服务：$ netstat -tnp 7.9 创建套接字最简单的方法就是使用netcat命令（或nc）。我们需要两个套接字：一个用来侦听，一个用来连接。 12345678# 设置侦听套接字：$ nc -l 1234# 这会在本地主机的端口1234上创建一个侦听套接字。# 连接到该套接字：$ nc HOST 1234# 要想发送消息，只需要在执行第2步操作的主机终端中输入信息并按回车键就行了。消息会出现在执行第1步操作的主机终端中。 在网络上进行快速文件复制 ： 12345# 在接收端执行下列命令：$ nc -l 1234 &gt; destination_filename# 在发送端执行下列命令：$ nc HOST 1234 &lt; source_filename 7.10 iptables防火墙设置12345678910# 阻塞发送到特定IP地址的流量：$ iptables -A OUTPUT -d 8.8.8.8 -j DROP# 阻塞发送到特定端口的流量：$ iptables -A OUTPUT -p tcp -dport 21 -j DROP# iptables中的第一个选项-A表明向链（chain）中添加一条新的规则，该规则由后续参数给出。OUTPUT链，它可以对所有出站（outgoing）的流量进行控制。-d指定了所要匹配的分组目的地址。-j来使iptables丢弃（DROP）符合条件的分组。-p指定该规则是适用于TCP， -dport指定了对应的端口。# 清除对iptables链所做出的所有改动。$ iptables --flush 8 当个好管家8.1 监视磁盘使用情况df 是disk free的缩写， du 是disk usage的缩写。 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 找出某个文件（或多个文件）占用的磁盘空间：$ du file.txt# 要获得某个目录中所有文件的磁盘使用情况，并在每一行中显示各个文件的磁盘占用详情，可以使用：$ du -a DIRECTORY# 以KB、 MB或块（block）为单位显示磁盘使用情况$ du -h FILENAME# 显示磁盘使用总计, -c 可以输出作为命令参数的所有文件和目录的磁盘使用情况$ du -c process_log.shpcpu.sh4 process_log.sh4 pcpu.sh8 total# -s（summarize，合计）则只输出合计数据。它可以配合 -h打印出人们易读的格式。$ du -sh slynux680K slynux# 打印以字节（默认输出）为单位的文件大小：$ du -b FILE(s)# 打印以KB为单位的文件大小：$ du -k FILE(s)# 打印以MB为单位的文件大小：$ du -m FILE(s)# 打印以指定块为单位的文件大小：$ du -B BLOCK_SIZE FILE(s)# 从磁盘使用统计中排除部分文件$ du --exclude "*.txt" FILES(s)# 排除所有的.txt文件$ du --exclude-from EXCLUDE.txt DIRECTORY# EXCLUDE.txt包含了需要排除的文件列表# --max-depth指定du应该遍历的目录层次的最大深度。$ du --max-depth 2 DIRECTORY# 找出指定目录中最大的10个文件$ du -ak /home/slynux | sort -nrk 1 | head -n 4$ find . -type f -exec du -k &#123;&#125; \; | sort -nrk 1 | head du提供磁盘使用情况信息，而df提供磁盘可用空间信息。 123456789$ df -hFilesystem Size Used Avail Use% Mounted on/dev/sda1 9.2G 2.2G 6.6G 25% /none 497M 240K 497M 1% /devnone 502M 168K 501M 1% /dev/shmnone 502M 88K 501M 1% /var/runnone 502M 0 502M 0% /var/locknone 502M 0 502M 0% /lib/init/rwnone 9.2G 2.2G 6.6G 25% /var/lib/ureadahead/debugfs 8.2 计算命令执行时间 real: %e user: %U sys: %S 12345678910111213141516171819202122$ time COMMAND# 可以用选项-o filename将相关的时间统计信息写入文件：$ /usr/bin/time -o output.txt COMMAND# 要将命令执行时间添加到文件而不影响其原有内容，使用选项-a以及-o：$ /usr/bin/time -a -o output.txt COMMAND# 创建格式化输出：$ /usr/bin/time -f "Time: %U" -a -o timing.log unameLinux# 用错误重定向操作符（2&gt;）对时间信息重定向。$ /usr/bin/time -f "Time: %U" uname&gt; command_output.txt 2&gt;time.log$ cat time.logTime: 0.00$ cat command_output.txtLinux# 使用参数%Z显示系统页面大小：$ /usr/bin/time -f "Page size: %Z bytes" ls&gt; /dev/nullPage size: 4096 bytes 三种不同类型的时： Real时间指的是挂钟时间（wall clock time），也就是命令从开始执行到结束的时间。这段时间包括其他进程所占用的时间片（time slice）以及进程被阻塞时所花费的时间（例如，为等待I/O操作完成所用的时间）。 User时间是指进程花费在用户模式（内核之外）中的CPU时间。这是唯一真正用于执行进程所花费的时间。执行其他进程以及花费在阻塞状态中的时间并没有计算在内。 Sys时间是指进程花费在内核中的CPU时间。它代表在内核中执行系统调用所使用的时间，这和库代码（library code）不同，后者仍旧运行在用户空间。与“user时间”类似，这也是真正由进程使用的CPU时间。 time命令 一些可以使用的参数： 参数 描述 %C 进行计时的命令名称以及命令行参数 %D 进程非共享数据区域的大小，以KB为单位 %E 进程使用的real时间（挂钟时间），显示格式为[小时:]分钟:秒 %x 命令的退出状态 %k 进程接收到的信号数量 %W 进程被交换出主存的次数 %Z 系统的页面大小。这是一个系统常量，但在不同的系统中，这个常量值也不同 %P 进程所获得的CPU时间百分比。这个值等于user+system时间除以总运行时间。结果以百分比形式显示 %K 进程的平均总（data+stack+text）内存使用量，以KB为单位 %w 进程主动进行上下文切换的次数，例如等待I/O操作完成 %c 进程被迫进行上下文切换的次数（由于时间片到期） 8.3 收集与当前登录用户、启动日志及启动故障的相关信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 获取当前登录用户的相关信息：$ whoslynux pts/0 2010-09-29 05:24 (slynuxs-macbook-pro.local)slynux tty7 2010-09-29 07:08 (:0)# 获得有关登录用户更详细的信息：$ w 07:09:05 up 1:45, 2 users, load average: 0.12, 0.06, 0.02USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATslynux pts/0 slynuxs 05:24 0.00s 0.65s 0.11s sshd: slynuxslynux tty7 :0 07:08 1:45m 3.28s 0.26s gnome-session# 第一行列出了当前时间，系统运行时间，当前登录的用户数量以及过去的1分钟、 5分钟、 15分钟内的系统平均负载。接下来的每一行显示了每一个登录用户的详细信息，其中包括登录名、 TTY、远程主机、登录时间、空闲时间、自该用户登录后所使用的总CPU时间、当前运行进程所使用的CPU时间以及进程所对应的命令行。# 列出当前登录主机的用户列表：$ usersslynux slynux slynux hacker$ users | tr ' ' '\n' | sort | uniqslynuxhacker# 查看系统已经加电运行了多长时间：$ uptime21:44:33 up 3:17, 8 users, load average: 0.09, 0.14, 0.09$ uptime | grep -Po '\d&#123;2&#125;\:\d&#123;2&#125;\:\d&#123;2&#125;'# 获取上一次启动以及用户登录会话的信息：$ lastslynux tty7 :0 Tue Sep 28 18:27 still logged inreboot system boot 2.6.32-21-generic Tue Sep 28 18:10 - 21:46 (03:35)slynux pts/0 :0.0 Tue Sep 28 05:31 - crash (12:39)# last命令可以提供登录会话信息。它实际上是一个系统登录日志，包括了登录tty、登录时间、状态等信息。# last命令以日志文件/var/log/wtmp作为输入日志数据。它也可以用选项-f明确地指定日志文件。例如：$ last -f /var/log/wtmp# 获取单个用户登录会话的信息：$ last USER# 获取重启会话（reboot session）信息：$ last rebootreboot system boot 2.6.32-21-generi Tue Sep 28 18:10 - 21:48 (03:37)reboot system boot 2.6.32-21-generi Tue Sep 28 05:14 - 21:48 (16:33)# 获取失败的用户登录会话信息：$ lastbtest tty8 :0 Wed Dec 15 03:56 - 03:56 (00:00)slynux tty8 :0 Wed Dec 15 03:55 - 03:55 (00:00) 8.4 使用 watch 监视命令输出watch命令可以用来在终端中以固定的间隔监视命令输出。 123456789101112$ watch ls$ watch 'ls -l | grep "^d"'# 只列出目录# 命令默认每2秒更新一次输出。# -n SECOND指定更新输出的时间间隔。例如：$ watch -n 5 'ls -l'# 以5秒为间隔，监视ls -l的输出# 突出标示watch输出中的差异, -d 可以启用这一功能：$ watch -d 'COMMANDS' 8.5 用 logrotate 管理日志文件用一种被称为轮替（rotation）的技术来限制日志文件的体积，一旦它超过了限定的大小，就对其内容进行抽取（strip），同时将 日志文件中的旧条目存储到日志目录中的归档文件内。旧的日志文件就会得以保存以便随后参阅。 logrotate 的配置目录位于/etc/logrotate.d。 12345678910$ cat /etc/logrotate.d/program/var/log/program.log &#123;missingoknotifemptysize 30kcompressweeklyrotate 5create 0600 root root&#125; 配置文件中各个参数的含义： 参数 描述 missingok 如果日志文件丢失，则忽略；然后返回（不对日志文件进行轮替） notifempty 仅当源日志文件非空时才对其进行轮替 size 30k 限制实施轮替的日志文件的大小。可以用1M表示1MB compress 允许用gzip压缩较旧的日志 weekly 指定进行轮替的时间间隔。可以是weekly、 yearly或daily rotate 5 这是需要保留的旧日志文件的归档数量。在这里指定的是5，所以这些文件名将会是program.log.1.gz、 program.log.2.gz等直到program.log.5.gz create 0600 root root 指定所要创建的归档文件的模式、用户以及用户组 8.6 用 syslog 记录日志每一个标准应用进程都可以利用syslog记录日志信息。 使用命令logger通过syslogd记录日志。 Linux中一些重要的日志文件 ： 日志文件 描述 /var/log/boot.log 系统启动信息 /var/log/httpd Apache Web服务器日志 /var/log/messages 发布内核启动信息 /var/log/auth.log 用户认证日志 /var/log/dmesg 系统启动信息 /var/log/mail.log 邮件服务器日志 /var/log/Xorg.0.log X服务器日志 12345678910111213# 向系统日志文件/var/log/message中写入日志信息：$ logger This is a test log line$ tail -n 1 /var/log/messagesSep 29 07:47:44 slynux-laptop slynux: This is a test log line # 如果要记录特定的标记（tag），可以使用：$ logger -t TAG This is a message$ tail -n 1 /var/log/messagesSep 29 07:48:42 slynux-laptop TAG: This is a message# 但是当logger发送消息时，它用标记字符串来确定应该记录到哪一个日志文件中。 syslogd使用与日志相关联的TAG来决定应该将其记录到哪一个文件中。你可以从/etc/rsyslog.d/目录下的配置文件中看到标记字符串以及与其相关联的日志文件。# 要将另一个日志文件的最后一行记录到系统日志中，可以使用：$ logger -f /var/log/source.log 8.7 通过监视用户登录找出入侵者入侵者定义为：屡次试图登入系统达两分钟以上，并且期间的登录过程全部失败。凡是这类用户都应该被检测出来并生成包含以下细节信息的报告： 试图登录的账户 试图登录的次数 攻击者的IP地址 IP地址所对应的主机 进行登录的时间段 为了处理SSH登录失败的情况，还得知道用户认证会话日志会被记录在日志文件/var/log/auth.log中。脚本需要扫描这个日志文件来检测出失败的登录信息，执行各种检查来获取所需要的数据。我们可以用host命令找出IP地址所对应的主机。 8.8 监视磁盘活动12345678# 交互式监视, iotop的-o选项只显示出那些正在进行I/O活动的进程：$ iotop -o# 用于shell脚本的非交互式用法：$ iotop -b -n 2# 监视特定进程$ iotop -p PID 8.9 检查磁盘及文件系统错误使用fsck的各种选项对文件系统错误进行检查和修复。 12345678910111213141516171819202122232425# 要检查分区或文件系统的错误，只需要将路径作为fsck的参数：$ fsck /dev/sdb3fsck from util-linux 2.20.1e2fsck 1.42.5 (29-Jul-2012)HDD2 has been mounted 26 times without being checked, check forced.Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary informationHDD2: 75540/16138240 files (0.7% non-contiguous), 48756390/64529088 blocks# 检查/etc/fstab中所配置的所有文件系统：$ fsck -A# 该命令会依次检查/etc/fstab中列出的文件系统。 fstab文件对磁盘及其挂载点之间的映射关系进行了配置，以便于更便捷地挂载文件系统# 指定fsck自动修复错误，无需询问是否进行修复：$ fsck -a /dev/sda2# 模拟fsck要执行的操作：$ fsck -ANfsck from util-linux 2.20.1[/sbin/fsck.ext4 (1) -- /] fsck.ext4 /dev/sda8[/sbin/fsck.ext4 (1) -- /home] fsck.ext4 /dev/sda7[/sbin/fsck.ext3 (1) -- /media/Data] fsck.ext3 /dev/sda6 9 管理重任9.1 收集进程信息1234567891011121314151617181920212223242526272829303132333435# 为了包含更多的信息，可以使用-f（表示full）来显示多列，如下所示：$ ps -fUID PID PPID C STIME TTY TIME CMDslynux 1220 1219 0 18:18 pts/0 00:00:00 -bashslynux 1587 1220 0 18:59 pts/0 00:00:00 ps -f# 使用选项 -e（every）。选项-ax（all）也可以生成同样的输出。# 运行如下命令之一： ps –e， ps –ef， ps -ax或ps –axf。$ ps -e | headPID TTY TIME CMD1 ? 00:00:00 init2 ? 00:00:00 kthreadd3 ? 00:00:00 migration/04 ? 00:00:00 ksoftirqd/05 ? 00:00:00 watchdog/06 ? 00:00:00 events/07 ? 00:00:00 cpuset8 ? 00:00:00 khelper9 ? 00:00:00 netns# 用 -o 来指定想要显示的列，以便只打印出我们需要的内容。# -o 的参数以逗号操作符（,）作为定界符。值得注意的是，逗号操作符与它分隔的参数之间是没有空格的。# -e和过滤器结合使用没有任何实际效果，依旧会显示所有的进程。# 示例如下，其中comm表示COMMAND， pcpu表示CPU占用率：$ ps -eo comm,pcpu | headCOMMAND %CPUinit 0.0kthreadd 0.0migration/0 0.0ksoftirqd/0 0.0watchdog/0 0.0events/0 0.0cpuset 0.0khelper 0.0netns 0.0 选项-o可以使用不同的参数： 参数 描述 pcpu CPU占用率 pid 进程ID ppid 父进程ID pmem 内存使用率 comm 可执行文件名 cmd 简单命令 user 启动进程的用户 nice 优先级 time 累计的CPU时间 etime 进程启动后流逝的时间 tty 所关联的TTY设备 euid 有效用户ID stat 进程状态 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# top, 默认会输出一个占用CPU最多的进程列表。输出结果每隔几秒就会更新。$ top# 根据参数对ps输出进行排序$ ps -eo comm,pcpu --sort -pcpu | headCOMMAND %CPUXorg 0.1hald-addon-stor 0.0ata/0 0.0scsi_eh_0 0.0gnome-settings- 0.0init 0.0hald 0.0pulseaudio 0.0gdm-simple-gree 0.0$ ps -eo comm,pid,pcpu,pmem | grep bashbash 1255 0.0 0.3bash 1680 5.5 0.3# 找出给定命令名所对应的进程ID，在参数后加上=就可以移除列名。$ ps -C bash -o pid=12551680$ pgrep bash12551680# 如果不使用换行符作为定界符，而是要自行指定可以像下面这样：$ pgrep bash -d ":"1255:1680# 指定进程的用户（拥有者）列表：$ pgrep -u root,slynux COMMAND# 根据真实用户或ID以及有效用户或ID过滤ps输出 用 -u EUSER1,EUSER2 …，指定有效用户列表； 用 -U RUSER1,RUSER2 …，指定真实用户列表$ ps -u root -U root -o user,pcpu# 用TTY过滤ps输出, 可以通过指定进程所属的TTY选择ps的输出。用选项 -t指定TTY列表：$ ps -t pts/0,pts/1PID TTY TIME CMD1238 pts/0 00:00:00 bash1835 pts/1 00:00:00 bash1864 pts/0 00:00:00 ps# 进程线程的相关信息# 通常与进程线程相关的信息在ps输出中是看不到的。我们可以用选项 –L 在ps输出中显示线程的相关信息。这会显示出两列： NLWP和NLP。 NLWP是进程的线程数量， NLP是ps输出中每个条目的线程ID。例如：$ ps -eLf# 指定输出宽度以及所要显示的列# 可以按照你自己的使用方式来进行应用。尝试以下选项: -f ps –ef u ps -e u ps ps -e w（w表示宽松输出）# 显示进程的环境变量# 了解某个进程依赖哪些环境变量，这类信息我们通常都用得着。进程的运行方式可能极其依赖某组环境变量。我们可以利用环境变量调试并修复与进程相关的问题。$ ps -eo pid,cmd e | tail -n 31162 hald-addon-acpi: listening on acpid socket /var/run/acpid.socket1172 sshd: slynux [priv]1237 sshd: slynux@pts/01238 -bash USER=slynux LOGNAME=slynux HOME=/home/slynuxPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/gamesMAIL=/var/mail/slynux SHELL=/bin/bash SSH_CLIENT=10.211.55.2 49277 22SSH_CONNECTION=10.211.55.2 49277 10.211.55.4 22 SSH_TTY=/dev/pts/0 TERM=xterm-colorLANG=en_IN XDG_SESSION_COOKIE=d1e96f5cc8a7a3bc3a0a73e44c95121a-1286499339.592429-1573657095 9.2 which、 whereis、 file、 whatis与平均负载1234567891011121314151617181920212223# which, which命令用来找出某个命令的位置。$ which ls/bin/ls# whereis# whereis与which命令类似，但它不仅返回命令的路径，还能够打印出其对应的命令手册的位置以及命令源代码的路径（如果有的话）$ whereis lsls: /bin/ls /usr/share/man/man1/ls.1.gz# file$ file FILENAME# 该命令会打印出与该文件类型相关的细节信息。$ file /bin/ls/bin/ls: ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked(uses shared libs), for GNU/Linux 2.6.15, stripped# whatis, whatis命令会输出作为参数的命令的简短描述信息。$ whatis lsls (1) - list directory contents# 平均负载$ uptime12:40:53 up 6:16, 2 users, load average: 0.00, 0.00, 0.00 9.3 杀死进程以及发送或响应信号信号是Linux中的一种进程间通信机制。 当进程接收到一个信号时，它会通过执行对应的信号处理程序（signal handler）来进行响应。 123456789101112131415# 列出所有可用的信号：$ kill -l# 终止进程：$ kill PROCESS_ID_LIST# kill命令默认发出一个TERM信号。进程ID列表使用空格作为进程ID之间的定界符。# 要通过kill命令向进程发送指定的信号，可以使用：$ kill -s SIGNAL PID# 参数SIGNAL要么是信号名称，要么是信号编号。# 我们经常要强行杀死进程，可以使用：$ kill -s SIGKILL PROCESS_ID或者$ kill -9 PROCESS_ID 常用到的信号量： SIGHUP 1——对控制进程或终端的终结进行挂起检测（hangup detection） SIGINT 2——当按下Ctrl + C时发送该信号 SIGKILL 9——用于强行杀死进程 SIGTERM 15——默认用于终止进程 SIGTSTP 20——当按下Ctrl + Z时发送该信号 1234567891011121314151617# killall命令通过命令名终止进程：$ killall process_name# 通过名称向进程发送信号：$ killall -s SIGNAL process_name# 通过名称强行杀死进程：$ killall -9 process_name# pkill命令和kill命令类似，不过默认情况下pkill接受的是进程名，而非进程ID。例如：$ pkill process_name$ pkill -s SIGNAL process_name# pkill不支持信号名称。# 捕捉并响应信号# trap命令在脚本中用来为信号分配信号处理程序。$ trap 'signal_handler_function_name' SIGNAL LIST 9.4 向用户终端发送消息1234567# wall命令用来向当前所有登录用户的终端写入消息。$ cat message | wall或者$ wall&lt; messageBroadcast Message from slynux@slynux-laptop(/dev/pts/1) at 12:54 ...This is a messag 9.5 采集系统信息1234567891011121314151617181920212223242526272829303132# 打印当前系统的主机名：$ hostname或者$ uname -n# 打印Linux内核版本、硬件架构等详细信息：$ uname -a# 打印内核发行版本：$ uname -r# 打印主机类型：$ uname -m# 打印CPU相关信息：$ cat /proc/cpuinfo# 获取处理器名称：$ cat /proc/cpuinfo | sed -n 5p# 打印内存的详细信息：$ cat /proc/meminfo# 打印系统可用内存总量：$ cat /proc/meminfo | head -1MemTotal: 1026096 kB# 列出系统的分区信息：$ cat /proc/partitions或者$ fdisk -l #如果没有输出，切换到root用户执行该命令# 获取系统的详细信息：$ lshw #建议以root用户来执行 9.6 使用 proc 采集信息以Bash为例，它的进程ID是4295（pgrep bash），那么就会有一个对应的目录/proc/4295。进程对应的目录中包含了大量有关进程的信息。 /proc/PID中一些重要的文件如下所示。 environ：包含与进程相关的环境变量。使用cat /proc/4295/environ，可以显示所有传递给该进程的环境变量 cwd： 是一个到进程工作目录（working directory）的符号链接 exe：是一个到当前进程所对应的可执行文件的符号链接 $ readlink /proc/4295/exe/bin/bash fd：包含了进程所使用的文件描述符 9.7 用 cron 进行调度crontab任务配置基本格式： 12* * * * * command分钟(0-59) 小时(0-23) 日期(1-31) 月份(1-12) 星期(0-6,0代表星期天) 命令 cron表中的每一个条目都由6部分组成，并按照下列顺序排列： 分钟（0～59） 小时（0～23） 天（1～31） 月份（1～12） 工作日（0～6） 命令（在指定时间执行的脚本或命令） 星号（*）指定命令应该在每个时间段执行。 除了数字还有几个个特殊的符号就是 &quot;*&quot; 、&quot;/&quot; 和 &quot;-&quot; 、&quot;,&quot; ，* 代表所有的取值范围内的数字，&quot;/&quot; 代表每的意思, &quot;*/5&quot; 表示每5个单位，&quot;-&quot; 代表从某个数字到某个数字, &quot;,&quot; 分开几个离散的数字。以下举几个例子说明问题： 1234567891011121314151617181920212223242526272829# 指定每小时的第5分钟执行一次ls命令5 * * * * ls # 指定每天的 5:30 执行ls命令30 5 * * * ls # 指定每月8号的7：30分执行ls命令30 7 8 * * ls # 指定每年的6月8日5：30执行ls命令30 5 8 6 * ls # 指定每星期日的6:30执行ls命令 [ 注：0表示星期天，1表示星期1，以此类推，也可以用英文来表示，sun表示星期天，mon表示星期一等。 ]30 6 * * 0 ls # 每月10号及20号的3：30执行ls命令 [注：“，”用来连接多个不连续的时段 ]30 3 10,20 * * ls # 每天8-11点的第25分钟执行ls命令 [注：“-”用来连接连续的时段 ]25 8-11 * * * ls # 每15分钟执行一次ls命令 [即每个小时的第0 15 30 45 60分钟执行ls命令 ]*/15 * * * * ls # 每个月中，每隔10天6:30执行一次ls命令[即每月的1、11、21、31日是的6：30执行一次ls命令。 ]30 6 */10 * * ls # 每天7：50以root 身份执行/etc/cron.daily目录中的所有可执行文件50 7 * * * root run-parts /etc/cron.daily # [ 注：run-parts参数表示，执行后面目录中的所有可执行文件。 ] 配置用户定时任务的语法： 123$ crontab [-u user]file$ crontab -u user[-i] 参数与说明： crontab -u //设定某个用户的cron服务 crontab -l //列出某个用户cron服务的详细内容 crontab -r //删除没个用户的cron服务 crontab -e //编辑某个用户的cron服务 9.8 从终端截图12345678# 取整个屏幕：$ import -window root screenshot.png# 手动选择部分区域进行抓取：$ import screenshot.png# 抓取特定窗口：$ import -window window_id screenshot.png]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令eval的用法]]></title>
    <url>%2F2017%2F04%2F13%2FLinux%E5%91%BD%E4%BB%A4eval%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[eval command-line其中command－line是在终端上键入的一条普通命令行。然而当在它前面放上eval时，其结果是shell在执行命令行之前扫描它两次。如： 123$ pipe="|"$ eval ls $pipe wc -l shell第1次扫描命令行时，它替换出pipe的值｜，接着eval使它再次扫描命令行，这时shell把｜作为管道符号了。 如果变量中包含任何需要shell直接在命令行中看到的字符（不是替换的结果），就可以使用eval。命令行结束符（；｜ &amp;），I／o重定向符（&lt; &gt;）和引号就属于对shell具有特殊意义的符号，必须直接出现在命令行中。 eval echo $$# 取得最后一个参数如： 12345$ cat lasteval echo $$#$ ./last one two three fourfour 第一遍扫描后，shell把反斜杠去掉了。当shell再次扫描该行时，它替换了$4的值，并执行echo命令 以下示意如何用eval命令创建指向变量的“指针”：1234567891011$ x=100$ ptrx=x$ eval echo $$ptrx #指向ptrx，用这里的方法可以理解b中的例子100 打印100$ eval $ptrx=50 #将50存到ptrx指向的变量中。$ echo $x50 打印50]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Linux Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议原理分析]]></title>
    <url>%2F2017%2F04%2F13%2FHTTP%E5%8D%8F%E8%AE%AE%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[文章来自深入理解HTTP协议、HTTP协议原理分析 基础概念篇介绍HTTP是Hyper Text Transfer Protocol（超文本传输协议）的缩写。它的发展是万维网协会（World Wide Web Consortium）和Internet工作小组IETF（Internet Engineering Task Force）合作的结果，（他们）最终发布了一系列的RFC，RFC 1945定义了HTTP/1.0版本。其中最著名的就是RFC 2616。RFC 2616定义了今天普遍使用的一个版本——HTTP 1.1。 HTTP协议（HyperText Transfer Protocol，超文本传输协议）是用于从WWW服务器传输超文本到本地浏览器的传送协议。它可以使浏览器更加高效，使网络传输减少。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示(如文本先于图形)等。 HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。HTTP是一个无状态的协议。 在TCP/IP协议栈中的位置HTTP协议通常承载于TCP协议之上，有时也承载于TLS或SSL协议层之上，这个时候，就成了我们常说的HTTPS。如下图所示： 默认HTTP的端口号为80，HTTPS的端口号为443。 HTTP的请求响应模型HTTP协议永远都是客户端发起请求，服务器回送响应。见下图： 这样就限制了使用HTTP协议，无法实现在客户端没有发起请求的时候，服务器将消息推送给客户端。 HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求是没有对应关系。 工作流程一次HTTP操作称为一个事务，其工作过程可分为四步： 1）首先客户机与服务器需要建立连接。只要单击某个超级链接，HTTP的工作开始。 2）建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容。 3）服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。 4）客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上，然后客户机与服务器断开连接。 如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。对于用户来说，这些过程是由HTTP自己完成的，用户只要用鼠标点击，等待信息显示就可以了。 使用Wireshark抓TCP、http包打开Wireshark，选择工具栏上的“Capture”-&gt;“Options”。 一般读者只需要选择最上边的下拉框，选择合适的Device，而后点击“Capture Filter”，此处选择的是“HTTP TCP port（80）”，选择后点击上图的“Start”开始抓包。 例如在浏览器中打开 http://image.baidu.com/，抓包如下图所示： 在上图中，可清晰的看到客户端浏览器（ip为192.168.2.33）与服务器的交互过程： 1）No1：浏览器（192.168.2.33）向服务器（220.181.50.118）发出连接请求。此为TCP三次握手第一步，此时从图中可以看出，为SYN，seq:X （x=0） 2）No2：服务器（220.181.50.118）回应了浏览器（192.168.2.33）的请求，并要求确认，此时为：SYN，ACK，此时seq：y（y为0），ACK：x+1（为1）。此为三次握手的第二步； 3）No3：浏览器（192.168.2.33）回应了服务器（220.181.50.118）的确认，连接成功。为：ACK，此时seq：x+1（为1），ACK：y+1（为1）。此为三次握手的第三步； 4）No4：浏览器（192.168.2.33）发出一个页面HTTP请求； 5）No5：服务器（220.181.50.118）确认； 6）No6：服务器（220.181.50.118）发送数据； 7）No7：客户端浏览器（192.168.2.33）确认； 8）No14：客户端（192.168.2.33）发出一个图片HTTP请求； 9）No15：服务器（220.181.50.118）发送状态响应码200 OK …… 头域每个头域由一个域名，冒号（:）和域值三部分组成。域名是大小写无关的，域值前可以添加任何数量的空格符，头域可以被扩展为多行，在每行开始处，使用至少一个空格或制表符。 在抓包的图中，No14点开可看到如下图所示： 回应的消息如下图所示： host头域Host头域指定请求资源的Intenet主机和端口号，必须表示请求url的原始服务器或网关的位置。HTTP/1.1请求必须包含主机头域，否则系统会以400状态码返回。 上图中host那行为： Referer头域Referer头域允许客户端指定请求uri的源资源地址，这可以允许服务器生成回退链表，可用来登陆、优化cache等。他也允许废除的或错误的连接由于维护的目的被追踪。如果请求的uri没有自己的uri地址，Referer不能被发送。如果指定的是部分uri地址，则此地址应该是一个相对地址。 在图4中，Referer行的内容为： User-Agent头域User-Agent头域的内容包含发出请求的用户信息。 在图4中，User-Agent行的内容为： Cache-Control头域Cache-Control指定请求和响应遵循的缓存机制。在请求消息或响应消息中设置Cache-Control并不会修改另一个消息处理过程中的缓存处理过程。请求时的缓存指令包括no-cache、no-store、max-age、max-stale、min-fresh、only-if-cached，响应消息中的指令包括public、private、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age。 在图5中的该头域为： Date头域Date头域表示消息发送的时间，时间的描述格式由rfc822定义。例如，Date:Mon,31Dec200104:25:57GMT。Date描述的时间表示世界标准时，换算成本地时间，需要知道用户所在的时区。 图5中，该头域如下图所示： HTTP的几个重要概念连接：connection一个传输层的实际环流，它是建立在两个相互通讯的应用程序之间。 在http1.1，request和reponse头中都有可能出现一个connection的头，此header的含义是当client和server通信时对于长链接如何进行处理。 在http1.1中，client和server都是默认对方支持长链接的， 如果client使用http1.1协议，但又不希望使用长链接，则需要在header中指明connection的值为close；如果server方也不想支持长链接，则在response中也需要明确说明connection的值为close。不论request还是response的header中包含了值为close的connection，都表明当前正在使用的tcp链接在当天请求处理完毕后会被断掉。以后client再进行新的请求时就必须创建新的tcp链接了。 消息：MessageHTTP通讯的基本单位，包括一个结构化的八元组序列并通过连接传输。 请求：Request一个从客户端到服务器的请求信息包括应用于资源的方法、资源的标识符和协议的版本号。 响应：Response一个从服务器返回的信息包括HTTP协议的版本号、请求的状态(例如“成功”或“没找到”)和文档的MIME类型。 资源：Resource由URI标识的网络数据对象或服务。 实体：Entity数据资源或来自服务资源的回映的一种特殊表示方法，它可能被包围在一个请求或响应信息中。一个实体包括实体头信息和实体的本身内容。 客户机：Client一个为发送请求目的而建立连接的应用程序。 用户代理：UserAgent初始化一个请求的客户机。它们是浏览器、编辑器或其它用户工具。 服务器：Server一个接受连接并对请求返回信息的应用程序。 源服务器：Originserver是一个给定资源可以在其上驻留或被创建的服务器。 代理：Proxy一个中间程序，它可以充当一个服务器，也可以充当一个客户机，为其它客户机建立请求。请求是通过可能的翻译在内部或经过传递到其它的服务器中。一个代理在发送请求信息之前，必须解释并且如果可能重写它。 代理经常作为通过防火墙的客户机端的门户，代理还可以作为一个帮助应用来通过协议处理没有被用户代理完成的请求。 网关：Gateway一个作为其它服务器中间媒介的服务器。与代理不同的是，网关接受请求就好象对被请求的资源来说它就是源服务器；发出请求的客户机并没有意识到它在同网关打交道。 网关经常作为通过防火墙的服务器端的门户，网关还可以作为一个协议翻译器以便存取那些存储在非HTTP系统中的资源。 通道：Tunnel是作为两个连接中继的中介程序。一旦激活，通道便被认为不属于HTTP通讯，尽管通道可能是被一个HTTP请求初始化的。当被中继的连接两端关闭时，通道便消失。当一个门户(Portal)必须存在或中介(Intermediary)不能解释中继的通讯时通道被经常使用。 缓存：Cache反应信息的局域存储。 附录：参考资料ttp_百度百科 结果编码和http状态响应码 分析TCP的三次握手 使用Wireshark来检测一次HTTP连接过程 http协议的几个重要概念 http协议中connection头的作用 协议详解篇HTTP/1.0和HTTP/1.1的比较RFC 1945定义了HTTP/1.0版本，RFC 2616定义了HTTP/1.1版本。 笔者在blog上提供了这两个RFC中文版的下载地址。 RFC1945下载地址： （HTTP）中文版.rar RFC2616下载地址： （HTTP）中文版.rar 建立连接方面HTTP/1.0 每次请求都需要建立新的TCP连接，连接不能复用。HTTP/1.1 新的请求可以在上次请求建立的TCP连接之上发送，连接可以复用。优点是减少重复进行TCP三次握手的开销，提高效率。 注意：在同一个TCP连接中，新的请求需要等上次请求收到响应后，才能发送。 Host域HTTP1.1在Request消息头里头多了一个Host域, HTTP1.0则没有这个域。 Eg： GET /pub/WWW/TheProject.html HTTP/1.1 Host: www.w3.org ​ 可能HTTP1.0的时候认为，建立TCP连接的时候已经指定了IP地址，这个IP地址上只有一个host。 日期时间戳(接收方向) 无论是HTTP1.0还是HTTP1.1，都要能解析下面三种date/time stamp： Sun, 06 Nov 1994 08:49:37 GMT ; RFC 822, updated by RFC 1123Sunday, 06-Nov-94 08:49:37 GMT ; RFC 850, obsoleted by RFC 1036Sun Nov 6 08:49:37 1994 ; ANSI C’s asctime() format (发送方向) HTTP1.0要求不能生成第三种asctime格式的date/time stamp； HTTP1.1则要求只生成RFC 1123(第一种)格式的date/time stamp。 状态响应码状态响应码100 (Continue) 状态代码的使用，允许客户端在发request消息body之前先用request header试探一下server，看server要不要接收request body，再决定要不要发request body。 客户端在Request头部中包含 Expect: 100-continue ​ Server看到之后呢如果回100 (Continue) 这个状态代码，客户端就继续发request body。这个是HTTP1.1才有的。 另外在HTTP/1.1中还增加了101、203、205等等性状态响应码 请求方式HTTP1.1增加了OPTIONS, PUT, DELETE, TRACE, CONNECT这些Request方法. ​ Method = “OPTIONS“ ; Section 9.2 ​ | “GET” ; Section 9.3 ​ | “HEAD” ; Section 9.4 ​ | “POST” ; Section 9.5 ​ | “PUT“ ; Section 9.6 ​ | “DELETE“ ; Section 9.7 ​ | “TRACE“ ; Section 9.8 ​ | “CONNECT“ ; Section 9.9 ​ | extension-method ​ extension-method = token HTTP请求消息请求消息格式请求消息格式如下所示： 请求行 通用信息头|请求头|实体头 CRLF(回车换行) 实体内容 其中“请求行”为：请求行 = 方法 [空格] 请求URI [空格] 版本号 [回车换行] 请求行实例： 12345678910111213141516Eg1：GET /index.html HTTP/1.1Eg2：POST http://192.168.2.217:8080/index.jsp HTTP/1.1HTTP请求消息实例：GET /hello.htm HTTP/1.1Accept: /Accept-Language: zh-cnAccept-Encoding: gzip, deflateIf-Modified-Since: Wed, 17 Oct 2007 02:15:55 GMTIf-None-Match: W/&quot;158-1192587355000&quot;User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)Host: 192.168.2.162:8080Connection: Keep-Alive 请求方法HTTP的请求方法包括如下几种： GET POST HEAD PUT DELETE OPTIONS TRACE CONNECT HTTP响应消息响应消息格式HTTP响应消息的格式如下所示： 状态行 通用信息头|响应头|实体头 CRLF 实体内容 其中：状态行 = 版本号 [空格] 状态码 [空格] 原因 [回车换行] 状态行举例： 1234567891011121314Eg1：HTTP/1.0 200 OK Eg2：HTTP/1.1 400 Bad Request HTTP响应消息实例如下所示：HTTP/1.1 200 OKETag: W/&quot;158-1192590101000&quot;Last-Modified: Wed, 17 Oct 2007 03:01:41 GMTContent-Type: text/htmlContent-Length: 158Date: Wed, 17 Oct 2007 03:01:59 GMTServer: Apache-Coyote/1.1 htpp的状态响应码1**：请求收到，继续处理100——客户必须继续发出请求 101——客户要求服务器根据请求转换HTTP协议版本 2**：操作成功收到，分析、接受200——交易成功 201——提示知道新文件的URL 202——接受和处理、但处理未完成 203——返回信息不确定或不完整 204——请求收到，但返回信息为空 205——服务器完成了请求，用户代理必须复位当前已经浏览过的文件 206——服务器已经完成了部分用户的GET请求 3**：完成此请求必须进一步处理300——请求的资源可在多处得到 301——删除请求数据 302——在其他地址发现了请求数据 303——建议客户访问其他URL或访问方式 304——客户端已经执行了GET，但文件未变化 305——请求的资源必须从服务器指定的地址得到 306——前一版本HTTP中使用的代码，现行版本中不再使用 307——申明请求的资源临时性删除 4**：请求包含一个错误语法或者不能完成400——错误请求，如语法错误 401——未授权 HTTP 401.1 - 未授权：登录失败 HTTP 401.2 - 未授权：服务器配置问题导致登录失败 HTTP 401.3 - ACL 禁止访问资源 HTTP 401.4 - 未授权：授权被筛选器拒绝 HTTP 401.5 - 未授权：ISAPI 或 CGI 授权失败 402——保留有效ChargeTo头响应 403——禁止访问 HTTP 403.1 禁止访问：禁止可执行访问 HTTP 403.2 - 禁止访问：禁止读访问 HTTP 403.3 - 禁止访问：禁止写访问 HTTP 403.4 - 禁止访问：要求 SSL HTTP 403.5 - 禁止访问：要求 SSL 128 HTTP 403.6 - 禁止访问：IP 地址被拒绝 HTTP 403.7 - 禁止访问：要求客户证书 HTTP 403.8 - 禁止访问：禁止站点访问 HTTP 403.9 - 禁止访问：连接的用户过多 HTTP 403.10 - 禁止访问：配置无效 HTTP 403.11 - 禁止访问：密码更改 HTTP 403.12 - 禁止访问：映射器拒绝访问 HTTP 403.13 - 禁止访问：客户证书已被吊销 HTTP 403.15 - 禁止访问：客户访问许可过多 HTTP 403.16 - 禁止访问：客户证书不可信或者无效 HTTP 403.17 - 禁止访问：客户证书已经到期或者尚未生效 404——没有发现文件、查询或URl 405——用户在Request-Line字段定义的方法不允许 406——根据用户发送的Accept拖，请求资源不可访问 407——类似401，用户必须首先在代理服务器上得到授权 408——客户端没有在用户指定的饿时间内完成请求 409——对当前资源状态，请求不能完成 410——服务器上不再有此资源且无进一步的参考地址 411——服务器拒绝用户定义的Content-Length属性请求 412——一个或多个请求头字段在当前请求中错误 413——请求的资源大于服务器允许的大小 414——请求的资源URL长于服务器允许的长度 415——请求资源不支持请求项目格式 416——请求中包含Range请求头字段，在当前请求资源范围内没有range指示值，请求也不包含If-Range请求头字段 417——服务器不满足请求Expect头字段指定的期望值，如果是代理服务器，可能是下一级服务器不能满足请求长。 5**：服务器执行一个完全有效请求失败HTTP 500 - 内部服务器错误 HTTP 500.100 - 内部服务器错误 - ASP 错误 HTTP 500-11 服务器关闭 HTTP 500-12 应用程序重新启动 HTTP 500-13 - 服务器太忙 HTTP 500-14 - 应用程序无效 HTTP 500-15 - 不允许请求 global.asa Error 501 - 未实现 HTTP 502 - 网关错误 使用Telnet进行http测试 在Windows下，可使用命令窗口进行http简单测试。 输入cmd进入命令窗口，在命令行键入如下命令后按回车： 1$ telnet www.baicu.com 80 而后在窗口中按下“Ctrl+]”后按回车可让返回结果回显。 接着开始发请求消息，例如发送如下请求消息请求baidu的首页消息，使用的HTTP协议为HTTP/1.1： GET /index.html HTTP/1.1 注意：copy如上的消息到命令窗口后需要按两个回车换行才能得到响应的消息，第一个回车换行是在命令后键入回车换行，是HTTP协议要求的。第二个是确认输入，发送请求。 可看到返回了200 OK的消息，如下图所示： 可看到，当采用HTTP/1.1时，连接不是在请求结束后就断开的。若采用HTTP1.0，在命令窗口键入： GET /index.html HTTP/1.0 此时可以看到请求结束之后马上断开。 读者还可以尝试在使用GET或POST等时，带上头域信息，例如键入如下信息： GET /index.html HTTP/1.1connection: closeHost: www.baidu.com 常用的请求方式常用的请求方式是GET和POST. GET方式：是以实体的方式得到由请求URI所指定资源的信息，如果请求URI只是一个数据产生过程，那么最终要在响应实体中返回的是处理过程的结果所指向的资源，而不是处理过程的描述。 POST方式：用来向目的服务器发出请求，要求它接受被附在请求后的实体，并把它当作请求队列中请求URI所指定资源的附加新子项，Post被设计成用统一的方法实现下列功能： 1：对现有资源的解释； 2：向电子公告栏、新闻组、邮件列表或类似讨论组发信息； 3：提交数据块； 4：通过附加操作来扩展数据库 。 从上面描述可以看出，Get是向服务器发索取数据的一种请求；而Post是向服务器提交数据的一种请求，要提交的数据位于信息头后面的实体中。 GET与POST方法有以下区别： （1） 在客户端，Get方式在通过URL提交数据，数据在URL中可以看到；POST方式，数据放置在HTML HEADER内提交。 （2） GET方式提交的数据最多只能有1024字节，而POST则没有此限制。 （3） 安全性问题。正如在（1）中提到，使用 Get 的时候，参数会显示在地址栏上，而 Post 不会。所以，如果这些数据是中文数据而且是非敏感数据，那么使用 get；如果用户输入的数据不是中文字符而且包含敏感数据，那么还是使用 post为好。 （4） 安全的和幂等的。所谓安全的意味着该操作用于获取信息而非修改信息。幂等的意味着对同一 URL 的多个请求应该返回同样的结果。完整的定义并不像看起来那样严格。换句话说，GET 请求一般不应产生副作用。从根本上讲，其目标是当用户打开一个链接时，她可以确信从自身的角度来看没有改变资源。比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。反之亦然。POST 请求就不那么轻松了。POST 表示可能改变服务器上的资源的请求。仍然以新闻站点为例，读者对文章的注解应该通过 POST 请求实现，因为在注解提交之后站点已经不同了（比方说文章下面出现一条注解）。 请求头HTTP最常见的请求头如下： Accept：浏览器可接受的MIME类型； Accept-Charset：浏览器可接受的字符集； Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间； Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到； Authorization：授权信息，通常出现在对服务器发送的WWW-Authenticate头的应答中； Connection：表示是否需要持久连接。如果Servlet看到这里的值为“Keep-Alive”，或者看到请求使用的是HTTP 1.1（HTTP 1.1默认进行持久连接），它就可以利用持久连接的优点，当页面包含多个元素时（例如Applet，图片），显著地减少下载所需要的时间。要实现这一点，Servlet需要在应答中发送一个Content-Length头，最简单的实现方法是：先把内容写入ByteArrayOutputStream，然后在正式写出内容之前计算它的大小； Content-Length：表示请求消息正文的长度； Cookie：这是最重要的请求头信息之一； From：请求发送者的email地址，由一些特殊的Web客户程序使用，浏览器不会用到它； Host：初始URL中的主机和端口； If-Modified-Since：只有当所请求的内容在指定的日期之后又经过修改才返回它，否则返回304“Not Modified”应答； Pragma：指定“no-cache”值表示服务器必须返回一个刷新后的文档，即使它是代理服务器而且已经有了页面的本地拷贝； Referer：包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。 User-Agent：浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用； UA-Pixels，UA-Color，UA-OS，UA-CPU：由某些版本的IE浏览器所发送的非标准的请求头，表示屏幕大小、颜色深度、操作系统和CPU类型。 响应头HTTP最常见的响应头如下所示： Allow：服务器支持哪些请求方法（如GET、POST等）； Content-Encoding：文档的编码（Encode）方法。只有在解码之后才可以得到Content-Type头指定的内容类型。利用gzip压缩文档能够显著地减少HTML文档的下载时间。Java的GZIPOutputStream可以很方便地进行gzip压缩，但只有Unix上的Netscape和Windows上的IE 4、IE 5才支持它。因此，Servlet应该通过查看Accept-Encoding头（即request.getHeader(“Accept-Encoding”)）检查浏览器是否支持gzip，为支持gzip的浏览器返回经gzip压缩的HTML页面，为其他浏览器返回普通页面； Content-Length：表示内容长度。只有当浏览器使用持久HTTP连接时才需要这个数据。如果你想要利用持久连接的优势，可以把输出文档写入ByteArrayOutputStram，完成后查看其大小，然后把该值放入Content-Length头，最后通过byteArrayStream.writeTo(response.getOutputStream()发送内容； Content-Type： 表示后面的文档属于什么MIME类型。Servlet默认为text/plain，但通常需要显式地指定为text/html。由于经常要设置Content-Type，因此HttpServletResponse提供了一个专用的方法setContentTyep。 可在web.xml文件中配置扩展名和MIME类型的对应关系； Date：当前的GMT时间。你可以用setDateHeader来设置这个头以避免转换时间格式的麻烦； Expires：指明应该在什么时候认为文档已经过期，从而不再缓存它。 Last-Modified：文档的最后改动时间。客户可以通过If-Modified-Since请求头提供一个日期，该请求将被视为一个条件GET，只有改动时间迟于指定时间的文档才会返回，否则返回一个304（Not Modified）状态。Last-Modified也可用setDateHeader方法来设置； Location：表示客户应当到哪里去提取文档。Location通常不是直接设置的，而是通过HttpServletResponse的sendRedirect方法，该方法同时设置状态代码为302； Refresh：表示浏览器应该在多少时间之后刷新文档，以秒计。除了刷新当前文档之外，你还可以通过setHeader(“Refresh”, “5; URL=””)让浏览器读取指定的页面。注意这种功能通常是通过设置HTML页面HEAD区的&lt;META HTTP-EQUIV=”Refresh” CONTENT=”5;URL=””&gt;实现，这是因为，自动刷新或重定向对于那些不能使用CGI或Servlet的HTML编写者十分重要。但是，对于Servlet来说，直接设置Refresh头更加方便。注意Refresh的意义是“N秒之后刷新本页面或访问指定页面”，而不是“每隔N秒刷新本页面或访问指定页面”。因此，连续刷新要求每次都发送一个Refresh头，而发送204状态代码则可以阻止浏览器继续刷新，不管是使用Refresh头还是&lt;META HTTP-EQUIV=”Refresh” …&gt;。注意Refresh头不属于HTTP 1.1正式规范的一部分，而是一个扩展，但Netscape和IE都支持它。 实体头实体头用坐实体内容的元信息，描述了实体内容的属性，包括实体信息类型，长度，压缩方法，最后一次修改时间，数据有效性等。 Allow：GET,POST Content-Encoding：文档的编码（Encode）方法，例如：gzip，见“2.5 响应头”； Content-Language：内容的语言类型，例如：zh-cn； Content-Length：表示内容长度，eg：80，可参考“2.5响应头”； Content-Location：表示客户应当到哪里去提取文档，例如：http://www.dfdf.org/dfdf.html，可参考“2.5响应头”； Content-MD5：MD5 实体的一种MD5摘要，用作校验和。发送方和接受方都计算MD5摘要，接受方将其计算的值与此头标中传递的值进行比较。Eg1：Content-MD5: &lt;base64 of 128 MD5 digest&gt;。Eg2：dfdfdfdfdfdfdff==； Content-Range：随部分实体一同发送；标明被插入字节的低位与高位字节偏移，也标明此实体的总长度。Eg1：Content-Range: 1001-2000/5000，eg2：bytes 2543-4532/7898 Content-Type：标明发送或者接收的实体的MIME类型。Eg：text/html; charset=GB2312 主类型/子类型； Expires：为0证明不缓存； `Last-Modified`：WEB 服务器认为对象的最后修改时间，比如文件的最后修改时间，动态页面的最后产生时间等等。例如：Last-Modified：Tue, 06 May 2008 02:42:43 GMT. 扩展头在HTTP消息中，也可以使用一些再HTTP1.1正式规范里没有定义的头字段，这些头字段统称为自定义的HTTP头或者扩展头，他们通常被当作是一种实体头处理。 现在流行的浏览器实际上都支持Cookie,Set-Cookie,Refresh和Content-Disposition等几个常用的扩展头字段。 Refresh：1;url=http://www.dfdf.org //过1秒跳转到指定位置； Content-Disposition：头字段,可参考“2.5响应头”； Content-Type：WEB 服务器告诉浏览器自己响应的对象的类型。 eg1：Content-Type：application/xml ； eg2：applicaiton/octet-stream； Content-Disposition：attachment; filename=aaa.zip。 附录：参考资料HTTP1.1和HTTP1.0的区别 HTTP请求（GET和POST区别）和响应 HTTP请求头概述_百度知道 实体头和扩展头实体头和扩展头 深入了解篇Cookie和SessionCookie和Session都为了用来保存状态信息，都是保存客户端状态的机制，它们都是为了解决HTTP无状态的问题而所做的努力。 Session可以用Cookie来实现，也可以用URL回写的机制来实现。用Cookie来实现的Session可以认为是对Cookie更高级的应用。 两者比较Cookie和Session有以下明显的不同点： 1）Cookie将状态保存在客户端，Session将状态保存在服务器端； 2）Cookies是服务器在本地机器上存储的小段文本并随每一个请求发送至同一个服务器。Cookie最早在RFC2109中实现，后续RFC2965做了增强。网络服务器用HTTP头向客户端发送cookies，在客户终端，浏览器解析这些cookies并将它们保存为一个本地文件，它会自动将同一服务器的任何请求缚上这些cookies。Session并没有在HTTP的协议中定义； 3）Session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器； 4）就安全性来说：当你访问一个使用session 的站点，同时在自己机子上建立一个cookie，建议在服务器端的SESSION机制更安全些.因为它不会任意读取客户存储的信息。 Session机制Session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。 当程序需要为某个客户端的请求创建一个session的时候，服务器首先检查这个客户端的请求里是否已包含了一个session标识 - 称为 session id，如果已包含一个session id则说明以前已经为此客户端创建过session，服务器就按照session id把这个 session检索出来使用（如果检索不到，可能会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个 session id将被在本次响应中返回给客户端保存。 Session的实现机制使用Cookie来实现服务器给每个Session分配一个唯一的JSESSIONID，并通过Cookie发送给客户端。 当客户端发起新的请求的时候，将在Cookie头中携带这个JSESSIONID。这样服务器能够找到这个客户端对应的Session。 流程如下图所示： 使用URL回显来实现URL回写是指服务器在发送给浏览器页面的所有链接中都携带JSESSIONID的参数，这样客户端点击任何一个链接都会把JSESSIONID带会服务器。 如果直接在浏览器输入服务端资源的url来请求该资源，那么Session是匹配不到的。 Tomcat对Session的实现，是一开始同时使用Cookie和URL回写机制，如果发现客户端支持Cookie，就继续使用Cookie，停止使用URL回写。如果发现Cookie被禁用，就一直使用URL回写。jsp开发处理到Session的时候，对页面中的链接记得使用response.encodeURL() 。 在J2EE项目中Session失效的几种情况1）Session超时：Session在指定时间内失效，例如30分钟，若在30分钟内没有操作，则Session会失效，例如在web.xml中进行了如下设置： 123&lt;session-config&gt; &lt;session-timeout&gt;30&lt;/session-timeout&gt; //单位：分钟&lt;/session-config&gt; 2）使用 session.invalidate() 明确的去掉Session。 与Cookie相关的HTTP扩展头1）Cookie：客户端将服务器设置的Cookie返回到服务器； 2）Set-Cookie：服务器向客户端设置Cookie； 3）Cookie2 (RFC2965)）：客户端指示服务器支持Cookie的版本； 4）Set-Cookie2 (RFC2965)：服务器向客户端设置Cookie。 Cookie的流程服务器在响应消息中用Set-Cookie头将Cookie的内容回送给客户端，客户端在新的请求中将相同的内容携带在Cookie头中发送给服务器。从而实现会话的保持。 流程如下图所示： 缓存的实现原理什么是Web缓存WEB缓存(cache)位于Web服务器和客户端之间。 缓存会根据请求保存输出内容的副本，例如html页面，图片，文件，当下一个请求来到的时候：如果是相同的URL，缓存直接使用副本响应访问请求，而不是向源服务器再次发送请求。 HTTP协议定义了相关的消息头来使WEB缓存尽可能好的工作。 缓存的优点 减少相应延迟：因为请求从缓存服务器（离客户端更近）而不是源服务器被相应，这个过程耗时更少，让web服务器看上去相应更快。 减少网络带宽消耗：当副本被重用时会减低客户端的带宽消耗；客户可以节省带宽费用，控制带宽的需求的增长并更易于管理。 与缓存相关的HTTP扩展消息头 Expires：指示响应内容过期的时间，格林威治时间GMT Cache-Control：更细致的控制缓存的内容 Last-Modified：响应中资源最后一次修改的时间 ETag：响应中资源的校验值，在服务器上某个时段是唯一标识的。 Date：服务器的时间 If-Modified-Since：客户端存取的该资源最后一次修改的时间，同Last-Modified。 If-None-Match：客户端存取的该资源的检验值，同ETag。 客户端缓存生效的常见流程服务器收到请求时，会在200OK中回送该资源的Last-Modified和ETag头，客户端将该资源保存在cache中，并记录这两个属性。当客户端需要发送相同的请求时，会在请求中携带If-Modified-Since和If-None-Match两个头。两个头的值分别是响应中Last-Modified和ETag头的值。服务器通过这两个头判断本地资源未发生变化，客户端不需要重新下载，返回304响应。常见流程如下图所示： Web缓存机制HTTP/1.1中缓存的目的是为了在很多情况下减少发送请求，同时在许多情况下可以不需要发送完整响应。前者减少了网络回路的数量；HTTP利用一个“过期（expiration）”机制来为此目的。后者减少了网络应用的带宽；HTTP用“验证（validation）”机制来为此目的。 HTTP定义了3种缓存机制： 1）Freshness：允许一个回应消息可以在源服务器不被重新检查，并且可以由服务器和客户端来控制。例如，Expires回应头给了一个文档不可用的时间。Cache-Control中的max-age标识指明了缓存的最长时间； 2）Validation：用来检查以一个缓存的回应是否仍然可用。例如，如果一个回应有一个Last-Modified回应头，缓存能够使用If-Modified-Since来判断是否已改变，以便判断根据情况发送请求； 3）Invalidation： 在另一个请求通过缓存的时候，常常有一个副作用。例如，如果一个URL关联到一个缓存回应，但是其后跟着POST、PUT和DELETE的请求的话，缓存就会过期。 断点续传和多线程下载的实现原理 HTTP协议的GET方法，支持只请求某个资源的某一部分； 206 Partial Content 部分内容响应； Range 请求的资源范围； Content-Range 响应的资源范围； 在连接断开重连时，客户端只请求该资源未下载的部分，而不是重新请求整个资源，来实现断点续传。 分块请求资源实例： Eg1：*Range: bytes=306302- *：请求这个资源从306302个字节到末尾的部分； Eg2：Content-Range: bytes 306302-604047/604048：响应中指示携带的是该资源的第306302-604047的字节，该资源共604048个字节； 客户端通过并发的请求相同资源的不同片段，来实现对某个资源的并发分块下载。从而达到快速下载的目的。目前流行的FlashGet和迅雷基本都是这个原理。 多线程下载的原理： 下载工具开启多个发出HTTP请求的线程； 每个http请求只请求资源文件的一部分：Content-Range: bytes 20000-40000/47000； 合并每个线程下载的文件。 https通信过程什么是httpsHTTPS（全称：Hypertext Transfer Protocol over Secure Socket Layer），是以安全为目标的HTTP通道，简单讲是HTTP的安全版。即HTTP下加入SSL层，HTTPS的安全基础是SSL，因此加密的详细内容请看SSL。 见下图： https所用的端口号是443。 https的实现原理有两种基本的加解密算法类型： 1）对称加密：密钥只有一个，加密解密为同一个密码，且加解密速度快，典型的对称加密算法有DES、AES等； 2）非对称加密：密钥成对出现（且根据公钥无法推知私钥，根据私钥也无法推知公钥），加密解密使用不同密钥（公钥加密需要私钥解密，私钥加密需要公钥解密），相对对称加密速度较慢，典型的非对称加密算法有RSA、DSA等。 下面看一下https的通信过程： https通信的优点： 1）客户端产生的密钥只有客户端和服务器端能得到； 2）加密的数据只有客户端和服务器端才能得到明文； 3）客户端到服务端的通信是安全的。 http代理http代理服务器代理服务器英文全称是Proxy Server，其功能就是代理网络用户去取得网络信息。形象的说：它是网络信息的中转站。 代理服务器是介于浏览器和Web服务器之间的一台服务器，有了它之后，浏览器不是直接到Web服务器去取回网页而是向代理服务器发出请求，Request信号会先送到代理服务器，由代理服务器来取回浏览器所需要的信息并传送给你的浏览器。 而且，大部分代理服务器都具有缓冲的功能，就好象一个大的Cache，它有很大的存储空间，它不断将新取得数据储存到它本机的存储器上，如果浏览器所请求的数据在它本机的存储器上已经存在而且是最新的，那么它就不重新从Web服务器取数据，而直接将存储器上的数据传送给用户的浏览器，这样就能显著提高浏览速度和效率。 更重要的是：Proxy Server(代理服务器)是Internet链路级网关所提供的一种重要的安全功能，它的工作主要在开放系统互联(OSI)模型的对话层。 http代理服务器的主要功能主要功能如下： 1）突破自身IP访问限制，访问国外站点。如：教育网、169网等网络用户可以通过代理访问国外网站； 2）访问一些单位或团体内部资源，如某大学FTP(前提是该代理地址在该资源的允许访问范围之内)，使用教育网内地址段免费代理服务器，就可以用于对教育 网开放的各类FTP下载上传，以及各类资料查询共享等服务； 3）突破中国电信的IP封锁：中国电信用户有很多网站是被限制访问的，这种限制是人为的，不同Serve对地址的封锁是不同的。所以不能访问时可以换一个国 外的代理服务器试试； 4）提高访问速度：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度； 5）隐藏真实IP：上网者也可以通过这种方法隐藏自己的IP，免受攻击。 http代理图示http代理的图示见下图： 对于客户端浏览器而言，http代理服务器相当于服务器。 而对于Web服务器而言，http代理服务器又担当了客户端的角色。 虚拟主机的实现什么是虚拟主机虚拟主机：是在网络服务器上划分出一定的磁盘空间供用户放置站点、应用组件等，提供必要的站点功能与数据存放、传输功能。 所谓虚拟主机，也叫“网站空间”就是把一台运行在互联网上的服务器划分成多个“虚拟”的服务器，每一个虚拟主机都具有独立的域名和完整的Internet服务器（支持WWW、FTP、E-mail等）功能。一台服务器上的不同虚拟主机是各自独立的，并由用户自行管理。但一台服务器主机只能够支持一定数量的虚拟主机，当超过这个数量时，用户将会感到性能急剧下降。 虚拟主机的实现原理虚拟主机是用同一个WEB服务器，为不同域名网站提供服务的技术。Apache、Tomcat等均可通过配置实现这个功能。 相关的HTTP消息头：Host。 例如：Host: www.baidu.com 客户端发送HTTP请求的时候，会携带Host头，Host头记录的是客户端输入的域名。这样服务器可以根据Host头确认客户要访问的是哪一个域名。 附录：参考资料理解Cookie和Session机制 浅析HTTP协议 http代理_百度百科 虚拟主机_百度百科 https_百度百科 HTTP与HTTPS的区别 HTTPS 为什么更安全，先看这些]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP思维导图]]></title>
    <url>%2F2017%2F04%2F12%2FHTTP%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[文章来自HTTP思维导图。 HTTP mindmap整理 source from 《HTTP权威指南》 概述-Summary 报文-Message 连接-Connection 代理-Proxy 缓存-Cache 网关、隧道与中继-Gateway、Tunnel and Relay 识别-Identification 认证-Authentication 安全-Security 实体与编码-Entity and Encoding]]></content>
      <categories>
        <category>http</category>
      </categories>
      <tags>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Webpack创建、运行vue.js项目及其目录结构详解]]></title>
    <url>%2F2017%2F04%2F10%2FWebpack%E5%88%9B%E5%BB%BA%E3%80%81%E8%BF%90%E8%A1%8Cvue-js%E9%A1%B9%E7%9B%AE%E5%8F%8A%E5%85%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[项目环境搭建：1.安装node 进入node官网进行下载。 版本查看： 12$ node -vv6.10.1 **注意：**node版本最好新一点，推介6.0以上。 2.全局安装vue-cli 1$ npm install -g vue-cli 注意： 如果安装失败可能需要root权限重新安装。 3.创建一个基于 webpack 模板的新项目 12$ vue init webpack project-name #(默认安装2.0版本)$ vue init webpack#1.0 project-name #(安装1.0版本) 项目目录结构： main.js是入口文件，主要作用是初始化vue实例并使用需要的插件 123456789101112131415// The Vue build version to load with the `import` command// (runtime-only or standalone) has been set in webpack.base.conf with an alias.import Vue from 'vue'import App from './App'import router from './router'Vue.config.productionTip = false/* eslint-disable no-new */new Vue(&#123; el: '#app', router, template: '&lt;App/&gt;', components: &#123; App &#125;&#125;) App.vue是我们的主组件，所有页面都是在App.vue下进行切换的。其实你也可以理解为所有的路由也是App.vue的子组件。所以我将router标示为App.vue的子组件。 123456789101112131415161718192021222324252627&lt;template&gt; &lt;div id=&quot;app&quot;&gt; &lt;img src=&quot;./assets/logo.png&quot;&gt; &lt;router-view&gt;&lt;/router-view&gt; &lt;hello&gt;&lt;/hello&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;export default &#123; name: &apos;app&apos;, components: &#123; Hello &#125;&#125;&lt;/script&gt;&lt;style&gt;#app &#123; font-family: &apos;Avenir&apos;, Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; text-align: center; color: #2c3e50; margin-top: 60px;&#125;&lt;/style&gt; index.html文件入口 src放置组件和入口文件 node_modules为依赖的模块 config中配置了路径端口值等 build中配置了webpack的基本配置、开发环境配置、生产环境配置等 运行项目：1234$ cd project-name$ npm install$ npm run dev# 上述步骤都完成后在浏览器输入：localhost:8080]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>webpack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lighttpd 配置https]]></title>
    <url>%2F2017%2F03%2F31%2Flighttpd-%E9%85%8D%E7%BD%AEhttps%2F</url>
    <content type="text"><![CDATA[确定安装的lighttpd支持ssl版本信息中含有（ssl）字样的信息说明支持ssl，可以在终端输入如下查看： 123$ lighttpd -vlighttpd/1.4.35 (ssl) - a light and fast webserverBuild-Date: Apr 25 2017 10:25:18 生成自签名证书完整的ssl证书分为四个部分： CA根证书（root CA） 中级证书（Intermediate Certificate） 域名证书 证书秘钥（仅由开发者提供） 证书相当于公钥，pem相当于私钥。 Self-Signed Certificates：包含公钥和私钥的结合体，证书（公钥）会在连接请求的时候发给浏览器，以便浏览器解密和加密。 创建Self-Signed Certificates： 1$ openssl req -new -x509 -keyout server.pem -out server.pem -days 365 -nodes 上边的命令生成一个server.pem文件。 lighttpd.conf 配置1234$SERVER["socket"] == "[::]:443" &#123; ssl.engine = "enable" ssl.pemfile = "/mnt/flash/server.pem"&#125; 强制定向到HTTPS下面是 lighttpd.conf 文件中关于强制 HTTP 定向到 HTTPS 的部分配置： 1234567$HTTP["scheme"] == "http" &#123; # capture vhost name with regex conditiona -&gt; %0 in redirect pattern # must be the most inner block to the redirect rule $HTTP["host"] =~ ".*" &#123; url.redirect = (".*" =&gt; "https://%0$0") &#125;&#125; 此功能需要lighttpd mod_redirect 模块支持。使用此功能前确保模块已经安装。 lighttpd安全配置禁用 SSL Compression (抵御 CRIME 攻击) 1ssl.use-compression = "disable" 禁用 SSLv2 及 SSLv3 12ssl.use-sslv2 = "disable"ssl.use-sslv3 = "disable" 抵御 Poodle 和 SSL downgrade 攻击 需要支持 TLS-FALLBACK-SCSV 以自动开启此功能。下列 openSSL 版本包含对 TLS-FALLBACK-SCSV 的支持，lighttpd 会自动启用此特性。 OpenSSL 1.0.1 在 1.0.1j 及之后的版本中支持 OpenSSL 1.0.0 在 1.0.0o 及之后的版本中支持 OpenSSL 0.9.8 在 0.9.8zc 及之后的版本中支持 加密及交换算法 一份推介的配置： 1ssl.cipher-list = "EECDH+AESGCM:EDH+AESGCM:AES128+EECDH:AES128+EDH" 如果您需要兼容一些老式系统和浏览器 (例如 Windows XP 和 IE6)，请使用下面的： 1ssl.cipher-list = "EECDH+AESGCM:EDH+AESGCM:ECDHE-RSA-AES128-GCM-SHA256:AES256+EECDH:AES256+EDH:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-SHA:ECDHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES128-SHA256:DHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES256-GCM-SHA384:AES128-GCM-SHA256:AES256-SHA256:AES128-SHA256:AES256-SHA:AES128-SHA:DES-CBC3-SHA:HIGH:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4" 配置 Forward Secrecy 和 DHE 参数 生成强 DHE 参数： 12$ cd /etc/ssl/certs$ openssl dhparam -out dhparam.pem 4096 建议您使用性能强劲的平台生成此文件，例如最新版的至强物理机。如果您只有一台小型 VPS，请使用 openssl dhparam -out dhparam.pem 2048 命令生成 2048bit 的参数文件。 添加到 SSL 配置文件： 12ssl.dh-file = "/etc/ssl/certs/dhparam.pem"ssl.ec-curve = "secp384r1" 启用 HSTS 1234server.modules += ( "mod_setenv" )$HTTP["scheme"] == "https" &#123; setenv.add-response-header = ( "Strict-Transport-Security" =&gt; "max-age=63072000; includeSubdomains; preload")&#125; 参考资料Lighttpd]]></content>
      <tags>
        <tag>http</tag>
        <tag>lighttpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lighttpd+fastcgi]]></title>
    <url>%2F2017%2F03%2F31%2Flighttpd-fastcgi%2F</url>
    <content type="text"><![CDATA[简介lighttpd 提供了一种外部程序调用的接口，即 FastCGI 接口。这是一种独立于平台和服务器的接口，它介于Web应用程序和Web服务器之间。 这就意味着能够在 Apache 服务器上运行的 FastCGI 程序，也一定可以无缝的在 lighttpd 上使用。 FastCGI介绍1）就像 CGI 一样，FastCGI 也是独立于编程语言的。2）就像 CGI 一样，FastCGI 程序运行在完全独立于核心 Web Server 之外的进程中，和 API 方式相比，提供了很大的安全性。（API会将程序代码与核心Web Server挂接在一起，这就意味着基于问题API的应用程序可能会使整个Web Server或另一个应用程序崩溃；一个恶意API还可以从核心Web Server或另一个应用程序中盗取安全密钥） 3) 虽然 FastCGI 不能一夜之间复制CGI的所有功能，但是 FastCGI 一直宣扬开放，这也使得我们拥有很多免费的 FastCGI 应用程序库（C/C++、Java、Perl、TCL）和免费的Server模块（Apache、ISS、Lighttpd）。 4) 就像 CGI 一样，FastCGI 并不依附于任何 Web Server 的内部架构，因此即使 Server 的技术实现变动，FastCGI 仍然非常稳定；而 API 设计是反映 Web Server 内部架构的，因此，一旦架构改变，API要随之变动。 5) FastCGI 程序可以运行在任何机器上，完全可以和 Web Server 不在一台机器上。这种分布式计算的思想可以确保可扩展性、提高系统可用性和安全性。 6) CGI 程序主要是对 HTTP 请求做计算处理，而 FastCGI 却还可以做得更多，例如模块化认证、授权检查、数据类型转换等等。在未来，FastCGI 还会有能力扮演更多角色。 7) FastCGI 移除了 CGI 程序的许多弊端。例如，针对每一个新请求，WebServer 都必须重启 CGI 程序来处理新请求，这导致 WebServer 的性能会大受影响。而 FastCGI 通过保持进程处理运行状态并持续处理请求的方式解决了该问题，这就将进程创建和销毁的时间节省了出来。 8) CGI 程序需要通过管道（pipe）方式与 Web Server 通信，而 FastCGI 则是通过 Unix-Domain-Sockets 或 TCP/IP 方式来实现与 Web Server 的通信。这确保了 FastCGI 可以运行在 Web Server 之外的服务器上。FastCGI 提供了 FastCGI 负载均衡器，它可以有效控制多个独立的 FastCGI Server 的负载，这种方式比 load-balancer+apache+mod_php 方式能够承担更多的流量。 FastCGI 模块若要 lighttpd 支持 fastcgi，则需要配置如下内容： 在 fastcgi.conf 中配置 1server.modules += ( &quot;mod_fastcgi&quot; ) 及在 module.conf 中配置 1include &quot;conf.d/fastcgi.conf&quot; FastCGI 配置选项lighttpd 通过 fastcgi 模块的方式实现了对 fastcgi 的支持，并且在配置文件中提供了三个相关的选项： 1） fastcgi.debug 可以设置一个从0到65535的值，用于设定 FastCGI 模块的调试等级。当前仅有0和1可用。1表示开启调试（会输出调试信息），0表示禁用。例如： 1fastcgi.debug = 1 2） fastcgi.map-extentsions 同一个 fastcgi server 能够映射多个扩展名，如 .php3 和 .php4 都对应 .php。例如： 1fastcgi.map-extensions = ( &quot;.php3&quot; =&gt; &quot;.php&quot; ) or for multiple 1fastcgi.map-extensions = ( &quot;.php3&quot; =&gt; &quot;.php&quot;, &quot;.php4&quot; =&gt; &quot;.php&quot; ) 3） fastcgi.server 这个配置是告诉 Web Server 将 FastCGI 请求发送到哪里，其中每一个文件扩展名可以处理一个类型的请求。负载均衡器可以实现对同一扩展名的多个对象的负载均衡。 fastcgi.server 的结构语法如下： 1234567891011121314151617181920( &lt;extension&gt; =&gt; ( [ &lt;name&gt; =&gt; ] ( # Be careful: lighty does *not* warn you if it doesn't know a specified option here (make sure you have no typos) "host" =&gt; &lt;string&gt; , "port" =&gt; &lt;integer&gt; , "socket" =&gt; &lt;string&gt;, # either socket or host+port "bin-path" =&gt; &lt;string&gt;, # optional "bin-environment" =&gt; &lt;array&gt;, # optional "bin-copy-environment" =&gt; &lt;array&gt;, # optional "mode" =&gt; &lt;string&gt;, # optional "docroot" =&gt; &lt;string&gt; , # optional if "mode" is not "authorizer" "check-local" =&gt; &lt;string&gt;, # optional "max-procs" =&gt; &lt;integer&gt;, # optional - when omitted, default is 4 "broken-scriptfilename" =&gt; &lt;boolean&gt;, # optional "kill-signal" =&gt; &lt;integer&gt;, # optional, default is SIGTERM(15) (v1.4.14+) ), ( "host" =&gt; ... ) )) 其中： extentsion ：文件名后缀或以”/”开头的前缀（也可为文件名）name ：这是一个可选项，表示handler的名称，在mod_status中用于统计功能，可以清晰的分辨出是哪一个handler处理了。 host ：FastCGI进程监听的IP地址。此处不支持hostname形式。port ：FastCGI进程所监听的TCP端口号bin-path ：本地FastCGI二进制程序的路径，当本地没有FastCGI正在运行时，会启动这个FastCGI程序。socket ：unix-domain-socket所在路径。mode ：可以选择FastCGI协议的模式，默认是“responder”，还可以选择authorizer。docroot ：这是一个可选项，对于responder模式来讲，表示远程主机docroot；对于authorizer模式来说，它表示MANDATORY，并且指向授权请求的docroot。check_local ：这是一个可选项，默认是enable。如果是enable，那么server会首先在本地（server.document-root）目录中检查被请求的文件是否存在，如果不存在，则给用户返回404（Not Found），而不会把这个请求传递给FastCGI。如果是disable，那么server不会检查本地文件，而是直接将请求转发给FastCGI。（disable的话，server从某种意义上说就变为了一个转发器）broken-scriptfilename ：以类似PHP抽取PATH_INFO的方式，抽取URL中的SCRIPT_FILENAME。 如果 bin-path 被设置了，那么： max-procs ：设置多少个FastCGI进程被启动bin-environment ：在FastCGI进程启动时设置一个环境变量bin-copy-environment ：清除环境，并拷贝指定的变量到全新的环境中。kill-signal ：默认的话，在停止FastCGI进程时，lighttpd会发送SIGTERM(-15)信号给子进程。此处可以设置发送的信号。 举例 ： 使用前缀来对应主机： 123456789fastcgi.server = ( "/remote_scripts/" =&gt; (( "host" =&gt; "192.168.0.3", "port" =&gt; 9000, "check-local" =&gt; "disable", "docroot" =&gt; "/" # remote server may use # it's own docroot ))) 如果有一个请求 “http://my.example.org/remote_scripts/test.cgi&quot;，那么server会将其转发给192.168.0.3的9000端口，并且 SCRIPT_NAME 会被赋值为 “/remote_scripts/test.cgi”。如果所设置的 handler 的末尾不是 “/” ，那么会被认为是一个文件。 负载均衡 ： FastCGI 模块提供了一种在多台 FastCGI 服务器间负载均衡的方法。 例如： 123456789fastcgi.server = ( ".php" =&gt; ( ( "host" =&gt; "10.0.0.2", "port" =&gt; 1030 ), ( "host" =&gt; "10.0.0.3", "port" =&gt; 1030 ) ) ) 为了更好的理解负载均衡实现的原理，建议你置 fastcgi.debug 为 1 。即使对于本机的多个 FastCGI ，你也会获得如下输出： 123456789101112proc: 127.0.0.1 1031 1 1 1 31454proc: 127.0.0.1 1028 1 1 1 31442proc: 127.0.0.1 1030 1 1 1 31449proc: 127.0.0.1 1029 1 1 2 31447proc: 127.0.0.1 1026 1 1 2 31438got proc: 34 31454release proc: 40 31438proc: 127.0.0.1 1026 1 1 1 31438proc: 127.0.0.1 1028 1 1 1 31442proc: 127.0.0.1 1030 1 1 1 31449proc: 127.0.0.1 1031 1 1 2 31454proc: 127.0.0.1 1029 1 1 2 31447 上述信息显示出了IP地址，端口号、当前链接数（也就是负载）（倒数第二列）、进程ID（倒数第一列）等等。整个输出信息总是以负载域来从小到大排序的。 参考文献 说说lighttpd的fastcgi Nginx + CGI/FastCGI + C/Cpp FastCGI+lighttpd开发之介绍和环境搭建 附：QC V3 PP 版本 lighttpd.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091$ cat /etc/qtilighttpd.conf # ------------------------------------------------------------------------------# Copyright (c) 2016 Qualcomm Technologies, Inc.# All Rights Reserved.# Confidential and Proprietary - Qualcomm Technologies, Inc.# ------------------------------------------------------------------------------server.document-root = "/opt/qcom/www"server.port = 80server.username = "apps"server.groupname = "apps"server.bind = "0.0.0.0"server.tag = "lighttpd"$SERVER["socket"] == "[::]:80" &#123; &#125;server.errorlog-use-syslog = "enable"accesslog.use-syslog = "enable"server.modules = ( "mod_access","mod_accesslog", "mod_cgi", "mod_fastcgi")fastcgi.debug = 1fastcgi.server = ( "/fsmoam" =&gt; ( "fsmoam.fcgi.handler" =&gt; ( "socket" =&gt; "/tmp/fsmoam.fcgi.socket", "check-local" =&gt; "disable", "bin-path" =&gt; "/opt/qcom/bin/tests/fsmWebServer --default-log-level=DEBUG", "max-procs" =&gt; 1) ))# mimetype mappingmimetype.assign = ( ".pdf" =&gt; "application/pdf", ".sig" =&gt; "application/pgp-signature", ".spl" =&gt; "application/futuresplash", ".class" =&gt; "application/octet-stream", ".ps" =&gt; "application/postscript", ".torrent" =&gt; "application/x-bittorrent", ".dvi" =&gt; "application/x-dvi", ".gz" =&gt; "application/x-gzip", ".pac" =&gt; "application/x-ns-proxy-autoconfig", ".swf" =&gt; "application/x-shockwave-flash", ".tar.gz" =&gt; "application/x-tgz", ".tgz" =&gt; "application/x-tgz", ".tar" =&gt; "application/x-tar", ".zip" =&gt; "application/zip", ".mp3" =&gt; "audio/mpeg", ".m3u" =&gt; "audio/x-mpegurl", ".wma" =&gt; "audio/x-ms-wma", ".wax" =&gt; "audio/x-ms-wax", ".ogg" =&gt; "audio/x-wav", ".wav" =&gt; "audio/x-wav", ".gif" =&gt; "image/gif", ".jpg" =&gt; "image/jpeg", ".jpeg" =&gt; "image/jpeg", ".png" =&gt; "image/png", ".xbm" =&gt; "image/x-xbitmap", ".xpm" =&gt; "image/x-xpixmap", ".xwd" =&gt; "image/x-xwindowdump", ".css" =&gt; "text/css", ".html" =&gt; "text/html", ".htm" =&gt; "text/html", ".js" =&gt; "text/javascript", ".asc" =&gt; "text/plain", ".c" =&gt; "text/plain", ".conf" =&gt; "text/plain", ".text" =&gt; "text/plain", ".txt" =&gt; "text/plain", ".dtd" =&gt; "text/xml", ".xml" =&gt; "text/xml", ".mpeg" =&gt; "video/mpeg", ".mpg" =&gt; "video/mpeg", ".mov" =&gt; "video/quicktime", ".qt" =&gt; "video/quicktime", ".avi" =&gt; "video/x-msvideo", ".asf" =&gt; "video/x-ms-asf", ".asx" =&gt; "video/x-ms-asf", ".wmv" =&gt; "video/x-ms-wmv", ".bz2" =&gt; "application/x-bzip", ".tbz" =&gt; "application/x-bzip-compressed-tar", ".tar.bz2" =&gt; "application/x-bzip-compressed-tar")index-file.names = ( "index.html" )cgi.assign = ( ".sh" =&gt; "/bin/sh" )]]></content>
      <tags>
        <tag>lighttpd</tag>
        <tag>FastCGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[svn 常用操作命令]]></title>
    <url>%2F2017%2F03%2F29%2Fsvn-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[检出123456$ svn checkout http://路径(目录或文件的全路径) [本地目录全路径] --username 用户名$ svn checkout svn://路径(目录或文件的全路径) [本地目录全路径] --username 用户名# 也可以使用缩写# 例子：$ svn co svn://localhost/测试工具 /home/testtools --username wzhnsc$ svn co http://localhost/test/testapp --username wzhnsc 注 ：如果不带–password 参数传输密码的话，会提示输入密码，建议不要用明文的–password 选项。 不指定本地目录全路径，则检出到当前目录下。 导出（导出一个干净的不带.svn文件夹的目录树）1234567$ svn export [-r 版本号] http://路径(目录或文件的全路径) [本地目录全路径] --username 用户名$ svn export [-r 版本号] svn://路径(目录或文件的全路径) [本地目录全路径] --username 用户名$ svn export 本地检出的(即带有.svn文件夹的)目录全路径 要导出的本地目录全路径# 例子：$ svn export svn://localhost/测试工具 /home/testtools --username wzhnsc$ svn export svn://localhost/test/testapp --username wzhnsc$ svn export /home/testapp /home/testtools 注 ：第一种从版本库导出干净工作目录树的形式是指定URL， ​ 如果指定了修订版本号，会导出相应的版本， ​ 如果没有指定修订版本，则会导出最新的，导出到指定位置。 ​ 如果省略 本地目录全路径，URL的最后一部分会作为本地目录的名字。 ​ 第二种形式是指定 本地检出的目录全路径 到 要导出的本地目录全路径，所有的本地修改将会保留， ​ 但是不在版本控制下(即没提交的新文件，因为.svn文件夹里没有与之相关的信息记录)的文件不会拷贝。 添加新文件1234567$ svn add 文件名# 注：告诉SVN服务器要添加文件了，还要用svn commint -m真实的上传上去！# 例子：$ svn add test.php # 添加test.php $ svn commit -m "添加我的测试用test.php" test.php$ svn add *.php # 添加当前目录下所有的php文件$ svn commit -m "添加我的测试用全部php文件" *.php 提交12345678910$ svn commit -m "提交备注信息文本" [-N] [--no-unlock] 文件名$ svn ci -m "提交备注信息文本" [-N] [--no-unlock] 文件名# 必须带上-m参数，参数可以为空，但是必须写上-m# 例子：$ svn commit -m "提交当前目录下的全部在版本控制下的文件" * # 注意这个*表示全部文件$ svn commit -m "提交我的测试用test.php" test.php$ svn commit -m "提交我的测试用test.php" -N --no-unlock test.php # 保持锁就用–no-unlock开关$ svn ci -m "提交当前目录下的全部在版本控制下的文件" * # 注意这个*表示全部文件$ svn ci -m "提交我的测试用test.php" test.php$ svn ci -m "提交我的测试用test.php" -N --no-unlock test.php # 保持锁就用–no-unlock开关 更新文件123456789101112$ svn update$ svn update -r 修正版本 文件名$ svn update 文件名# 例子：# 后面没有目录，默认将当前目录以及子目录下的所有文件都更新到最新版本$ svn update # 将版本库中的文件 test.cpp 还原到修正版本（revision）200$ svn update -r 200 test.cpp# 更新与版本库同步。提交的时候提示过期冲突，需要先 update 修改文件，然后清除svn resolved，最后再提交commit。$ svn update test.php 删除文件123456789$ svn delete svn://路径(目录或文件的全路径) -m "删除备注信息文本"# 推荐如下操作：$ svn delete 文件名 $ svn ci -m "删除备注信息文本"# 例子：$ svn delete svn://localhost/testapp/test.php -m "删除测试文件test.php"# 推荐如下操作：$ svn delete test.php $ svn ci -m "删除测试文件test.php" 加锁 / 解锁12345$ svn lock -m "加锁备注信息文本" [--force] 文件名 $ svn unlock 文件名# 例子：$ svn lock -m "锁信测试用test.php文件" test.php $ svn unlock test.php 比较差异12345678$ svn diff 文件名 $ svn diff -r 修正版本号m:修正版本号n 文件名# 例子：# 将修改的文件与基础版本比较$ svn diff test.php # 对修正版本号200 和 修正版本号201 比较差异$ svn diff -r 200:201 test.php 查看文件或者目录状态123456789$ svn st 目录路径/名# 目录下的文件和子目录的状态，正常状态不显示.# 【?：不在svn的控制中； M：内容被修改；C：发生冲突；A：预定加入到版本库；K：被锁定】 $ svn status 目录路径/名 $ svn -v 目录路径/名# 显示文件和子目录状态# 【第一列保持相同，第二列显示工作版本号，第三和第四列显示最后一次修改的版本号和修改人】$ svn status -v 目录路径/名 注 ：svn status、svn diff和 svn revert这三条命令在没有网络的情况下也可以执行的，原因是svn在本地的.svn中保留了本地版本的原始拷贝。 查看日志1234$ svn log 文件名# 例子：# 显示这个文件的所有修改记录，及其版本号的变化$ svn log test.php 查看文件详细信息123$ svn info 文件名# 例子：$ svn info test.php SVN 帮助1234# 全部功能选项$ svn help# 具体功能的说明$ svn help ci 查看版本库下的文件和目录列表1234567$ svn list svn://路径(目录或文件的全路径)$ svn ls svn://路径(目录或文件的全路径)# 例子：$ svn list svn://localhost/test# 显示svn://localhost/test目录下的所有属于版本库的文件和目录$ svn ls svn://localhost/test 创建纳入版本控制下的新目录12345$ svn mkdir 目录名$ svn mkdir -m "新增目录备注文本" http://目录全路径# 例子：$ svn mkdir newdir$ svn mkdir -m "Making a new dir." svn://localhost/test/newdir 注 ： 添加完子目录后，一定要回到根目录更新一下，不然在该目录下提交文件会提示“提交失败” 1$ svn update 注 ：如果手工在checkout出来的目录里创建了一个新文件夹newsubdir， ​ 再用svn mkdir newsubdir命令后，SVN会提示： ​ svn: 尝试用 “svn add”或 “svn add –non-recursive”代替？ ​ svn: 无法创建目录“hello”: 文件已经存在 此时，用如下命令解决： svn add --non-recursive newsubdir​ 在进入这个newsubdir文件夹，用ls -a查看它下面的全部目录与文件，会发现多了：.svn目录 ​ 再用 svn mkdir -m “添hello功能模块文件” svn://localhost/test/newdir/newsubdir 命令， ​ SVN提示： ​ svn: File already exists: filesystem ‘/data/svnroot/test/db’, transaction ‘4541-1’, ​ path ‘/newdir/newsubdir ‘ 恢复本地修改12345678$ svn revert [--recursive] 文件名# 注意: 本子命令不会存取网络，并且会解除冲突的状况。但是它不会恢复被删除的目录。# 例子：# 丢弃对一个文件的修改$ svn revert foo.c# 恢复一整个目录的文件，. 为当前目录$ svn revert --recursive . 把工作拷贝更新到别的URL1234$ svn switch http://目录全路径 本地目录全路径# 例子：# (原为123的分支)当前所在目录分支到localhost/test/456$ svn switch http://localhost/test/456 . 解决冲突12345678910111213$ svn resolved [本地目录全路径]# 例子：$ svn updateC foo.cUpdated to revision 31.# 如果你在更新时得到冲突，你的工作拷贝会产生三个新的文件：$ lsfoo.cfoo.c.minefoo.c.r30foo.c.r31# 当你解决了foo.c的冲突，并且准备提交，运行svn resolved让你的工作拷贝知道你已经完成了所有事情。# 你可以仅仅删除冲突的文件并且提交，但是svn resolved除了删除冲突文件，还修正了一些记录在工作拷贝管理区域的记录数据，所以我们推荐你使用这个命令。 不checkout而查看输出特定文件或URL的内容1234567891011$ svn cat http://文件全路径# 例子：$ svn cat http://localhost/test/readme.txt# 新建一个分支copy# 从branchA拷贝出一个新分支branchB$ svn copy branchA branchB -m "make B branch" # 合并内容到分支merge# 把对branchA的修改合并到分支branchB$ svn merge branchA branchB]]></content>
      <tags>
        <tag>svn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 使用指南]]></title>
    <url>%2F2017%2F03%2F28%2Fhexo-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[安装、初始化和配置准备工作 git node.js github 安装和初始化首先确定已经安装好了 nodejs 和 npm 以及 git 12345$ npm install hexo -g$ hexo init blog$ cd blog$ npm install$ hexo server 访问http://localhost:4000，会看到生成好的博客。 主目录结构12345678|-- _config.yml|-- package.json|-- scaffolds|-- source |-- _posts|-- themes|-- .gitignore|-- package.json _config.yml 全局配置文件，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。这个文件下面会做较为详细的介绍。 package.json hexo框架的参数和所依赖插件，如下： 12345678910111213141516171819&#123; "name": "hexo-site", "version": "0.0.0", "private": true, "hexo": &#123; "version": "3.2.0" &#125;, "dependencies": &#123; "hexo": "^3.2.0", "hexo-generator-archive": "^0.1.4", "hexo-generator-category": "^0.1.3", "hexo-generator-index": "^0.2.0", "hexo-generator-tag": "^0.2.0", "hexo-renderer-ejs": "^0.2.0", "hexo-renderer-stylus": "^0.3.1", "hexo-renderer-marked": "^0.2.10", "hexo-server": "^0.2.0" &#125;&#125; scaffold scaffolds是“脚手架、骨架”的意思，当你新建一篇文章（hexo new ‘title’）的时候，hexo是根据这个目录下的文件进行构建的。基本不用关心。 _config.yml文件 _config.yml 采用YAML语法格式，具体语法自行学习 。具体配置可以参考官方文档，_config.yml 文件中的内容，并对主要参数做简单的介绍 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Hexo #网站标题subtitle: #网站副标题description: #网站描述author: John Doe #作者language: #语言timezone: #网站时区。Hexo 默认使用您电脑的时区。时区列表。比如说：America/New_York, Japan, 和 UTC 。# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.com #你的站点Urlroot: / #站点的根目录permalink: :year/:month/:day/:title/ #文章的 永久链接 格式 permalink_defaults: #永久链接中各部分的默认值# Directory source_dir: source #资源文件夹，这个文件夹用来存放内容public_dir: public #公共文件夹，这个文件夹用于存放生成的站点文件。tag_dir: tags #标签文件夹 archive_dir: archives #归档文件夹category_dir: categories #分类文件夹code_dir: downloads/code #Include code 文件夹i18n_dir: :lang #国际化（i18n）文件夹skip_render: #跳过指定文件的渲染，您可使用 glob 表达式来匹配路径。 # Writingnew_post_name: :title.md #新文章的文件名称default_layout: post #预设布局titlecase: false #把标题转换为 title caseexternal_link: true #在新标签中打开链接filename_case: 0 #把文件名称转换为 (1) 小写或 (2) 大写render_drafts: false #是否显示草稿post_asset_folder: false #是否启动 Asset 文件夹relative_link: false #把链接改为与根目录的相对位址 future: true #显示未来的文章highlight: #内容中代码块的设置 enable: true line_number: true auto_detect: false tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map: #分类别名tag_map: #标签别名# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #日期格式time_format: HH:mm:ss #时间格式 # Pagination## Set per_page to 0 to disable paginationper_page: 10 #分页数量pagination_dir: page # Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape #主题名称# Deployment## Docs: https://hexo.io/docs/deployment.html# 部署部分的设置deploy: type: git #类型，常用的git repo: https://github.com/nanshanyi/nanshanyi.github.io.git #github仓库的地址 注意如果页面中出现中文，应以UTF-8无BOM编码格式，所以不要用win自带的记事本，而是用notepad++这种支持编码转换的编辑器。 由于google在天朝大陆被墙，进入 themes\landscape\layout\_partial ，打开 head.ejs ，删掉第31行 fonts.googleapis.com 的链接。 下载下来 jQuery-2.0.3.min.js ，放到 themes\landscape\source\js 文件夹中。之后进入 themes\landscape\layout\_partial ，打开 after-footer.ejs ，将第17行的路径替换为 /js/jquery-2.0.3.min.js 。 至此大功告成。 写文章&amp;草稿文章命令行输入： 1$ hexo new post "new article" 之后在 soource/_posts 目录下面多了一个 new-article.md 的文件。 文章属性 Setting Description Default layout Layout post或page title 文章的标题 date 穿件日期 文件的创建日期 updated 修改日期 文件的修改日期 comments 是否开启评论 true tags 标签 categories 分类 permalink url中的名字 文件名 toc 是否开启目录 true reward 是否开启打赏 true 分类和标签12345categories: - 日记tags: - Hexo - node.js 摘要&lt;!--more--&gt; 之上的内容为摘要。 草稿草稿相当于很多博客都有的“私密文章”功能。 1$ hexo new draft "new draft" 会在 source/_drafts 目录下生成一个 new-draft.md 文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到 _drafts 目录之中。 如果你希望强行预览草稿，更改配置文件： 1render_drafts: true 或者，如下方式启动server： 1$ hexo server --drafts 下面这条命令可以把草稿变成文章，或者页面： 1234$ hexo publish [layout] &lt;filename&gt;eg:$ hexo publish drafts hexo-使用指南 Blog中出入图片和音乐文章推介：Hexo 博客中插入音乐/视频 ​ 使用七牛为Hexo存储图片 [hexo主题中添加相册功能](http://www.cnblogs.com/xljzlw/p/5137622.html)​ 为 Hexo 主题添加多种图片样式(主题不错考虑移植) ​ Hexo折腾记——基本配置篇 ​ hexo博客进阶－相册和独立域名 插入图片基本分为两种办法** ： （1） 放在本地文件 首先在根目录下确认 _config.yml 中有 post_asset_folder:true 。在 hexo 目录，执行： 1$ npm install https://github.com/CodeFalling/hexo-asset-image --save 之后再使用 hexo new &#39;new&#39;创建新博客的时候，会在 source/_posts 里面创建 .md 文件的同时生成一个相同的名字的文件夹。把该文章中需要使用的图片放在该文件夹下即可。使用的时候 123![“图片描述”（可以不写）](/文件夹名/你的图片名字.JPG)例如：！[ ] (new/text.jpg) （2）放在七牛上，需要先注册，上传图片生成链接，直接在文章中使用链接即可。 插入音乐 ： 可以使用网易云音乐，搜索想要的歌曲，点击歌曲名字进入播放器页面，点击生成外链播放器；复制代码，直接粘贴到博文中即可。这样会显示一个网易的播放器，可以把 12&lt;iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=298 height=52 src="http://music.163.com/outchain/player?type=2&amp;id=32192436&amp;auto=1&amp;height=32"&gt;&lt;/iframe&gt;//其中的width=298 height=52 均改为0就看不到了，依然可以播放音乐 代码高亮highlight.js支持highlightjs官网 highlightjs主题风格 其他Hexo，Yilia主题添加站内搜索功能 为Hexo博客添加目录 Hexo站点中添加文章目录以及归档 使用LeanCloud平台为Hexo博客添加文章浏览量统计组件 使用hexo搭建静态博客 Hexo Docs中文 ： （二）基本用法]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs+webpack+vuejs 搭建开发环境学习套路]]></title>
    <url>%2F2017%2F03%2F27%2Fnodejs-webpack-vuejs-%E6%90%AD%E5%BB%BA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E5%AD%A6%E4%B9%A0%E5%A5%97%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[官方文档官方手册 中文官网 vuejs 2.0 中文文档 ECMAScript 6 入门 node.js相关的中文文档及教程 Node.js中文网API Webpack 中文指南 webpack2.2中文文档 以上是提供的一些官方资料，下面开始我们的套路吧： 环境构建1.新建一个目录vuepro2.初始化 1234$ cd vuepro# 初始化的时候可以一路回车，在最后输入"yes"后会生成package.json文件$ npm init 3.安装模块，先装这么多，有需要再安装 1$ npm install vue webpack babel-loader babel-core babel-preset-env babel-cli babel-preset-es2015 html-webpack-plugin --save-dev 4.创建良好的目录层级 12$ mkdir src$ cd src &amp;&amp; mkdir -p html jssrc webapp html放置模板文件，jssrc放置js文件，最终编译好的文件放置在webapp目录里，这个目录也就是我们网站的目录。 5.在项目根目录下创建webpack配置文件：webpack.config.js 1234567891011121314151617181920212223242526272829303132333435363738var HtmlWebpackPlugin = require('html-webpack-plugin');var webpack=require("webpack");module.exports =&#123; entry: &#123; //入口文件 "index":__dirname+'/src/jssrc/index.js', &#125;, output: &#123; path: __dirname+'/src/webapp/js', //输出文件夹 filename:'[name].js' //最终打包生成的文件名(只是文件名，不带路径的哦) &#125;, /*resolve: &#123; alias: &#123; vue: 'vue/dist/vue.js' &#125; &#125;,*/ externals: &#123; &#125;, module:&#123; loaders:[ &#123;test:/\.js$/,loader:"babel-loader",query:&#123;compact:true&#125;&#125;, //这里肯定要加入n个loader 譬如vue-loader、babel-loader、css-loader等等 ] &#125;, plugins:[ new HtmlWebpackPlugin(&#123; filename: __dirname+'/src/webapp/index.html', //目标文件 template: __dirname+'/src/html/index.html', //模板文件 inject:'body', hash:true, //代表js文件后面会跟一个随机字符串,解决缓存问题 chunks:["index"] &#125;) ]&#125; 6.同样在根目录下创建babel配置文件：.babelrc 123&#123; &quot;presets&quot; : [&quot;es2015&quot;]&#125; 然后就可以在webpack里面配置loader，我们上面webpack配置中已经写了： 1234loaders:[ &#123;test:/\.js$/,loader:"babel-loader",query:&#123;compact:true&#125;&#125;, // 经过测试旧版用的是loader:"babel",在新版中用的是loader:"babel-loader" ] 这句话意思就是：凡是 .js 文件都使用 babel-loader , 并且压缩。 学习vue最简单的一个套路思考：数据如何渲染？ 套路如下： 首先要有个数据块标记 vue里面可以像模板引擎一样写上 {\{name\}} 其中 name 就是变量名 接下来进行实战练习 index.htm l如下： 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="me"&gt; 我的年龄是&#123;age&#125; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; index.js 如下： 123456import Vue from "vue"; //会去node_modules\vue\package.jsonnew Vue(&#123; el:"#me", data:&#123;age:18&#125;&#125;) 至此，我们需要用 webpack 打包，打包到 webapp 目录下。 需要修改2个地方： (1)因为我们的 webpack 不是全局安装的，所以不能直接执行 webpack 命令，我们这里借助 npm 来执行。所以需要修改项目根目录下的 package.json 文件，加入： 1234"scripts": &#123; "test": "echo \"Error: no test specified\" &amp;&amp; exit 1", "build": "webpack"&#125;, 表示：执行build，就会去node_modules.bin\下去寻找webpack命令。build 这个名字是自定义的。 (2)还需要修改 webpack 配置文件：webpack.config.js 12345resolve: &#123; alias: &#123; vue: 'vue/dist/vue.js' &#125; &#125;, 我们之前把这个注释掉了，现在打开。此处的意义是找到 node_modules/vue/dist/vue.js 最后，我们就来打包，看看结果是怎样的？ 终端里还是cd到项目根目录下，执行： 1$ npm run build index.html 就是打包之后的模板文件，js/index.js 就是打包之后的js文件，在 index.html 被引用了。 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;首页&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;div id="me"&gt; 我的年龄是&#123;age&#125; &lt;/div&gt;&lt;script type="text/javascript" src="js/index.js?43c73980e35f1569ef72"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 预览一下index.html: 这样就完成了 vueJS 的一个简单案列]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 支持的语言]]></title>
    <url>%2F2017%2F03%2F23%2FMarkdown-%E6%94%AF%E6%8C%81%E7%9A%84%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[Markdown支持的语言 名称 关键字 调用的js AppleScript applescript shBrushAppleScript.js ActionScript 3.0 actionscript3 , as3 shBrushAS3.js Shell bash , shell shBrushBash.js ColdFusion coldfusion , cf shBrushColdFusion.js C cpp , c shBrushCpp.js C# c# , c-sharp , csharp shBrushCSharp.js CSS css shBrushCss.js Delphi delphi , pascal , pas shBrushDelphi.js diff&amp;patch diff patch shBrushDiff.js Erlang erl , erlang shBrushErlang.js Groovy groovy shBrushGroovy.js Java java shBrushJava.js JavaFX jfx , javafx shBrushJavaFX.js JavaScript js , jscript , javascript shBrushJScript.js Perl perl , pl , Perl shBrushPerl.js PHP php shBrushPhp.js text text , plain shBrushPlain.js Python py , python shBrushPython.js Ruby ruby , rails , ror , rb shBrushRuby.js SASS&amp;SCSS sass , scss shBrushSass.js Scala scala shBrushScala.js SQL sql shBrushSql.js Visual Basic vb , vbnet shBrushVb.js XML xml , xhtml , xslt , html shBrushXml.js Objective C objc , obj-c shBrushObjectiveC.js F# f# f-sharp , fsharp shBrushFSharp.js R r , s , splus shBrushR.js matlab matlab shBrushMatlab.js swift swift shBrushSwift.js GO go , golang shBrushGo.js]]></content>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[很棒的GitBook]]></title>
    <url>%2F2017%2F03%2F21%2F%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90Go-%E5%BE%88%E6%A3%92%E7%9A%84GitBook%2F</url>
    <content type="text"><![CDATA[很不错的golang剖析，建议有golang基础的再看，讲解的非常详细。深入解析Go]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Rss 加载失败解决]]></title>
    <url>%2F2017%2F03%2F21%2FGoogle-Rss-%E5%8A%A0%E8%BD%BD%E5%A4%B1%E8%B4%A5%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[博客在google浏览器中使用RSS会出现RSS源码，原因是google浏览器没有安装RSS插件。解决办法，到google网上应用店安装 RSS Subscription Extension 即可解决问题。]]></content>
      <tags>
        <tag>rss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + Github 博客多终端同步]]></title>
    <url>%2F2017%2F03%2F20%2FHexo-Github-%E5%8D%9A%E5%AE%A2%E5%A4%9A%E7%BB%88%E7%AB%AF%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[原文链接 主体的思路是将博文内容相关文件放在Github项目中master中，将Hexo配置写博客用的相关文件放在Github项目的hexo分支上，这个是关键，多终端的同步只需要对分支hexo进行操作。下面是详细的步骤讲解： 1. 准备条件安装了Node.js,Git,Hexo环境完成Github与本地Hexo的对接这部分大家可以参考史上最详细的Hexo博客搭建图文教程 配置好这些，就可以捋起袖子大干一场了！ 2. 在其中一个中单操作，push本地文件夹Hexo中的必要文件到yourname.github.io的hexo分支上在利用Github+Hexo搭建自己的博客时，新建了一个Hexo的文件夹，并进行相关的配置，这部分主要是将这些配置的文件托管到Github项目的分支上，其中只托管部分用于多终端的同步的文件，如完成的效果图所示： 123456789101112131415161718# 初始化本地仓库$ git init# 将必要的文件依次添加，有些文件夹如npm install产生的node_modules由于路径过长不好处理，所以这里没有用`git add .`命令了，而是依次添加必要文件$ git add source$ git commit -m "Blog Source Hexo"# 新建hexo分支$ git branch hexo# 切换到hexo分支上$ git checkout hexo# 将本地与Github项目对接$ git remote add origin https://github.com/yourname/yourname.github.io.git# push到Github项目的hexo分支上$ git push origin hexo 这样你的github项目中就会多出一个Hexo分支，这个就是用于多终端同步关键的部分。 3. 另一终端完成clone和push更新此时在另一终端更新博客，只需要将Github的hexo分支clone下来，进行初次的相关配置 123456789101112131415161718192021222324# 将Github中hexo分支clone到本地$ git clone -b hexo https://github.com/yourname/yourname.github.io.git# 切换到刚刚clone的文件夹内$ cd yourname.github.io# cheackout 远程代码到本地hexo分支$ git checkout -b hexo origin/hexo# 注意，这里一定要切换到刚刚clone的文件夹内执行，安装必要的所需组件，不用再init$ npm install# 新建一个.md文件，并编辑完成自己的博客内容$ hexo new post "new blog name"# 经测试每次只要更新sorcerer中的文件到Github中即可，因为只是新建了一篇新博客$ git add source$ git commit -m "XX"# 更新分支$ git push origin hexo# push更新完分支之后将自己写的博客对接到自己搭的博客网站上，同时同步了Github中的master$ hexo d -g 4. 不同终端间愉快地玩耍在不同的终端已经做完配置，就可以愉快的分享自己更新的博客进入自己相应的文件夹 123456789101112# 先pull完成本地与远端的融合$ git pull origin hexo$ hexo new post " new blog name"$ git add source$ git commit -m "XX"$ git push origin hexo$ hexo d -g]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git远程操作详解]]></title>
    <url>%2F2017%2F01%2F27%2Fgit-remote-operation%2F</url>
    <content type="text"><![CDATA[Git是目前最流行的版本管理系统，学会Git几乎成了开发者的必备技能。 Git有很多优势，其中之一就是远程操作非常简便。本文详细介绍5个Git命令，它们的概念和用法，理解了这些内容，你就会完全掌握Git远程操作。 git clone git remote git fetch git pull git push 本文针对初级用户，从最简单的讲起，但是需要读者对Git的基本用法有所了解。同时，本文覆盖了上面5个命令的几乎所有的常用用法，所以对于熟练用户也有参考价值。 一、git clone远程操作的第一步，通常是从远程主机克隆一个版本库，这时就要用到 git clone 命令。 $ git clone &lt;版本库的网址&gt;比如，克隆jQuery的版本库。 $ git clone https://github.com/jquery/jquery.git该命令会在本地主机生成一个目录，与远程主机的版本库同名。如果要指定不同的目录名，可以将目录名作为 git clone 命令的第二个参数。 $ git clone &lt;版本库的网址&gt; &lt;本地目录名&gt;git clone 支持多种协议，除了HTTP(s)以外，还支持SSH、Git、本地文件协议等，下面是一些例子。 $ git clone http[s]://example.com/path/to/repo.git/ $ git clone ssh://example.com/path/to/repo.git/ $ git clone git://example.com/path/to/repo.git/ $ git clone /opt/git/project.git $ git clone file:///opt/git/project.git $ git clone ftp[s]://example.com/path/to/repo.git/ $ git clone rsync://example.com/path/to/repo.git/SSH协议还有另一种写法。 $ git clone [user@]example.com:path/to/repo.git/通常来说，Git协议下载速度最快，SSH协议用于需要用户认证的场合。各种协议优劣的详细讨论请参考官方文档。 二、git remote为了便于管理，Git要求每个远程主机都必须指定一个主机名。git remote 命令就用于管理主机名。 不带选项的时候，git remote 命令列出所有远程主机。 $ git remote origin使用 -v 选项，可以参看远程主机的网址。 $ git remote -v origin git@github.com:jquery/jquery.git (fetch) origin git@github.com:jquery/jquery.git (push)上面命令表示，当前只有一台远程主机，叫做 origin，以及它的网址。 克隆版本库的时候，所使用的远程主机自动被Git命名为 origin。如果想用其他的主机名，需要用 git clone 命令的 -o 选项指定。 $ git clone -o jQuery https://github.com/jquery/jquery.git $ git remote jQuery上面命令表示，克隆的时候，指定远程主机叫做jQuery。 git remote show 命令加上主机名，可以查看该主机的详细信息。 $ git remote show &lt;主机名&gt;git remote add 命令用于添加远程主机。 $ git remote add &lt;主机名&gt; &lt;网址&gt;git remote rm 命令用于删除远程主机。 $ git remote rm &lt;主机名&gt;git remote rename 命令用于远程主机的改名。 $ git remote rename &lt;原主机名&gt; &lt;新主机名&gt;三、git fetch一旦远程主机的版本库有了更新（Git术语叫做commit），需要将这些更新取回本地，这时就要用到 git fetch 命令。 $ git fetch &lt;远程主机名&gt;上面命令将某个远程主机的更新，全部取回本地。 git fetch 命令通常用来查看其他人的进程，因为它取回的代码对你本地的开发代码没有影响。 默认情况下，git fetch 取回所有分支（branch）的更新。如果只想取回特定分支的更新，可以指定分支名。 $ git fetch &lt;远程主机名&gt; &lt;分支名&gt;比如，取回 origin 主机的 master 分支。 $ git fetch origin master所取回的更新，在本地主机上要用”远程主机名/分支名”的形式读取。比如 origin 主机的 master，就要用 origin/master 读取。 git branch 命令的 -r 选项，可以用来查看远程分支，-a 选项查看所有分支。 $ git branch -r origin/master $ git branch -a * master remotes/origin/master上面命令表示，本地主机的当前分支是 master，远程分支是 origin/master。 取回远程主机的更新以后，可以在它的基础上，使用 git checkout 命令创建一个新的分支。 $ git checkout -b newBrach origin/master上面命令表示，在 origin/master 的基础上，创建一个新分支。 此外，也可以使用 git merge 命令或者 git rebase 命令，在本地分支上合并远程分支。 $ git merge origin/master # 或者 $ git rebase origin/master上面命令表示在当前分支上，合并 origin/master。 四、git pullgit pull 命令的作用是，取回远程主机某个分支的更新，再与本地的指定分支合并。它的完整格式稍稍有点复杂。 $ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;比如，取回 origin 主机的 next 分支，与本地的 master 分支合并，需要写成下面这样。 $ git pull origin next:master如果远程分支是与当前分支合并，则冒号后面的部分可以省略。 $ git pull origin next上面命令表示，取回 origin/next 分支，再与当前分支合并。实质上，这等同于先做 git fetch ，再做 git merge。 $ git fetch origin $ git merge origin/next在某些场合，Git会自动在本地分支与远程分支之间，建立一种追踪关系（tracking）。比如，在 git clone 的时候，所有本地分支默认与远程主机的同名分支，建立追踪关系，也就是说，本地的 master 分支自动”追踪” origin/master 分支。 Git也允许手动建立追踪关系。 git branch --set-upstream master origin/next上面命令指定 master 分支追踪 origin/next 分支。 如果当前分支与远程分支存在追踪关系，git pull 就可以省略远程分支名。 $ git pull origin上面命令表示，本地的当前分支自动与对应的 origin 主机”追踪分支”（remote-tracking branch）进行合并。 如果当前分支只有一个追踪分支，连远程主机名都可以省略。 $ git pull上面命令表示，当前分支自动与唯一一个追踪分支进行合并。 如果合并需要采用 rebase 模式，可以使用 --rebase 选项。 $ git pull --rebase &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;如果远程主机删除了某个分支，默认情况下，git pull 不会在拉取远程分支的时候，删除对应的本地分支。这是为了防止，由于其他人操作了远程主机，导致 git pull 不知不觉删除了本地分支。 但是，你可以改变这个行为，加上参数 -p 就会在本地删除远程已经删除的分支。 $ git pull -p # 等同于下面的命令 $ git fetch --prune origin $ git fetch -p五、git pushgit push 命令用于将本地分支的更新，推送到远程主机。它的格式与 git pull 命令相仿。 $ git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;注意，分支推送顺序的写法是&lt;来源地&gt;:&lt;目的地&gt;，所以 git pull 是&lt;远程分支&gt;:&lt;本地分支&gt;，而 git push 是&lt;本地分支&gt;:&lt;远程分支&gt;。 如果省略远程分支名，则表示将本地分支推送与之存在”追踪关系”的远程分支（通常两者同名），如果该远程分支不存在，则会被新建。 $ git push origin master上面命令表示，将本地的 master 分支推送到 origin 主机的 master 分支。如果后者不存在，则会被新建。 如果省略本地分支名，则表示删除指定的远程分支，因为这等同于推送一个空的本地分支到远程分支。 $ git push origin :master # 等同于 $ git push origin --delete master上面命令表示删除 origin 主机的 master 分支。 如果当前分支与远程分支之间存在追踪关系，则本地分支和远程分支都可以省略。 $ git push origin上面命令表示，将当前分支推送到 origin 主机的对应分支。 如果当前分支只有一个追踪分支，那么主机名都可以省略。 $ git push如果当前分支与多个主机存在追踪关系，则可以使用 -u 选项指定一个默认主机，这样后面就可以不加任何参数使用 git push。 $ git push -u origin master上面命令将本地的 master 分支推送到 origin 主机，同时指定 origin 为默认主机，后面就可以不加任何参数使用 git push 了。 不带任何参数的 git push，默认只推送当前分支，这叫做simple方式。此外，还有一种matching方式，会推送所有有对应的远程分支的本地分支。Git 2.0版本之前，默认采用matching方法，现在改为默认采用simple方式。如果要修改这个设置，可以采用 git config 命令。 $ git config --global push.default matching # 或者 $ git config --global push.default simple还有一种情况，就是不管是否存在对应的远程分支，将本地的所有分支都推送到远程主机，这时需要使用 --all 选项。 $ git push --all origin上面命令表示，将所有本地分支都推送到 origin 主机。 如果远程主机的版本比本地版本更新，推送时Git会报错，要求先在本地做 git pull 合并差异，然后再推送到远程主机。这时，如果你一定要推送，可以使用 --force 选项。 $ git push --force origin 上面命令使用 --force 选项，结果导致远程主机上更新的版本被覆盖。除非你很确定要这样做，否则应该尽量避免使用 --force 选项。 最后，git push 不会推送标签（tag），除非使用 --tags 选项。 $ git push origin --tags]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git工作流程]]></title>
    <url>%2F2017%2F01%2F27%2Fgit-workflow%2F</url>
    <content type="text"><![CDATA[Git 作为一个源码管理系统，不可避免涉及到多人协作。 协作必须有一个规范的工作流程，让大家有效地合作，使得项目井井有条地发展下去。”工作流程”在英语里，叫做”workflow”或者”flow”，原意是水流，比喻项目像水流那样，顺畅、自然地向前流动，不会发生冲击、对撞、甚至漩涡。 本文介绍三种广泛使用的工作流程： Git flow Github flow Gitlab flow 一、功能驱动本文的三种工作流程，有一个共同点：都采用”功能驱动式开发”（Feature-driven development，简称FDD）。 它指的是，需求是开发的起点，先有需求再有功能分支（feature branch）或者补丁分支（hotfix branch）。完成开发后，该分支就合并到主分支，然后被删除。 二、Git Flow最早诞生、并得到广泛采用的一种工作流程，就是Git flow 。 2.1 特点它最主要的特点有两个。 首先，项目存在两个长期分支。 主分支 master 开发分支 develop 前者用于存放对外发布的版本，任何时候在这个分支拿到的，都是稳定的分布版；后者用于日常开发，存放最新的开发版。 其次，项目存在三种短期分支。 功能分支（feature branch） 补丁分支（hotfix branch） 预发分支（release branch） 一旦完成开发，它们就会被合并进 develop 或 master ，然后被删除。 2.2 评价Git flow的优点是清晰可控，缺点是相对复杂，需要同时维护两个长期分支。大多数工具都将 master 当作默认分支，可是开发是在 develop 分支进行的，这导致经常要切换分支，非常烦人。 更大问题在于，这个模式是基于”版本发布”的，目标是一段时间以后产出一个新版本。但是，很多网站项目是”持续发布”，代码一有变动，就部署一次。这时， master 分支和 develop 分支的差别不大，没必要维护两个长期分支。 三、Github flowGithub flow 是Git flow的简化版，专门配合”持续发布”。它是 Github.com 使用的工作流程。 3.1 流程它只有一个长期分支，就是 master ，因此用起来非常简单。 官方推荐的流程如下。 第一步：根据需求，从 master 拉出新分支，不区分功能分支或补丁分支。 第二步：新分支开发完成后，或者需要讨论的时候，就向 master 发起一个 pull request（简称PR）。 第三步：Pull Request 既是一个通知，让别人注意到你的请求，又是一种对话机制，大家一起评审和讨论你的代码。对话过程中，你还可以不断提交代码。 第四步：你的 Pull Request 被接受，合并进 master，重新部署后，原来你拉出来的那个分支就被删除。（先部署再合并也可。）3.2 评价Github flow 的最大优点就是简单，对于”持续发布”的产品，可以说是最合适的流程。 问题在于它的假设：master 分支的更新与产品的发布是一致的。也就是说，master 分支的最新代码，默认就是当前的线上代码。 可是，有些时候并非如此，代码合并进入 master 分支，并不代表它就能立刻发布。比如，苹果商店的APP提交审核以后，等一段时间才能上架。这时，如果还有新的代码提交，master 分支就会与刚发布的版本不一致。另一个例子是，有些公司有发布窗口，只有指定时间才能发布，这也会导致线上版本落后于 master 分支。 上面这种情况，只有 master 一个主分支就不够用了。通常，你不得不在master分支以外，另外新建一个 production 分支跟踪线上版本。 四、Gitlab flowGitlab flow 是 Git flow 与 Github flow 的综合。它吸取了两者的优点，既有适应不同开发环境的弹性，又有单一主分支的简单和便利。它是 Gitlab.com 推荐的做法。 4.1 上游优先Gitlab flow 的最大原则叫做”上游优先”（upsteam first），即只存在一个主分支 master，它是所有其他分支的”上游”。只有上游分支采纳的代码变化，才能应用到其他分支。 Chromium项目就是一个例子，它明确规定，上游分支依次为： Linus Torvalds的分支 子系统（比如netdev）的分支 设备厂商（比如三星）的分支 4.2 持续发布Gitlab flow 分成两种情况，适应不同的开发流程。 对于”持续发布”的项目，它建议在 master 分支以外，再建立不同的环境分支。比如，”开发环境”的分支是 master，”预发环境”的分支是 pre-production，”生产环境”的分支是 production。 开发分支是预发分支的”上游”，预发分支又是生产分支的”上游”。代码的变化，必须由”上游”向”下游”发展。比如，生产环境出现了bug，这时就要新建一个功能分支，先把它合并到 master，确认没有问题，再 cherry-pick 到 pre-production ，这一步也没有问题，才进入 production。 只有紧急情况，才允许跳过上游，直接合并到下游分支。 4.3 版本发布 对于”版本发布”的项目，建议的做法是每一个稳定版本，都要从 master 分支拉出一个分支，比如 2-3-stable、2-4-stable 等等。 以后，只有修补bug，才允许将代码合并到这些分支，并且此时要更新小版本号。 五、一些小技巧5.1 Pull Request 功能分支合并进 master 分支，必须通过 Pull Request（Gitlab里面叫做 Merge Request）。 前面说过，Pull Request本质是一种对话机制，你可以在提交的时候，@相关人员或团队，引起他们的注意。 5.2 Protected branchmaster 分支应该受到保护，不是每个人都可以修改这个分支，以及拥有审批 Pull Request 的权力。Github 和 Gitlab 都提供”保护分支”（Protected branch）这个功能。 5.3 IssueIssue 用于 Bug追踪和需求管理。建议先新建 Issue，再新建对应的功能分支。功能分支总是为了解决一个或多个 Issue。 功能分支的名称，可以与issue的名字保持一致，并且以issue的编号起首，比如”15-require-a-password-to-change-it”。 开发完成后，在提交说明里面，可以写上 &quot;fixes #14&quot; 或者 &quot;closes #67&quot;。Github规定，只要 commit message 里面有下面这些动词 + 编号，就会关闭对应的issue。 close closes closed fix fixes fixed resolve resolves resolved 这种方式还可以一次关闭多个issue，或者关闭其他代码库的issue，格式是 username/repository#issue_number。 Pull Request被接受以后，issue关闭，原始分支就应该删除。如果以后该issue重新打开，新分支可以复用原来的名字。 5.4 Merge节点Git有两种合并：一种是”直进式合并”（fast forward），不生成单独的合并节点；另一种是”非直进式合并”（none fast-forword），会生成单独节点。 前者不利于保持 commit 信息的清晰，也不利于以后的回滚，建议总是采用后者（即使用 --no-ff 参数）。只要发生合并，就要有一个单独的合并节点。 5.5 Squash 多个 commit为了便于他人阅读你的提交，也便于 cherry-pick 或撤销代码变化，在发起 Pull Request 之前，应该把多个 commit 合并成一个。（前提是，该分支只有你一个人开发，且没有跟 master 合并过。） 这可以采用rebase命令附带的squash操作，具体方法请参考《Git 使用规范流程》。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用规范流程]]></title>
    <url>%2F2017%2F01%2F27%2Fgit-using-standard-process%2F</url>
    <content type="text"><![CDATA[团队开发中，遵循一个合理、清晰的Git使用流程，是非常重要的。 否则，每个人都提交一堆杂乱无章的commit，项目很快就会变得难以协调和维护。 下面是ThoughtBot 的Git使用规范流程。我从中学到了很多，推荐你也这样使用Git。 第一步：新建分支首先，每次开发新功能，都应该新建一个单独的分支（这方面可以参考《Git分支管理策略》）。 # 获取主干最新代码 $ git checkout master $ git pull # 新建一个开发分支myfeature $ git checkout -b myfeature第二步：提交分支commit分支修改后，就可以提交commit了。 $ git add --all $ git status $ git commit --verbosegit add 命令的all参数，表示保存所有变化（包括新建、修改和删除）。从Git 2.0开始，all是 git add 的默认参数，所以也可以用 git add . 代替。 git status 命令，用来查看发生变动的文件。 git commit 命令的 verbose 参数，会列出 diff 的结果。 第三步：撰写提交信息提交commit时，必须给出完整扼要的提交信息，下面是一个范本。 Present-tense summary under 50 characters * More information about commit (under 72 characters). * More information about commit (under 72 characters). http://project.management-system.com/ticket/123第一行是不超过50个字的提要，然后空一行，罗列出改动原因、主要变动、以及需要注意的问题。最后，提供对应的网址（比如Bug ticket）。 第四步：与主干同步分支的开发过程中，要经常与主干保持同步。 $ git fetch origin $ git rebase origin/master第五步：合并commit分支开发完成后，很可能有一堆 commit，但是合并到主干的时候，往往希望只有一个（或最多两三个）commit，这样不仅清晰，也容易管理。 那么，怎样才能将多个 commit 合并呢？这就要用到 git rebase 命令。 $ git rebase -i origin/mastergit rebase 命令的 i 参数表示互动（interactive），这时git会打开一个互动界面，进行下一步操作。 pick 07c5abd Introduce OpenPGP and teach basic usage pick de9b1eb Fix PostChecker::Post#urls pick 3e7ee36 Hey kids, stop all the highlighting pick fa20af3 git interactive rebase, squash, amend # Rebase 8db7e8b..fa20af3 onto 8db7e8b # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like &quot;squash&quot;, but discard this commit&apos;s log message # x, exec = run command (the rest of the line) using shell # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out上面的互动界面，先列出当前分支最新的4个 commit（越下面越新）。每个 commit 前面有一个操作命令，默认是 pick，表示该行 commit 被选中，要进行 rebase 操作。 4个commit的下面是一大堆注释，列出可以使用的命令。 pick：正常选中 reword：选中，并且修改提交信息； edit：选中，rebase时会暂停，允许你修改这个commit（参考这里） squash：选中，会将当前commit与上一个commit合并 fixup：与squash相同，但不会保存当前commit的提交信息 exec：执行其他shell命令 上面这6个命令当中，squash 和 fixup 可以用来合并 commit。先把需要合并的 commit 前面的动词，改成 squash（或者s）。 pick 07c5abd Introduce OpenPGP and teach basic usage s de9b1eb Fix PostChecker::Post#urls s 3e7ee36 Hey kids, stop all the highlighting pick fa20af3 git interactive rebase, squash, amend这样一改，执行后，当前分支只会剩下两个commit。第二行和第三行的commit，都会合并到第一行的commit。提交信息会同时包含，这三个commit的提交信息。 # This is a combination of 3 commits. # The first commit&apos;s message is: Introduce OpenPGP and teach basic usage # This is the 2nd commit message: Fix PostChecker::Post#urls # This is the 3rd commit message: Hey kids, stop all the highlighting如果将第三行的 squash 命令改成 fixup 命令。 pick 07c5abd Introduce OpenPGP and teach basic usage s de9b1eb Fix PostChecker::Post#urls f 3e7ee36 Hey kids, stop all the highlighting pick fa20af3 git interactive rebase, squash, amend运行结果相同，还是会生成两个commit，第二行和第三行的commit，都合并到第一行的commit。但是，新的提交信息里面，第三行commit的提交信息，会被注释掉。 # This is a combination of 3 commits. # The first commit&apos;s message is: Introduce OpenPGP and teach basic usage # This is the 2nd commit message: Fix PostChecker::Post#urls # This is the 3rd commit message: # Hey kids, stop all the highlightingPony Foo提出另外一种合并commit的简便方法，就是先撤销过去5个commit，然后再建一个新的。 $ git reset HEAD~5 $ git add . $ git commit -am &quot;Here&apos;s the bug fix that closes #28&quot; $ git push --forcesquash 和 fixup 命令，还可以当作命令行参数使用，自动合并commit。 $ git commit --fixup $ git rebase -i --autosquash 这个用法请参考http://fle.github.io/git-tip-keep-your-branch-clean-with-fixup-and-autosquash.html，这里就不解释了。 第六步：推送到远程仓库合并commit后，就可以推送当前分支到远程仓库了。 $ git push --force origin myfeaturegit push 命令要加上 force 参数，因为 rebase 以后，分支历史改变了，跟远程分支不一定兼容，有可能要强行推送。 第七步：发出Pull Request提交到远程仓库以后，就可以发出 Pull Request 到 master 分支，然后请求别人进行代码 review，确认可以合并到 master。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用Git命令清单]]></title>
    <url>%2F2017%2F01%2F27%2Fgit-common-list%2F</url>
    <content type="text"><![CDATA[我每天使用 Git ，但是很多命令记不住。 一般来说，日常使用只要记住下图6个命令，就可以了。但是熟练使用，恐怕要记住60～100个命令。 下面是我整理的常用 Git 命令清单。几个专用名词的译名如下。 Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 一、新建代码库# 在当前目录新建一个Git代码库 $ git init # 新建一个目录，将其初始化为Git代码库 $ git init [project-name] # 下载一个项目和它的整个代码历史 $ git clone [url]二、配置Git的设置文件为 .gitconfig，它可以在用户主目录下（全局配置），也可以在项目目录下（项目配置）。 # 显示当前的Git配置 $ git config --list # 编辑Git配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.name &quot;[name]&quot; $ git config [--global] user.email &quot;[email address]&quot;三、增加/删除文件# 添加指定文件到暂存区 $ git add [file1] [file2] ... # 添加指定目录到暂存区，包括子目录 $ git add [dir] # 添加当前目录的所有文件到暂存区 $ git add . # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ... # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file] # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed]四、代码提交# 提交暂存区到仓库区 $ git commit -m [message] # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message] # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a # 提交时显示所有diff信息 $ git commit -v # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message] # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ...五、分支# 列出所有本地分支 $ git branch # 列出所有远程分支 $ git branch -r # 列出所有本地分支和远程分支 $ git branch -a # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name] # 新建一个分支，并切换到该分支 $ git checkout -b [branch] # 新建一个分支，指向指定commit $ git branch [branch] [commit] # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch] # 切换到指定分支，并更新工作区 $ git checkout [branch-name] # 切换到上一个分支 $ git checkout - # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch] # 合并指定分支到当前分支 $ git merge [branch] # 选择一个commit，合并进当前分支 $ git cherry-pick [commit] # 删除分支 $ git branch -d [branch-name] # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch]六、标签# 列出所有tag $ git tag # 新建一个tag在当前commit $ git tag [tag] # 新建一个tag在指定commit $ git tag [tag] [commit] # 删除本地tag $ git tag -d [tag] # 删除远程tag $ git push origin :refs/tags/[tagName] # 查看tag信息 $ git show [tag] # 提交指定tag $ git push [remote] [tag] # 提交所有tag $ git push [remote] --tags # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag]七、查看信息# 显示有变更的文件 $ git status # 显示当前分支的版本历史 $ git log # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat # 搜索提交历史，根据关键词 $ git log -S [keyword] # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s # 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件 $ git log [tag] HEAD --grep feature # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file] # 显示指定文件相关的每一次diff $ git log -p [file] # 显示过去5次提交 $ git log -5 --pretty --oneline # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn # 显示指定文件是什么人在什么时间修改过 $ git blame [file] # 显示暂存区和工作区的差异 $ git diff # 显示暂存区和上一个commit的差异 $ git diff --cached [file] # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch] # 显示今天你写了多少行代码 $ git diff --shortstat &quot;@{0 day ago}&quot; # 显示某次提交的元数据和内容变化 $ git show [commit] # 显示某次提交发生变化的文件 $ git show --name-only [commit] # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename] # 显示当前分支的最近几次提交 $ git reflog八、远程同步# 下载远程仓库的所有变动 $ git fetch [remote] # 显示所有远程仓库 $ git remote -v # 显示某个远程仓库的信息 $ git remote show [remote] # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url] # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch] # 上传本地指定分支到远程仓库 $ git push [remote] [branch] # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force # 推送所有分支到远程仓库 $ git push [remote] --all九、撤销# 恢复暂存区的指定文件到工作区 $ git checkout [file] # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file] # 恢复暂存区的所有文件到工作区 $ git checkout . # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file] # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit] # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit] # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit] # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit] # 暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop十、其他# 生成一个可供发布的压缩包 $ git archive]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git分支管理策略]]></title>
    <url>%2F2017%2F01%2F27%2Fgit-branch%2F</url>
    <content type="text"><![CDATA[如果你严肃对待编程，就必定会使用”版本管理系统”（Version Control System）。 眼下最流行的”版本管理系统”，非Git莫属。 相比同类软件，Git有很多优点。其中很显著的一点，就是版本的分支（branch）和合并（merge）十分方便。有些传统的版本管理软件，分支操作实际上会生成一份现有代码的物理拷贝，而Git只生成一个指向当前版本（又称”快照”）的指针，因此非常快捷易用。 但是，太方便了也会产生副作用。如果你不加注意，很可能会留下一个枝节蔓生、四处开放的版本库，到处都是分支，完全看不出主干发展的脉络。 Vincent Driessen提出了一个分支管理的策略，我觉得非常值得借鉴。它可以使得版本库的演进保持简洁，主干清晰，各个分支各司其职、井井有条。理论上，这些策略对所有的版本管理系统都适用，Git只是用来举例而已。如果你不熟悉Git，跳过举例部分就可以了。 一、主分支Master首先，代码库应该有一个、且仅有一个主分支。所有提供给用户使用的正式版本，都在这个主分支上发布。 Git主分支的名字，默认叫做Master。它是自动建立的，版本库初始化以后，默认就是在主分支在进行开发。 二、开发分支Develop主分支只用来分布重大版本，日常开发应该在另一条分支上完成。我们把开发用的分支，叫做Develop。 这个分支可以用来生成代码的最新隔夜版本（nightly）。如果想正式对外发布，就在Master分支上，对Develop分支进行”合并”（merge）。 Git创建Develop分支的命令： git checkout -b develop master将Develop分支发布到Master分支的命令： # 切换到Master分支 git checkout master # 对Develop分支进行合并 git merge --no-ff develop这里稍微解释一下，上一条命令的 --no-ff 参数是什么意思。默认情况下，Git执行”快进式合并”（fast-farward merge），会直接将Master分支指向Develop分支。 使用 --no-ff 参数后，会执行正常合并，在Master分支上生成一个新节点。为了保证版本演进的清晰，我们希望采用这种做法。 三、临时性分支前面讲到版本库的两条主要分支：Master和Develop。前者用于正式发布，后者用于日常开发。其实，常设分支只需要这两条就够了，不需要其他了。 但是，除了常设分支以外，还有一些临时性分支，用于应对一些特定目的的版本开发。临时性分支主要有三种： * 功能（feature）分支 * 预发布（release）分支 * 修补bug（fixbug）分支这三种分支都属于临时性需要，使用完以后，应该删除，使得代码库的常设分支始终只有Master和Develop。 四、功能分支接下来，一个个来看这三种”临时性分支”。 第一种是功能分支，它是为了开发某种特定功能，从Develop分支上面分出来的。开发完成后，要再并入Develop。 功能分支的名字，可以采用 feature-* 的形式命名。 创建一个功能分支： git checkout -b feature-x develop开发完成后，将功能分支合并到develop分支： git checkout develop git merge --no-ff feature-x删除feature分支： git branch -d feature-x五、预发布分支第二种是预发布分支，它是指发布正式版本之前（即合并到Master分支之前），我们可能需要有一个预发布的版本进行测试。 预发布分支是从Develop分支上面分出来的，预发布结束以后，必须合并进Develop和Master分支。它的命名，可以采用 release-* 的形式。 创建一个预发布分支： git checkout -b release-1.2 develop确认没有问题后，合并到master分支： git checkout master git merge --no-ff release-1.2 # 对合并生成的新节点，做一个标签 git tag -a 1.2再合并到develop分支： git checkout develop git merge --no-ff release-1.2最后，删除预发布分支： git branch -d release-1.2六、修补bug分支最后一种是修补bug分支。软件正式发布以后，难免会出现bug。这时就需要创建一个分支，进行bug修补。 修补bug分支是从Master分支上面分出来的。修补结束以后，再合并进Master和Develop分支。它的命名，可以采用 fixbug-* 的形式。 创建一个修补bug分支： git checkout -b fixbug-0.1 master修补结束后，合并到master分支： git checkout master git merge --no-ff fixbug-0.1 git tag -a 0.1.1再合并到develop分支： git checkout develop git merge --no-ff fixbug-0.1最后，删除”修补bug分支”： git branch -d fixbug-0.1]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 学习笔记]]></title>
    <url>%2F2016%2F12%2F23%2FDocker-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Docker基本命令常用Docker命令 1234567891011121314151617181920# 开启Docker守护进程调试模式$ sudo docker daemon -D# 查看Docker信息$ sudo docker info # 停止或者启动Docker$ sudo service docker stop/start # 以命令行模式运行一个容器$ sudo docker run -i -t ubuntu /bin/bash # 给容器命名$ sudo docker run --name Micheal_container -i -t ubuntu /bin/bash# 启动或者停止运行的容器$ sudo docker start/stop Micheal_container # 附着到正在运行的容器$ sudo docker attach Micheal_container 创建守护式容器 1$ sudo docker run --name daemon_dave -d ubuntu /bin/sh -c "while true; do echo hello world; sleep 1; done" 上面的docker run 使用了-d参数，因此Docker会将容器放到后台运行。 Docker日志 1234567891011121314# 获取守护式容器的日志$ sudo docker logs daemon_dave# 跟踪守护式容器的日志$ sudo docker logs -f daemon_dave# 获取日志的最后10行$ sudo docker logs --tail 10 daemon_dave # 跟踪某个容器的最新日志$ sudo docker logs --tail 0 -f daemon_dave# -t 标志为每条日志项加上时间戳$ sudo docker logs -ft daemon_dave Docker日志驱动 1$ sudo docker run --log-driver="syslog" --name daemon_dave -d ubuntu /bin/sh -c "while true; do echo hello world; sleep 1; done" 使用syslog将会禁用docker logs命令，并且将所有容器的日志输出都重定向到Syslog。 查看容器内的进程 1$ sudo docker top daemon_dave Docker统计信息 1$ sudo docker stats daemon_dave daemon_kate daemon_clear daemon_sarah 以上命令可以看到一个守护容器的列表，以及他们的CPU、内存、网络I/O以及存储I/O的性能和指标。这对快速监控一台主机上的一组容器非常有用。 在容器内部运行进程 1$ sudo docker exec -d daemon_dave touch /etc/new_config_file -d表示需要运行一个后台进程 12# 在容器内运行交互命令$ sudo docker exec -t -i daemon_dave /bin/bash 自动重启容器 1$ sudo docker run --restart=always --name daemon_dave -d ubuntu /bin/sh -c "while true; do echo hello world; sleep 1; done" --restart标志被设置为always。无论容器的退出代码是什么，Docker都会自动重启改容器。除了always，还可以将这个标志设为on-failure，这样，只有当容器的退出代码为非0值的时候，才会自动重启。另外，on-failure还接受一个可选的重启次数参数，--restart=on-failure:5,Docker会尝试自动重启改容器，最多重启5次。 深入容器 1$ sudo docker inspect daemon_dave docker inspect命令会对容器进行详细的检查，然后返回其配置信息，包括名称、命令、网络配置以及很多有用的数据。可以使用-f或者--format标志来选定查看结果。 1$ sudo docker inspect --format='&#123;.State.Running&#125;' daemon_dave 查看多个容器 1$ sudo docker inspect --format '&#123;.Name&#125; &#123;.State.Running&#125;' daemon_dave Micheal_container 删除容器​ 1234$ sudo docker rm daemon_dave# 删除所有容器$ sudo docker rm `sudo docker ps -a -q` 列出所有镜像 1$ sudo docker images 拉去镜像 1$ sudo docker pull ubuntu:16.04 运行一个带标签的Docker镜像 1$ sudo docker run -i -t --name new_container ubuntu:16.04 /bin/bash 查找镜像 1$ sudo docker search puppet 构建镜像 使用docker commit命令 使用docker build命令和Dockerfile文件 用Docker的commit命令创建镜像 123456789101112131415$ sudo docker run -i -t ubuntu /bin/bash# 接下来安装需要安装的工具，安装完成后exit退出容器, eg：$ apt-get -yqq update$ apt-get -y install apache2# 指定提交修改过的容器的ID（可以通过docker ps -l -q命令得到刚创建的容器的ID）$ sudo docker commit 4aab3cecb76 micheal/apache2 # 检查新创建的镜像sudo docker images micheal/apache2 # 提交另一个新定制容器# -m 选项用来指定新创建的镜像的提交信息，-a 用来列出该镜像的作者信息。$ sudo docker commit -m"A new custom image" -a"Micheal" 4aab3cecb76 micheal/apache2:webserver 用Dockerfile构建镜像 Dockerfile文件示例： 123456# Vsersion: 0.0.1FROM ubuntu:16.04MAINTAINER Micheal "miaopei@baicells.com"RUN apt-get -yqq update &amp;&amp; apt-get -y install nginxRUN echo 'Hi, I an in your container' &gt; /usr/share/nginx/html/index.htmlEXPOSE 80 Dockerfile中的指令会按照顺序从上到下执行，所以根据需要合理安排指令的顺序。 如果Dockerfile由于某些原因没有正常结束，那么用户得到了一个可以使用的镜像。这对调试非常有帮助：可以基于改镜像运行一个具备交互功能的容器，使用最后创建的镜像对为什么用户指令会失败进行调试。 每个Dockerfile的第一条指令必须是FROM,FROM指令指定一个已经存在的镜像，后续指令都将基于该镜像进行，这个镜像被称为基础镜像。 MAINTAINER指令告诉Docker镜像的作者是谁，以及作者的电子邮件。有助于标识镜像的所有者和联系方式。 默认情况下，RUN指令会在shell里使用命令包装器/bin/sh -c来执行，如果是在一个不支持shell的平台上运行或者不希望在shell中运行（比如避免shell字符串篡改），也可以使用exec格式的RUN指令，如下所示： 1RUN [ "apt-get", " install", "-y", "nginx" ] EXPOSE指令告诉Docker该容器内的应用程序将会使用该容器的指定端口。 基于Dockerfile构建新镜像 12345678$ sudo docker build -t="micheal/static_web" .$ sudo docker build -t="micheal/static_web:v1" .# 这里Docker假设在这个Git仓库的根目录下存在Dockerfile文件$ sudo docker build -t="micheal/static_web:v1" git@github.com:micheal/docker_static_web # 忽略Dockerfile的构建缓存$ sudo docker build --no-cache -t="micheal/static_web" . 查看镜像 1234567# 列出Docker镜像$ sudo docker images# 查看镜像每一层，以及创建这些层的Dockerfile指令$ sudo docker history micheal/static_web $ sudo docker run -d -p 80 --name statix_web micheal/static_web nginx -g "daemon off;" nginx -g “daemon off;”,这将以前台的方式启动Nginx。 -p标志用来控制Docker在运行时应该公开那些网络端口给外部（宿主机）。运行一个容器时，Docker可以通过两种方式来在宿主机上分配端口。 Docker可以在宿主机上随机选择一个位于32768 ~ 61000的一个比较大的端口号来映射到容器中的80端口上。 可以在Docker宿主机只指定一个具体的端口号来映射到容器中的80端口上。 查看Docker端口映射情况 1234567$ sudo docker ps -l# 返回宿主机中映射的端口$ sudo socker port static_web 80 # -p会将容器内的80端口绑定到宿主机的8080端口上$ sudo docker run -d -p 8080:80 --name statix_web micheal/static_web nginx -g "daemon off;" Dockerfile指令 CMD CMD指令用于指定一个容器启动时要运行的命令。这有点儿类似于RUN指令，只是RUN指令是指定容器镜像被构建时要运行的命令，而CMD是指定容器被启动时要运行的命令。 1CMD ["/bin/bash/", "-l"] ENTRYPOINT ENTRYPOINT和CMD指令非常类似，我们可在docker run命令行中覆盖CMD指令，而ENTRYPOINT指令提供的命令则不容易在启动容器的时候被覆盖。 可以组合使用ENTRYPOINT和CMD指令来完成一些巧妙的工作。 12ENTRYPOINT ["/usr/sbin/nginx"]CMD ["-h"] WORKDIR WORKDIR指令用来在从镜像创建一个新容器时，在容器内部设置一个工作目录，ENTRYPOINT和/或CMD指定的程序会在这个目录下执行。 1234WORKDIR /opt/webapp/dbRUN bundle installWORKDIR /opt/webappENTRYPOINT ["rackup"] 可以通过-w标志在运行时覆盖工作目录 1$ sudo docker run -ti -w /var/log ubuntu pwd/var/log ENV ENV指令用来在镜像构建过程中设置环境变量。这些变量会持久保存到从我们镜像创建的任何容器中。 1ENV RVM_PATH /home/rvm 也可以使用docker run命令行的-e标志来传递环境变量。这些环境变量只会在运行时有效。 1$ sudo docker run -ti -e "WEB_PORT=8080" ubuntu env USER USER指令用来指定该镜像会以什么样的用户身份来运行。我们可以指定用户名或者UID以及组或GID，甚至是两者的组合。 123456USER userUSER user:groupUSER uidUSER uid:gidUSER user:gidUSER uid:group 也可以在docker run命令行中通过-u标志覆盖该指令指定的值。 VOLUME VOLUME指令用来向基于镜像创建的容器添加卷。一个卷可以存在于一个或者多个容器内特定的目录，这个目录可以绕过联合文件系统，并提供如下共享数据或者对数据进行持久化的功能。 卷可以在容器间共享和重用 一个容器可以不是必须和其他容器共享卷 对卷的修改是立即生效的 对卷的修改不会对更新镜像产生影响 卷会一直存在直到没有任何容器再使用它 卷功能让我们可以将数据（如源代码）、数据库或者其他内容添加到镜像中而不是将这些内容提交到镜像中，并且允许我们在多个容器间共享这些内容，我们可以利用此功能来测试容器和内部应用程序代码，管理日志，或者处理容器内部的数据库。 1VOLUME ["/opt/project"] 这条指令将会基于此镜像的任何容器创建一个名为/opt/project的挂载点。 也可以通过指定数组的方式指定多个卷 1VOLUME ["/opt/project", "/data"] ADD ADD指令用来将构建环境下的文件和目录复制到镜像中。不能对构建目录或者上下文之外的文件进行ADD操作。 12ADD software.lic /opt/application/software.licADD latest.tar.gz /var/www/wordpress/ //这条指令会将归档文件解开到指定的目录下 COPY COPY指令非常类似ADD，它们根本不同是COPY只关心构建上下文中复制本地文件，而不会去做文件提取（extraction）和解压（decompression）的工作。 1COPY conf.d/ /etc/apache2/ LABEL LABEL指令用于为Docker镜像添加元数据。元数据以键值对的形式展现 12LABEL version="1.0"LABEL location="New York" type="Data Center" role="Web Server" 可以使用docker inspect命令查看容器标签 1$ sudo docker inspect micheal/apache2 STOPSIGNAL STOPSIGNAL指令用来设置停止容器时发送什么系统调用信号给容器。 ARG ARG指令用来定义可以在docker build命令运行时传递给构建运行时的变量，我们只需要在构建时使用–build-arg标志即可。用户只能在构建时指定在Dockerfile文件汇总定义过的参数。 1234ARG buildARG webapp_user=user$ docker build --build-arg build=1234 -t micheal/webapp . ONBUILD ONBUILD指令能为镜像添加触发器（trigger）。当一个镜像被用做其他镜像的基础镜像时（比如用户的镜像需要从某未准备好的位置添加源代码，或者用户需要执行特定于构建镜像的环境的构建脚本），该镜像中的触发器将会被执行。 触发器会在构建过程中插入新指令，我们可以认为这些指令是紧跟在FROM之后指定的。触发器可以是任何构建指令。 12ONBUILD ADD . /app/srcONBUILD RUN cd /app/src/ &amp;&amp; make 上面的代码将会在创建的镜像中加入ONBUILD触发器，ONBUILD指令可以在镜像上运行docker inspect命令查看。 Docker Networking 容器之间的连接用网络创建，这被称为Docker Networking。Docker Networking允许用户创建自己的网络，容器可以通过这个网上互相通信。更重要的是，现在容器可以跨越不同的宿主机来通信，并且网络配置可以更灵活的定制。Docker Networking也和Docker Compose以及Swarm进行了集成。 要想使用Docker网络，需要先创建一个网络，然后在这个网络下启动容器。 1$ sudo docker network create app 这里使用docker network命令创建了一个桥接网络，命名为app。可以使用docker network inspect命令查看新创建的这个网络。 1$ sudo docker network inspect app 我们可以看到这个新网络是一个本地的桥接网络（这非常像docker0网络），而且现在没有容器再这个网络中运行。 可以使用docker network ls命令列出当前系统中所有的网络。 1$ sudo docker network ls 也可以使用 docker network rm命令删除一个Docker网络。 在Docker网络中创建Redis容器 1$ sudo docker run -d --net=app --name db micheal/redis --net标志指定了新容器将会在那个网络中运行。 1$ sudo docker network inspect app 将已有容器连接到Docker网络 1$ sudo docker network connect app db2 可以通过docker network disconnect 命令断开一个容器与指定网络的连接 1$ sudo docker network disconnect app db2 通过Docker链接连接容器 启动一个Redis容器 1$ sudo docker run -d --name redis micheal/redis 注意：这里没有公开容器的任何端口。一会就能看到这么做的原因。 链接Redis容器 1$ sudo docker run -p 4567 --name webapp --link redis:db -t -i -v $PWD/webapp_redis:/opt/webapp micheal/sinatra /bin/bash 这个命令做了不少事情，我们逐一解释。首先，我们使用-p标志公开4567端口，这样就能从外面访问web应用程序。 我们还使用--name标志给容器命名为webapp，并且使用了-v标志把web应用程序目录作为卷挂载到了容器里。 然而，这次我们使用了一个新标志--link。--link标志创建了两个容器间的客户-服务链接。这个标志需要两个参数：一个是要链接的容器的名字，另一个是链接的别名。这个例子中我们创建了客户联系，webapp容器是客户，redis容器是“服务”，并且为这个服务增加了db作为别名。这个别名让我们可以一致地访问容器公开信息，而无须关注底层容器的名字。链接让服务容器有能力与客户容器通信，并且能分享一些连接细节，这些细节有助于在应用程序中配置并使用这个链接。 连接也能得到一些安全上的好处。注意，启动 Redis 容器时，并没有使用-p标志公开Redis的端口。因为不需要这么做。通过把容器链接在一起，可以让客户直接访问任意服务容器的公开端口（即客户webapp容器可以连接到服务redis容器的6379端口）。更妙的是，只有使用--link标志链接到这个容器的容器才能连接到这个端口。容器的端口不需要对本地宿主机公开，现在我们已经拥有一个非常安全的模型。通过这个安全模型，就可以限制容器化应用程序被攻击面，减少应用暴露的网络。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
</search>
